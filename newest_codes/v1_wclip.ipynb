{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "k-OWRYMLzzG0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee6f258085744229aaa9b7e5abd4ee95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ba730fdc64a46aba15caea30a5abed0",
              "IPY_MODEL_a6b38b4bc4e04d56ac49f970ced17b1f"
            ],
            "layout": "IPY_MODEL_713cf2003a8945339baddfeac2906198"
          }
        },
        "0ba730fdc64a46aba15caea30a5abed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3db12ff245f4d9d81f70e237af11b62",
            "placeholder": "​",
            "style": "IPY_MODEL_0c82f6e0fd3f4de6b8d6d48bfb3b00a7",
            "value": "0.050 MB of 0.050 MB uploaded\r"
          }
        },
        "a6b38b4bc4e04d56ac49f970ced17b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3507b5e072414af6b724ccfdc6ecaa04",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c838cc883634e55aff24163e57194c1",
            "value": 1
          }
        },
        "713cf2003a8945339baddfeac2906198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3db12ff245f4d9d81f70e237af11b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c82f6e0fd3f4de6b8d6d48bfb3b00a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3507b5e072414af6b724ccfdc6ecaa04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c838cc883634e55aff24163e57194c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31ed795314fe40a68664748e114bef5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29c0c6c0d8a24d1c865e951953374e91",
              "IPY_MODEL_921c561c799d4492b89462a64e296a45"
            ],
            "layout": "IPY_MODEL_4e4a4dee79f64fafa44c7a674b048cb5"
          }
        },
        "29c0c6c0d8a24d1c865e951953374e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e750425b974a4fdd8478c6eac999c635",
            "placeholder": "​",
            "style": "IPY_MODEL_3355252a466946edab86b21dda816b37",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "921c561c799d4492b89462a64e296a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d62a089d5f419482d27148c5153345",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dabad221f3664809ae9ce10630536910",
            "value": 1
          }
        },
        "4e4a4dee79f64fafa44c7a674b048cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e750425b974a4fdd8478c6eac999c635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3355252a466946edab86b21dda816b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d62a089d5f419482d27148c5153345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dabad221f3664809ae9ce10630536910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pip installs"
      ],
      "metadata": {
        "id": "k-OWRYMLzzG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install black\n",
        "!pip install fair-esm\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlB53XWXSmW",
        "outputId": "b5122a7f-9856-4674-eef4-0962c40ea4c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 rapidfuzz-3.9.6\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting einops_exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops_exts) (0.8.0)\n",
            "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: einops_exts\n",
            "Successfully installed einops_exts-0.0.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting black\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, mypy-extensions, black\n",
            "Successfully installed black-24.8.0 mypy-extensions-1.0.0 pathspec-0.12.1\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "lCVCpLwsz2ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7PkQA3ZYDcT",
        "outputId": "0c7db680-5a4d-4d89-bc95-050c8804c6df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "zYIaRxvBMU0G",
        "outputId": "b6e24549-a8d8-45ba-a95c-9a7de182ede2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import re\n",
        "import esm\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import wandb\n",
        "wandb.login()\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/')\n",
        "\n",
        "# ESM Model Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model = esm_model.to(device)\n",
        "esm_model.eval()\n",
        "for param in esm_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[dataset['protein_RCSB'] != dataset['peptide_source_RCSB']]\n",
        "\n",
        "# Dataset Class\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.mismatched_lengths = 0\n",
        "        self.total_samples = len(dataframe)\n",
        "        self.check_lengths()\n",
        "\n",
        "    def check_lengths(self):\n",
        "        for idx in range(self.total_samples):\n",
        "            row = self.dataframe.iloc[idx]\n",
        "            peptide_seq = row['peptide_derived_sequence']\n",
        "            energy_scores = row['energy_scores']\n",
        "\n",
        "            energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "            energy_scores = [float(score) for score in energy_scores]\n",
        "\n",
        "            if len(energy_scores) != len(peptide_seq):\n",
        "                self.mismatched_lengths += 1\n",
        "\n",
        "        print(f\"Total samples: {self.total_samples}\")\n",
        "        print(f\"Mismatched lengths: {self.mismatched_lengths}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        peptide_seq = row['peptide_derived_sequence']\n",
        "        protein_seq = row['protein_derived_sequence']\n",
        "        energy_scores = row['energy_scores']\n",
        "\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = self.one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        # Convert energy scores to tensor\n",
        "        energy_scores = torch.tensor(energy_scores, dtype=torch.float32)\n",
        "\n",
        "        return energy_scores, peptide_seq, protein_seq # energy scores are aligned with the peptide (we will keep peptide as protien)\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hot_encode_energy_scores(scores):\n",
        "        return [1 if score <= -1 else 0 for score in scores]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and preprocess data\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "train_snp = filter_datasets(train_snp)\n",
        "val_snp = filter_datasets(val_snp)\n",
        "test_snp = filter_datasets(test_snp)\n",
        "\n",
        "train_snp, val_snp, test_snp = train_snp[:16], val_snp[16:24], test_snp[24:32] # subset code\n",
        "\n",
        "# Calculate max_length\n",
        "all_seqs = pd.concat([\n",
        "    train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "    val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence'],\n",
        "    test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence']\n",
        "])\n",
        "max_length = max(len(seq) for seq in all_seqs)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinInteractionDataset(train_snp)\n",
        "val_dataset = ProteinInteractionDataset(val_snp)\n",
        "test_dataset = ProteinInteractionDataset(test_snp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53-ZaGmiF1Hh",
        "outputId": "59491e21-e9fb-4ecc-dc2a-fcaad4af487c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 16\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):  # Adjust range to view more samples\n",
        "    energy_scores, protein_seq, peptide_seq = train_dataset[i]\n",
        "    print(f\"Sample {i}:\")\n",
        "\n",
        "    # Print energy scores and their length\n",
        "    print(f\"Energy Scores: {energy_scores}\")\n",
        "    print(f\"Length of Energy Scores: {energy_scores.shape[0]}\")\n",
        "\n",
        "    # Print protein sequence and its length\n",
        "    print(f\"Protein Sequence: {protein_seq}\")\n",
        "    print(f\"Length of Protein Sequence: {len(protein_seq)}\")\n",
        "\n",
        "    # Print peptide sequence and its length\n",
        "    print(f\"Peptide Sequence: {peptide_seq}\")\n",
        "    print(f\"Length of Peptide Sequence: {len(peptide_seq)}\")\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk9sFe_PF5uB",
        "outputId": "6cc81513-baf8-4802-fa04-72787b188241"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Energy Scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 221\n",
            "Protein Sequence: EVQLVQSGAEVKKPGESLNISCKASGYSFTIYWIAWVRQLPGKGLEWMGIIYPGDSDTRYSPSFQGQVTISADKSISTAYLQWRSLKASDSAVYYCARGVAVDWYFDLWGRGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKS\n",
            "Length of Protein Sequence: 221\n",
            "Peptide Sequence: QSVLTQPPSVSGAPGQRVTISCAGSSSNIGAGFDVYWYQQLPGTAPKLLIYGNNNRPSGVPDRFSGSKSGTSASLAITGLQAEDEADYYCQSSGSVLSDLYVFGTGTKVTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAPTE\n",
            "Length of Peptide Sequence: 216\n",
            "\n",
            "\n",
            "Sample 1:\n",
            "Energy Scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 270\n",
            "Protein Sequence: NLTCDFNDVYKLEFHPNQQTSVTKLCNLTPNVLEKVTIKCGSDKLNYNLYPPTCFEEVYASRNMMHLKKIKEFVIGSSMFMRRSLTPNKINEVSFRIPPNMMPEKPIYCFCENKKTITINGSNGNPSSKKDIINRGIVEIIIPSLNEKVKGCDFTTSESTIFSKGYSINEISQDIVCTVKAHANDLIGFKCPSNYSVEPHDCFVSAFNLSGKNENLENKLKLTNIIMDHYNNTFYSRLPSLISDNWKFFCVCSKDNEKKLVFTVEASISS\n",
            "Length of Protein Sequence: 270\n",
            "Peptide Sequence: LQESGGGLVQAGGSLRLSCAASGRTFSSYGMGWFRQAPGTEREFVAAISWSGDSTYYADSVKGRFTISIDKAKNTVYLQMNSLKPEDTAVYYCAADHALVVGGTYNYWGQGTQVTVSS\n",
            "Length of Peptide Sequence: 118\n",
            "\n",
            "\n",
            "Sample 2:\n",
            "Energy Scores: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Length of Energy Scores: 110\n",
            "Protein Sequence: NMFKSKHKLDFSLVSMDQRGKHILGYELVNMGGYDLVHYDDLAYVASAHQELLKTGASGMIAYRYQKKDGEWQWLQTSSRLVYKNSKPDFVICTHRQLMDEEGHDLLGKR\n",
            "Length of Protein Sequence: 110\n",
            "Peptide Sequence: TEFISRHNIEGIFTFVDHRCVATVGYQPQELLGKNIVEFCHPEDQQLLRDSFQQVVKLKGQVLSVMFRFRSKTREWLWMRTSSFTFQNPYSDEIEYIICTNTNV\n",
            "Length of Peptide Sequence: 104\n",
            "\n",
            "\n",
            "Sample 3:\n",
            "Energy Scores: tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 132\n",
            "Protein Sequence: ANAFLLRPGSLRCKQCSFARIFKDARTKLFWISYSDGDQCASSPCQNGGSCKDQLQSYICFCLPAFEGRNCETHKDDQLICVNENGGCEQYCSDHTGTKRSCRCHEGYSLLADGVSCTPTVEYPCGKIPILE\n",
            "Length of Protein Sequence: 132\n",
            "Peptide Sequence: EPLYENSPEFTPYLETNLGQPTIQSFEQVGTKVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTVNRKSTDSPVECM\n",
            "Length of Peptide Sequence: 116\n",
            "\n",
            "\n",
            "Sample 4:\n",
            "Energy Scores: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 154\n",
            "Protein Sequence: SMAASRRLMKELEEIRKCGMKNFRNIQVDEANLLTWQGLIVPDNPPYDKGAFRIEINFPAEYPFKPPKITFKTKIYHPNIDEKGQVCLPVISAENWKPATKTDQVIQSLIALVNDPQPEHPLRADLAEEYSKDRKKFCKNAEEFTKKYGEKRPV\n",
            "Length of Protein Sequence: 154\n",
            "Peptide Sequence: GSEFQECAVCGWALPHNRMQALTSCECTICPDCFRQHFTIALKEKHITDMVCPACGRPDLTDDTQLLSYFSTLDIQLRESLEPDAYALFHKKLTEGVLMRD\n",
            "Length of Peptide Sequence: 101\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "b5j2iJtxz8rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RefinedLatentDiffusion(nn.Module):\n",
        "    def __init__(self, esm_model, num_steps, device):\n",
        "        super().__init__()\n",
        "        self.esm_model = esm_model\n",
        "        self.num_steps = num_steps\n",
        "        self.device = device\n",
        "        self.latent_dim = esm_model.embed_dim  # Using full ESM latent space (1280)\n",
        "\n",
        "        # CLIP model\n",
        "        self.clip_model = CLIPModel(embed_dim=self.latent_dim, projection_dim=self.latent_dim)\n",
        "\n",
        "        # Refined representation\n",
        "        self.refined_representation = RefinedRepresentation(seq_len=1000)  # Adjust seq_len as needed\n",
        "\n",
        "        # ESM attention layers for self and cross attention\n",
        "        self.esm_attention_layers = nn.ModuleList([\n",
        "            self.esm_model.layers[i] for i in range(-4, 0)  # Using the last 4 layers\n",
        "        ])\n",
        "\n",
        "        # Attention modules\n",
        "        self.self_attention = SelfAttentionModule(self.esm_attention_layers)\n",
        "        self.cross_attention = CrossAttentionModule(self.esm_attention_layers)\n",
        "\n",
        "        # Define beta schedule\n",
        "        self.beta = torch.linspace(1e-4, 0.02, num_steps).to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "\n",
        "        # Add epitope projection layer\n",
        "        self.epitope_proj = nn.Sequential(\n",
        "            nn.LazyLinear(256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.Linear(1024, 1280),\n",
        "        )\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0).to(self.device)\n",
        "        return (\n",
        "            self.sqrt_alpha_bar[t, None, None] * x0 +\n",
        "            self.sqrt_one_minus_alpha_bar[t, None, None] * noise\n",
        "        )\n",
        "\n",
        "    def p_losses(self, ab_latent, ag_latent, epitope_latent, t, target_seq):\n",
        "        noise = torch.randn_like(ab_latent).to(self.device)\n",
        "        x_noisy = self.q_sample(ab_latent, t, noise=noise)\n",
        "\n",
        "        # Project epitope_latent to match ab_latent dimension\n",
        "        epitope_latent = self.epitope_proj(epitope_latent)\n",
        "\n",
        "        # Apply self and cross attention\n",
        "        ag_latent = self.self_attention(ag_latent)  # ag<>ag self-attention\n",
        "        x_noisy = self.self_attention(x_noisy)  # ab<>ab self-attention\n",
        "        x_noisy, epitope_latent = self.cross_attention(x_noisy, epitope_latent)  # ab<>epitope and epitope<>ab cross-attention\n",
        "        predicted_noise = x_noisy - ab_latent  # Simplified noise prediction\n",
        "\n",
        "        # Calculate diffusion loss\n",
        "        diff_loss = F.mse_loss(predicted_noise, noise)\n",
        "\n",
        "        # Calculate CLIP loss\n",
        "        ab_clip, ag_clip, clip_loss = self.clip_model(ab_latent, ag_latent)\n",
        "\n",
        "        # Calculate categorical cross-entropy loss\n",
        "        logits = self.esm_model.lm_head(x_noisy)\n",
        "        ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq.view(-1))\n",
        "\n",
        "        total_loss = diff_loss + clip_loss + ce_loss\n",
        "        return total_loss, clip_loss, ce_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x, ag_latent, epitope_latent, t):\n",
        "        betas_t = self.beta[t][:, None, None]\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alpha_bar[t][:, None, None]\n",
        "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alpha[t])[:, None, None]\n",
        "\n",
        "        # Project epitope_latent to match ab_latent dimension\n",
        "        epitope_latent = self.epitope_proj(epitope_latent)\n",
        "\n",
        "        # Apply self and cross attention\n",
        "        ag_latent = self.self_attention(ag_latent)  # ag<>ag self-attention\n",
        "        x = self.self_attention(x)  # ab<>ab self-attention\n",
        "        x, epitope_latent = self.cross_attention(x, epitope_latent)  # ab<>epitope and epitope<>ab cross-attention\n",
        "\n",
        "        noise_pred = x - ag_latent  # Simplified noise prediction\n",
        "\n",
        "        model_mean = sqrt_recip_alphas_t * (\n",
        "            x - betas_t * noise_pred / sqrt_one_minus_alphas_cumprod_t\n",
        "        )\n",
        "\n",
        "        if t[0] > 0:\n",
        "            noise = torch.randn_like(x).to(self.device)\n",
        "            x0_t = model_mean + torch.sqrt(betas_t) * noise\n",
        "        else:\n",
        "            x0_t = model_mean\n",
        "\n",
        "        # Dynamic thresholding (eagle and imagegen both include)\n",
        "        p = 0.995  # Set this to desired percentile (e.g., 99.5%)\n",
        "        s = torch.quantile(torch.abs(x0_t), p, dim=(1, 2), keepdim=True)\n",
        "        s = torch.maximum(s, torch.ones_like(s))\n",
        "        x0_t = torch.clip(x0_t, -s, s) / s\n",
        "\n",
        "        return x0_t\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, ag_seq, epitope_seq, num_samples=1, guidance_scale=2.0):\n",
        "        device = next(self.parameters()).device\n",
        "        batch_converter = esm_model.alphabet.get_batch_converter()\n",
        "        _, _, ag_tokens = batch_converter([(i, seq) for i, seq in enumerate(ag_seq)])\n",
        "        _, _, epitope_tokens = batch_converter([(i, seq) for i, seq in enumerate(epitope_seq)])\n",
        "        ag_tokens = ag_tokens.to(device)\n",
        "        epitope_tokens = epitope_tokens.to(device)\n",
        "\n",
        "        max_seq_length = max(ag_tokens.size(1), epitope_tokens.size(1))\n",
        "\n",
        "        ag_tokens = pad_or_truncate(ag_tokens, max_seq_length, pad_value=esm_model.alphabet.padding_idx)\n",
        "        epitope_tokens = pad_or_truncate(ag_tokens, max_seq_length, pad_value=esm_model.alphabet.padding_idx)\n",
        "\n",
        "        ag_latent = self.esm_model(ag_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "        epitope_latent = self.esm_model(epitope_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "        shape = (num_samples, ag_latent.shape[1], self.latent_dim)\n",
        "        x = torch.randn(shape, device=device)\n",
        "\n",
        "        for t in reversed(range(0, self.num_steps)):\n",
        "            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "\n",
        "            # Generate both conditional and unconditional samples\n",
        "            x_cond = self.p_sample(x, ag_latent, epitope_latent, t_batch)\n",
        "            x_uncond = self.p_sample(x, torch.zeros_like(ag_latent), torch.zeros_like(epitope_latent), t_batch)\n",
        "\n",
        "            # Apply classifier-free guidance\n",
        "            x = x_uncond + guidance_scale * (x_cond - x_uncond)\n",
        "\n",
        "        # Convert latent to amino acid sequence\n",
        "        logits = self.esm_model.lm_head(x)\n",
        "        sequences = logits.argmax(dim=-1)\n",
        "        return self.esm_model.decode(sequences)\n",
        "\n",
        "class SelfAttentionModule(nn.Module):\n",
        "    def __init__(self, esm_layers):\n",
        "        super().__init__()\n",
        "        self.esm_attention_layers = esm_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create attention mask\n",
        "        attention_mask = torch.ones(x.shape[0], x.shape[1], dtype=torch.bool, device=x.device)\n",
        "\n",
        "        for esm_layer in self.esm_attention_layers:\n",
        "            # Transpose x to match ESM's expected format: (seq_length, batch_size, embed_dim)\n",
        "            x = x.transpose(0, 1)\n",
        "\n",
        "            # Apply self-attention\n",
        "            residual = x\n",
        "            x = esm_layer.self_attn_layer_norm(x)\n",
        "            x, _ = esm_layer.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=~attention_mask,\n",
        "                need_weights=False\n",
        "            )\n",
        "            x = residual + x\n",
        "\n",
        "            # Apply feed-forward network\n",
        "            residual = x\n",
        "            x = esm_layer.final_layer_norm(x)\n",
        "            x = esm_layer.fc1(x)\n",
        "            x = F.gelu(x)\n",
        "            x = esm_layer.fc2(x)\n",
        "            x = residual + x\n",
        "\n",
        "            # Transpose x back to (batch_size, seq_length, embed_dim)\n",
        "            x = x.transpose(0, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CrossAttentionModule(nn.Module):\n",
        "    def __init__(self, esm_layers):\n",
        "        super().__init__()\n",
        "        self.esm_attention_layers = esm_layers\n",
        "\n",
        "    def forward(self, ab_latent, epitope_latent):\n",
        "        # Transpose to match ESM's expected format: (seq_length, batch_size, embed_dim)\n",
        "        ab_latent = ab_latent.transpose(0, 1)\n",
        "        epitope_latent = epitope_latent.transpose(0, 1)\n",
        "\n",
        "        for esm_layer in self.esm_attention_layers:\n",
        "            # Cross-attention of ab to epitope\n",
        "            ab_residual = ab_latent\n",
        "            ab_latent = esm_layer.self_attn_layer_norm(ab_latent)\n",
        "            ab_latent, _ = esm_layer.self_attn(\n",
        "                query=ab_latent,\n",
        "                key=epitope_latent,\n",
        "                value=epitope_latent,\n",
        "                need_weights=False\n",
        "            )\n",
        "            ab_latent = ab_residual + ab_latent\n",
        "\n",
        "            # Cross-attention of epitope to ab\n",
        "            epitope_residual = epitope_latent\n",
        "            epitope_latent = esm_layer.self_attn_layer_norm(epitope_latent)\n",
        "            epitope_latent, _ = esm_layer.self_attn(\n",
        "                query=epitope_latent,\n",
        "                key=ab_latent,\n",
        "                value=ab_latent,\n",
        "                need_weights=False\n",
        "            )\n",
        "            epitope_latent = epitope_residual + epitope_latent\n",
        "\n",
        "            # Apply feed-forward networks\n",
        "            ab_latent = self._apply_feed_forward(esm_layer, ab_latent)\n",
        "            epitope_latent = self._apply_feed_forward(esm_layer, epitope_latent)\n",
        "\n",
        "        # Transpose back to (batch_size, seq_length, embed_dim)\n",
        "        ab_latent = ab_latent.transpose(0, 1)\n",
        "        epitope_latent = epitope_latent.transpose(0, 1)\n",
        "        return ab_latent, epitope_latent\n",
        "\n",
        "    def _apply_feed_forward(self, esm_layer, x):\n",
        "        residual = x\n",
        "        x = esm_layer.final_layer_norm(x)\n",
        "        x = esm_layer.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = esm_layer.fc2(x)\n",
        "        x = residual + x\n",
        "        return x\n",
        "\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self, embed_dim, projection_dim):\n",
        "        super().__init__()\n",
        "        self.ab_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.ag_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.project_ab = nn.Linear(embed_dim, projection_dim)\n",
        "        self.project_ag = nn.Linear(embed_dim, projection_dim)\n",
        "\n",
        "    def forward(self, ab_emb, ag_emb):\n",
        "        ab_vec = self.ab_encoder(ab_emb)\n",
        "        ag_vec = self.ag_encoder(ag_emb)\n",
        "\n",
        "        ab_embed = F.normalize(self.project_ab(ab_vec[:, 0]), dim=-1)\n",
        "        ag_embed = F.normalize(self.project_ag(ag_vec[:, 0]), dim=-1)\n",
        "\n",
        "        # Symmetrical CLIP loss\n",
        "        similarity = torch.matmul(ab_embed, ag_embed.t())\n",
        "        labels = torch.arange(similarity.size(0)).to(similarity.device)\n",
        "        loss_i = F.cross_entropy(similarity, labels)\n",
        "        loss_t = F.cross_entropy(similarity.t(), labels)\n",
        "        clip_loss = (loss_i + loss_t) / 2\n",
        "\n",
        "        return ab_embed, ag_embed, clip_loss\n",
        "\n",
        "class RefinedRepresentation(nn.Module):\n",
        "    def __init__(self, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, tokens, energy_scores):\n",
        "        # One-hot encoding\n",
        "        one_hot = F.one_hot(tokens, num_classes=len(esm_model.alphabet))\n",
        "        # Binary motif channel\n",
        "        motif_channel = (energy_scores <= -1).float().unsqueeze(-1)\n",
        "        # Combine representations\n",
        "        combined = torch.cat([one_hot, motif_channel], dim=-1)\n",
        "        return combined\n",
        "\n",
        "\n",
        "def pad_or_truncate(tensor, target_length, pad_value=0):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < target_length:\n",
        "        padding = torch.full((tensor.size(0), target_length - current_length, *tensor.size()[2:]), pad_value, device=tensor.device)\n",
        "        return torch.cat([tensor, padding], dim=1)\n",
        "    else:\n",
        "        return tensor[:, :target_length]\n",
        "\n",
        "def train_clip(model, esm_model, train_loader, val_loader, optimizer, num_epochs, device):\n",
        "    wandb.init(project=\"protein_binding_clip\", entity=\"vskavi2003\")\n",
        "    wandb.config.update({\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": train_loader.batch_size\n",
        "    })\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"CLIP Epoch {epoch+1}/{num_epochs}\"):\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            protein_tokens = protein_seq.to(device)\n",
        "            peptide_tokens = peptide_seq.to(device)\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(esm_model.alphabet)).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                protein_embedding = esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "                peptide_embedding = esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            _, _, clip_loss = model(peptide_embedding, protein_embedding)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            clip_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += clip_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                energy_scores, protein_seq, peptide_seq = batch\n",
        "                energy_scores = energy_scores.to(device)\n",
        "                protein_tokens = protein_seq.to(device)\n",
        "                peptide_tokens = peptide_seq.to(device)\n",
        "                protein_onehot = F.one_hot(protein_tokens, num_classes=len(esm_model.alphabet)).float()\n",
        "\n",
        "                protein_embedding = esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "                peptide_embedding = esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "                _, _, val_clip_loss = model(peptide_embedding, protein_embedding)\n",
        "                total_val_loss += val_clip_loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"clip_train_loss\": avg_train_loss,\n",
        "            \"clip_val_loss\": avg_val_loss\n",
        "        })\n",
        "\n",
        "        print(f\"CLIP Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_diff_loss = 0\n",
        "    total_clip_loss = 0\n",
        "    total_ce_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            protein_tokens = protein_seq.to(device)\n",
        "            peptide_tokens = peptide_seq.to(device)\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(model.esm_model.alphabet)).float()\n",
        "\n",
        "            protein_embedding = esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "            peptide_embedding = esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            epitope_latent = model.refined_representation(protein_tokens, (energy_scores <= -1).float())\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            loss, clip_loss, ce_loss = model.p_losses(peptide_embedding, protein_embedding, epitope_latent, t, peptide_tokens)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_clip_loss += clip_loss.item()\n",
        "            total_ce_loss += ce_loss.item()\n",
        "            total_diff_loss += (loss - clip_loss - ce_loss).item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_diff_loss = total_diff_loss / len(dataloader)\n",
        "    avg_clip_loss = total_clip_loss / len(dataloader)\n",
        "    avg_ce_loss = total_ce_loss / len(dataloader)\n",
        "\n",
        "    return avg_loss, avg_diff_loss, avg_clip_loss, avg_ce_loss\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
        "    wandb.init(project=\"protein_binding_diffusion\", entity=\"vskavi2003\")\n",
        "    wandb.config.update({\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": train_loader.batch_size\n",
        "    })\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_diff_loss = 0\n",
        "        total_train_clip_loss = 0\n",
        "        total_train_ce_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            protein_tokens = protein_seq.to(device)\n",
        "            peptide_tokens = peptide_seq.to(device)\n",
        "\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(model.esm_model.alphabet)).float()\n",
        "\n",
        "            protein_embedding = esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "            peptide_embedding = esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            epitope_latent = model.refined_representation(protein_tokens, (energy_scores <= -1).float())\n",
        "\n",
        "            # Implement classifier-free guidance\n",
        "            if random.random() < 0.1:  # 10% of the time, remove antigen conditioning\n",
        "                protein_embedding = torch.zeros_like(protein_embedding)\n",
        "                epitope_latent = torch.zeros_like(epitope_latent)\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            loss, clip_loss, ce_loss = model.p_losses(peptide_embedding, protein_embedding, epitope_latent, t, peptide_tokens)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_clip_loss += clip_loss.item()\n",
        "            total_train_ce_loss += ce_loss.item()\n",
        "            total_train_diff_loss += (loss - clip_loss - ce_loss).item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_train_diff_loss = total_train_diff_loss / len(train_loader)\n",
        "        avg_train_clip_loss = total_train_clip_loss / len(train_loader)\n",
        "        avg_train_ce_loss = total_train_ce_loss / len(train_loader)\n",
        "\n",
        "        val_loss, val_diff_loss, val_clip_loss, val_ce_loss = validate(model, val_loader, device)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"train_diff_loss\": avg_train_diff_loss,\n",
        "            \"train_clip_loss\": avg_train_clip_loss,\n",
        "            \"train_ce_loss\": avg_train_ce_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_diff_loss\": val_diff_loss,\n",
        "            \"val_clip_loss\": val_clip_loss,\n",
        "            \"val_ce_loss\": val_ce_loss\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss_plot.png')\n",
        "    wandb.log({\"loss_plot\": wandb.Image('loss_plot.png')})\n",
        "\n",
        "def generate_random_protein_binders(model, num_samples=5, seq_length=100):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Generate random protein sequence\n",
        "    protein_seq = ''.join(random.choice('ACDEFGHIKLMNPQRSTVWY') for _ in range(seq_length))\n",
        "\n",
        "    # Generate random epitope (motif)\n",
        "    epitope_start = random.randint(0, seq_length - 10)\n",
        "    epitope_end = epitope_start + random.randint(5, 10)\n",
        "    epitope_seq = protein_seq[epitope_start:epitope_end]\n",
        "\n",
        "    # Generate binders\n",
        "    generated_binders = model.sample(protein_seq, epitope_seq, num_samples=num_samples)\n",
        "\n",
        "    return protein_seq, epitope_seq, generated_binders\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    energy_scores, protein_seqs, peptide_seqs = zip(*batch)\n",
        "    batch_converter = esm_model.alphabet.get_batch_converter()\n",
        "    _, _, protein_tokens = batch_converter([(i, seq) for i, seq in enumerate(protein_seqs)])\n",
        "    _, _, peptide_tokens = batch_converter([(i, seq) for i, seq in enumerate(peptide_seqs)])\n",
        "\n",
        "    padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
        "\n",
        "    max_seq_length = max(protein_tokens.size(1), peptide_tokens.size(1))\n",
        "\n",
        "    padded_energy_scores = torch.stack([\n",
        "        pad_or_truncate(score.unsqueeze(0), max_seq_length, pad_value=0).squeeze(0)\n",
        "        for score in padded_energy_scores\n",
        "    ])\n",
        "\n",
        "    padded_protein_tokens = pad_or_truncate(protein_tokens, max_seq_length, pad_value=esm_model.alphabet.padding_idx)\n",
        "\n",
        "    padded_peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_length, pad_value=esm_model.alphabet.padding_idx)\n",
        "\n",
        "    return padded_energy_scores, padded_protein_tokens, padded_peptide_tokens\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "    val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "    test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "    train_snp = filter_datasets(train_snp)\n",
        "    val_snp = filter_datasets(val_snp)\n",
        "    test_snp = filter_datasets(test_snp)\n",
        "\n",
        "    train_snp, val_snp, test_snp = train_snp[:32], val_snp[:16], test_snp[:16] # subset code\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ProteinInteractionDataset(train_snp)\n",
        "    val_dataset = ProteinInteractionDataset(val_snp)\n",
        "    test_dataset = ProteinInteractionDataset(test_snp)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=4, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_steps = 1000\n",
        "    model = RefinedLatentDiffusion(esm_model, num_steps, device).to(device)\n",
        "\n",
        "    # Train CLIP model\n",
        "    clip_optimizer = torch.optim.Adam(model.clip_model.parameters(), lr=1e-4)\n",
        "    train_clip(model.clip_model, esm_model, train_loader, val_loader, clip_optimizer, num_epochs=2, device=device)\n",
        "\n",
        "    # Train Latent Diffusion model\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    train_losses, val_losses = train(model, train_loader, val_loader, optimizer, num_epochs=2, device=device)\n",
        "\n",
        "    # Plot losses\n",
        "    plot_losses(train_losses, val_losses)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), 'protein_binder_model.pth')\n",
        "\n",
        "    # Generate examples\n",
        "    print(\"Generating protein binders for random cases:\")\n",
        "    for i in range(3):\n",
        "        protein_seq, epitope_seq, generated_binders = generate_random_protein_binders(model)\n",
        "        print(f\"\\nCase {i+1}:\")\n",
        "        print(f\"Protein sequence: {protein_seq}\")\n",
        "        print(f\"Epitope sequence: {epitope_seq}\")\n",
        "        print(\"Generated binders:\")\n",
        "        for j, binder in enumerate(generated_binders):\n",
        "            print(f\"Binder {j+1}: {binder}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ee6f258085744229aaa9b7e5abd4ee95",
            "0ba730fdc64a46aba15caea30a5abed0",
            "a6b38b4bc4e04d56ac49f970ced17b1f",
            "713cf2003a8945339baddfeac2906198",
            "f3db12ff245f4d9d81f70e237af11b62",
            "0c82f6e0fd3f4de6b8d6d48bfb3b00a7",
            "3507b5e072414af6b724ccfdc6ecaa04",
            "9c838cc883634e55aff24163e57194c1",
            "31ed795314fe40a68664748e114bef5a",
            "29c0c6c0d8a24d1c865e951953374e91",
            "921c561c799d4492b89462a64e296a45",
            "4e4a4dee79f64fafa44c7a674b048cb5",
            "e750425b974a4fdd8478c6eac999c635",
            "3355252a466946edab86b21dda816b37",
            "91d62a089d5f419482d27148c5153345",
            "dabad221f3664809ae9ce10630536910"
          ]
        },
        "id": "HbV8-2KMz-CL",
        "outputId": "70215ba5-5f54-439d-d338-498636cb725a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 32\n",
            "Mismatched lengths: 0\n",
            "Total samples: 16\n",
            "Mismatched lengths: 0\n",
            "Total samples: 16\n",
            "Mismatched lengths: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:vbqs9304) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.037 MB of 0.037 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee6f258085744229aaa9b7e5abd4ee95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>train_ce_loss</td><td>█▁</td></tr><tr><td>train_clip_loss</td><td>▁█</td></tr><tr><td>train_diff_loss</td><td>█▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>val_ce_loss</td><td>█▁</td></tr><tr><td>val_clip_loss</td><td>▁█</td></tr><tr><td>val_diff_loss</td><td>█▁</td></tr><tr><td>val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_ce_loss</td><td>6.07927</td></tr><tr><td>train_clip_loss</td><td>0.69381</td></tr><tr><td>train_diff_loss</td><td>1.9358544173680936e+21</td></tr><tr><td>train_loss</td><td>1.9358544173680936e+21</td></tr><tr><td>val_ce_loss</td><td>7.17994</td></tr><tr><td>val_clip_loss</td><td>0.69336</td></tr><tr><td>val_diff_loss</td><td>1.8219785346410076e+21</td></tr><tr><td>val_loss</td><td>1.8219785346410076e+21</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clean-terrain-14</strong> at: <a href='https://wandb.ai/vskavi2003/protein_binding_diffusion/runs/vbqs9304' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_diffusion/runs/vbqs9304</a><br/> View project at: <a href='https://wandb.ai/vskavi2003/protein_binding_diffusion' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_diffusion</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240815_013300-vbqs9304/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:vbqs9304). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1EBYzKC5cdf8cu4kkYeHgWcmUZAPc6Fli/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/wandb/run-20240815_015637-0wp1l76n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vskavi2003/protein_binding_clip/runs/0wp1l76n' target=\"_blank\">rare-sun-28</a></strong> to <a href='https://wandb.ai/vskavi2003/protein_binding_clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vskavi2003/protein_binding_clip' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_clip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vskavi2003/protein_binding_clip/runs/0wp1l76n' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_clip/runs/0wp1l76n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCLIP Epoch 1/2:   0%|          | 0/16 [00:00<?, ?it/s]<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "CLIP Epoch 1/2: 100%|██████████| 16/16 [00:11<00:00,  1.36it/s]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Epoch 1, Train Loss: 0.6936, Val Loss: 0.6931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCLIP Epoch 2/2:   0%|          | 0/16 [00:00<?, ?it/s]<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "CLIP Epoch 2/2: 100%|██████████| 16/16 [00:11<00:00,  1.39it/s]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Epoch 2, Train Loss: 0.6930, Val Loss: 0.6931\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:0wp1l76n) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31ed795314fe40a68664748e114bef5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>clip_train_loss</td><td>█▁</td></tr><tr><td>clip_val_loss</td><td>▁█</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>clip_train_loss</td><td>0.69296</td></tr><tr><td>clip_val_loss</td><td>0.69312</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rare-sun-28</strong> at: <a href='https://wandb.ai/vskavi2003/protein_binding_clip/runs/0wp1l76n' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_clip/runs/0wp1l76n</a><br/> View project at: <a href='https://wandb.ai/vskavi2003/protein_binding_clip' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_clip</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240815_015637-0wp1l76n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:0wp1l76n). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1EBYzKC5cdf8cu4kkYeHgWcmUZAPc6Fli/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/wandb/run-20240815_015722-m7d7sjba</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vskavi2003/protein_binding_diffusion/runs/m7d7sjba' target=\"_blank\">hardy-waterfall-15</a></strong> to <a href='https://wandb.ai/vskavi2003/protein_binding_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vskavi2003/protein_binding_diffusion' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vskavi2003/protein_binding_diffusion/runs/m7d7sjba' target=\"_blank\">https://wandb.ai/vskavi2003/protein_binding_diffusion/runs/m7d7sjba</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/2:   0%|          | 0/16 [00:00<?, ?it/s]<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "Epoch 1/2: 100%|██████████| 16/16 [00:16<00:00,  1.03s/it]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1470981151833051889664.0000, Val Loss: 1393094320978609045504.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2/2:   0%|          | 0/16 [00:00<?, ?it/s]<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "Epoch 2/2: 100%|██████████| 16/16 [00:16<00:00,  1.02s/it]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n",
            "<ipython-input-32-15ffa2cd4872>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_energy_scores = [F.pad(torch.tensor(score, dtype=torch.float32), (1, 1), value=0) for score in energy_scores]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Train Loss: 1324843187817529999360.0000, Val Loss: 1633061968480628113408.0000\n",
            "Generating protein binders for random cases:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (300x1280 and 34x256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-15ffa2cd4872>\u001b[0m in \u001b[0;36m<cell line: 567>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-15ffa2cd4872>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating protein binders for random cases:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mprotein_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepitope_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_binders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_random_protein_binders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nCase {i+1}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Protein sequence: {protein_seq}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-15ffa2cd4872>\u001b[0m in \u001b[0;36mgenerate_random_protein_binders\u001b[0;34m(model, num_samples, seq_length)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;31m# Generate binders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0mgenerated_binders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepitope_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprotein_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepitope_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_binders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-15ffa2cd4872>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, ag_seq, epitope_seq, num_samples, guidance_scale)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Generate both conditional and unconditional samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mx_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepitope_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mx_uncond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepitope_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-15ffa2cd4872>\u001b[0m in \u001b[0;36mp_sample\u001b[0;34m(self, x, ag_latent, epitope_latent, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Project epitope_latent to match ab_latent dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mepitope_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepitope_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepitope_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Apply self and cross attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (300x1280 and 34x256)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACN1klEQVR4nOzdd3iV9f3/8ec5J3tvZggrCXtvhKBiERFFtCKigrgXWrW/aq0W1G9ta2upKNVWBUexTtDWhajsPQLISoBAIASySEJ2cs75/XHHA0dmckLujNfjurgu8/6c8T4YMS8+9+d9W5xOpxMRERERERHxiNXsBkRERERERJoChSsREREREZE6oHAlIiIiIiJSBxSuRERERERE6oDClYiIiIiISB1QuBIREREREakDClciIiIiIiJ1QOFKRERERESkDihciYiIiIiI1AGFKxGRBmjatGm0b9++Vs+dOXMmFoulbhtqYA4cOIDFYmH+/Pn1/t4Wi4WZM2e6vp4/fz4Wi4UDBw6c97nt27dn2rRpddqPJ98rIiJStxSuRERqwGKxXNCvpUuXmt1qszdjxgwsFgt79+4962OeeuopLBYL27Ztq8fOau7IkSPMnDmT5ORks1tx+Sng/uUvfzG7FRGRBsPL7AZERBqTd9991+3rd955h2+//fa0eteuXT16n3/96184HI5aPfd3v/sdTzzxhEfv3xRMmTKFOXPmsGDBAp555pkzPub999+nZ8+e9OrVq9bvc+utt3LTTTfh6+tb69c4nyNHjjBr1izat29Pnz593NY8+V4REZG6pXAlIlIDt9xyi9vXa9eu5dtvvz2t/nMlJSUEBARc8Pt4e3vXqj8ALy8vvLz0x/vgwYPp3Lkz77///hnD1Zo1a0hLS+OPf/yjR+9js9mw2WwevYYnPPleERGRuqXLAkVE6tioUaPo0aMHmzZtYuTIkQQEBPDb3/4WgM8++4xx48bRunVrfH196dSpE8899xx2u93tNX5+jubUS7D++c9/0qlTJ3x9fRk4cCAbNmxwe+6ZzlxZLBYefPBBFi1aRI8ePfD19aV79+58/fXXp/W/dOlSBgwYgJ+fH506deL111+/4HNcK1as4Je//CXt2rXD19eX2NhYfvWrX1FaWnra5wsKCiIjI4MJEyYQFBREdHQ0jz/++Gm/F/n5+UybNo3Q0FDCwsKYOnUq+fn55+0FjN2r3bt3s3nz5tPWFixYgMViYfLkyVRUVPDMM8/Qv39/QkNDCQwMZMSIEfzwww/nfY8znblyOp08//zztG3bloCAAC699FJ27Nhx2nPz8vJ4/PHH6dmzJ0FBQYSEhDB27Fi2bt3qeszSpUsZOHAgALfffrvr0tOfzpud6cxVcXExjz32GLGxsfj6+pKYmMhf/vIXnE6n2+Nq8n1RW1lZWdxxxx20aNECPz8/evfuzdtvv33a4/7zn//Qv39/goODCQkJoWfPnvz97393rVdWVjJr1izi4+Px8/MjMjKSSy65hG+//dbtdXbv3s0NN9xAREQEfn5+DBgwgM8//9ztMRf6WiIiNaW/2hQRuQhyc3MZO3YsN910E7fccgstWrQAjB/Eg4KCePTRRwkKCuL777/nmWeeobCwkBdffPG8r7tgwQJOnDjBPffcg8Vi4c9//jMTJ05k//79593BWLlyJZ9++in3338/wcHBvPzyy1x//fWkp6cTGRkJwJYtW7jyyitp1aoVs2bNwm638+yzzxIdHX1Bn/ujjz6ipKSE++67j8jISNavX8+cOXM4fPgwH330kdtj7XY7Y8aMYfDgwfzlL39hyZIl/PWvf6VTp07cd999gBFSrr32WlauXMm9995L165dWbhwIVOnTr2gfqZMmcKsWbNYsGAB/fr1c3vvDz/8kBEjRtCuXTtycnJ44403mDx5MnfddRcnTpzgzTffZMyYMaxfv/60S/HO55lnnuH555/nqquu4qqrrmLz5s384he/oKKiwu1x+/fvZ9GiRfzyl7+kQ4cOHDt2jNdff52kpCR27txJ69at6dq1K88++yzPPPMMd999NyNGjABg2LBhZ3xvp9PJNddcww8//MAdd9xBnz59+Oabb/j1r39NRkYGf/vb39wefyHfF7VVWlrKqFGj2Lt3Lw8++CAdOnTgo48+Ytq0aeTn5/Pwww8D8O233zJ58mQuv/xy/vSnPwGwa9cuVq1a5XrMzJkzeeGFF7jzzjsZNGgQhYWFbNy4kc2bN3PFFVcAsGPHDoYPH06bNm144oknCAwM5MMPP2TChAl88sknXHfddRf8WiIiteIUEZFae+CBB5w//6M0KSnJCThfe+210x5fUlJyWu2ee+5xBgQEOMvKyly1qVOnOuPi4lxfp6WlOQFnZGSkMy8vz1X/7LPPnIDzv//9r6v2+9///rSeAKePj49z7969rtrWrVudgHPOnDmu2vjx450BAQHOjIwMVy01NdXp5eV12mueyZk+3wsvvOC0WCzOgwcPun0+wPnss8+6PbZv377O/v37u75etGiRE3D++c9/dtWqqqqcI0aMcALOefPmnbengQMHOtu2beu02+2u2tdff+0EnK+//rrrNcvLy92ed/z4cWeLFi2c06dPd6sDzt///veur+fNm+cEnGlpaU6n0+nMyspy+vj4OMeNG+d0OByux/32t791As6pU6e6amVlZW59OZ3Gv2tfX1+335sNGzac9fP+/Hvlp9+z559/3u1xN9xwg9Nisbh9D1zo98WZ/PQ9+eKLL571MbNnz3YCzvfee89Vq6iocA4dOtQZFBTkLCwsdDqdTufDDz/sDAkJcVZVVZ31tXr37u0cN27cOXu6/PLLnT179nT7b8nhcDiHDRvmjI+Pr9FriYjUhi4LFBG5CHx9fbn99ttPq/v7+7v++cSJE+Tk5DBixAhKSkrYvXv3eV930qRJhIeHu77+aRdj//79533u6NGj6dSpk+vrXr16ERIS4nqu3W5nyZIlTJgwgdatW7se17lzZ8aOHXve1wf3z1dcXExOTg7Dhg3D6XSyZcuW0x5/7733un09YsQIt8/y5Zdf4uXl5drJAuOM00MPPXRB/YBxTu7w4cMsX77cVVuwYAE+Pj788pe/dL2mj48PAA6Hg7y8PKqqqhgwYMAZLyk8lyVLllBRUcFDDz3kdinlI488ctpjfX19sVqN/xXb7XZyc3MJCgoiMTGxxu/7ky+//BKbzcaMGTPc6o899hhOp5OvvvrKrX6+7wtPfPnll7Rs2ZLJkye7at7e3syYMYOioiKWLVsGQFhYGMXFxee8LC8sLIwdO3aQmpp6xvW8vDy+//57brzxRtd/Wzk5OeTm5jJmzBhSU1PJyMi4oNcSEakthavzWL58OePHj6d169ZYLBYWLVpUo+cvXbqUa6+9llatWhEYGEifPn3497//7faYHTt2cP3119O+fXssFguzZ8+uuw8gIqZo06aN64f1U+3YsYPrrruO0NBQQkJCiI6Odg3DKCgoOO/rtmvXzu3rn4LW8ePHa/zcn57/03OzsrIoLS2lc+fOpz3uTLUzSU9PZ9q0aURERLjOUSUlJQGnfz4/P7/TLjc8tR+AgwcP0qpVK4KCgtwel5iYeEH9ANx0003YbDYWLFgAQFlZGQsXLmTs2LFuQfXtt9+mV69erjM40dHRfPHFFxf07+VUBw8eBCA+Pt6tHh0d7fZ+YAS5v/3tb8THx+Pr60tUVBTR0dFs27atxu976vu3bt2a4OBgt/pPEyx/6u8n5/u+8MTBgweJj493Bciz9XL//feTkJDA2LFjadu2LdOnTz/t3Nezzz5Lfn4+CQkJ9OzZk1//+tduI/T37t2L0+nk6aefJjo62u3X73//e8D4Hr+Q1xIRqS2Fq/MoLi6md+/evPrqq7V6/urVq+nVqxeffPIJ27Zt4/bbb+e2227jf//7n+sxJSUldOzYkT/+8Y+0bNmyrloXEROduoPzk/z8fJKSkti6dSvPPvss//3vf/n2229dZ0wuZJz22abSOX82qKCun3sh7HY7V1xxBV988QW/+c1vWLRoEd9++61r8MLPP199TdiLiYnhiiuu4JNPPqGyspL//ve/nDhxgilTprge89577zFt2jQ6derEm2++yddff823337LZZdddlHHnP/hD3/g0UcfZeTIkbz33nt88803fPvtt3Tv3r3exqtf7O+LCxETE0NycjKff/6567zY2LFj3c7WjRw5kn379vHWW2/Ro0cP3njjDfr168cbb7wBnPz+evzxx/n222/P+OunvyQ432uJiNSWBlqcx9ixY895OUx5eTlPPfUU77//Pvn5+fTo0YM//elPjBo1CsA1IewnDz/8MIsXL+bTTz/l6quvBmDgwIGuSVC6N41I07V06VJyc3P59NNPGTlypKuelpZmYlcnxcTE4Ofnd8ab7p7rRrw/2b59OykpKbz99tvcdtttrronE9ji4uL47rvvKCoqctu92rNnT41eZ8qUKXz99dd89dVXLFiwgJCQEMaPH+9a//jjj+nYsSOffvqp26V8P+141LRngNTUVDp27OiqZ2dnn7Yb9PHHH3PppZfy5ptvutXz8/OJiopyfX0hkxpPff8lS5Zw4sQJt92rny47/am/+hAXF8e2bdtwOBxuu1dn6sXHx4fx48czfvx4HA4H999/P6+//jpPP/20KxRFRERw++23c/vtt1NUVMTIkSOZOXMmd955p+v32tvbm9GjR5+3t3O9lohIbWnnykMPPvgga9as4T//+Q/btm3jl7/8JVdeeeU5r+MuKCggIiKiHrsUkYbgpx2CU3cEKioqmDt3rlktubHZbIwePZpFixZx5MgRV33v3r2nndM52/PB/fM5nU63cdo1ddVVV1FVVcU//vEPV81utzNnzpwavc6ECRMICAhg7ty5fPXVV0ycOBE/P79z9r5u3TrWrFlT455Hjx6Nt7c3c+bMcXu9M13ybbPZTtsh+uijj1xng34SGBgIcEEj6K+66irsdjuvvPKKW/1vf/sbFovlgs/P1YWrrrqKo0eP8sEHH7hqVVVVzJkzh6CgINclo7m5uW7Ps1qtrhs7l5eXn/ExQUFBdO7c2bUeExPDqFGjeP3118nMzDytl+zsbNc/n++1RERqSztXHkhPT2fevHmkp6e7Dn8//vjjfP3118ybN48//OEPpz3nww8/ZMOGDbz++uv13a6ImGzYsGGEh4czdepUZsyYgcVi4d13363Xy6/OZ+bMmSxevJjhw4dz3333uX5I79GjB8nJyed8bpcuXejUqROPP/44GRkZhISE8Mknn3h0dmf8+PEMHz6cJ554ggMHDtCtWzc+/fTTGp9HCgoKYsKECa5zV6deEghw9dVX8+mnn3Ldddcxbtw40tLSeO211+jWrRtFRUU1eq+f7tf1wgsvcPXVV3PVVVexZcsWvvrqK7fdqJ/e99lnn+X2229n2LBhbN++nX//+99uO14AnTp1IiwsjNdee43g4GACAwMZPHgwHTp0OO39x48fz6WXXspTTz3FgQMH6N27N4sXL+azzz7jkUcecRteURe+++47ysrKTqtPmDCBu+++m9dff51p06axadMm2rdvz8cff8yqVauYPXu2a2ftzjvvJC8vj8suu4y2bdty8OBB5syZQ58+fVzns7p168aoUaPo378/ERERbNy4kY8//pgHH3zQ9Z6vvvoql1xyCT179uSuu+6iY8eOHDt2jDVr1nD48GHX/cMu5LVERGpD4coD27dvx263k5CQ4FYvLy8/471BfvjhB26//Xb+9a9/0b179/pqU0QaiMjISP73v//x2GOP8bvf/Y7w8HBuueUWLr/8csaMGWN2ewD079+fr776iscff5ynn36a2NhYnn32WXbt2nXeaYbe3t7897//ZcaMGbzwwgv4+flx3XXX8eCDD9K7d+9a9WO1Wvn888955JFHeO+997BYLFxzzTX89a9/pW/fvjV6rSlTprBgwQJatWrFZZdd5rY2bdo0jh49yuuvv84333xDt27deO+99/joo49YunRpjft+/vnn8fPz47XXXuOHH35g8ODBLF68mHHjxrk97re//S3FxcUsWLCADz74gH79+vHFF1+cdom4t7c3b7/9Nk8++ST33nsvVVVVzJs374zh6qffs2eeeYYPPviAefPm0b59e1588UUee+yxGn+W8/n666/PeNPh9u3b06NHD5YuXcoTTzzB22+/TWFhIYmJicybN49p06a5HnvLLbfwz3/+k7lz55Kfn0/Lli2ZNGkSM2fOdF1OOGPGDD7//HMWL15MeXk5cXFxPP/88/z61792vU63bt3YuHEjs2bNYv78+eTm5hITE0Pfvn155plnXI+7kNcSEakNi7Mh/ZVpA2exWFi4cCETJkwA4IMPPmDKlCns2LHjtAPBQUFBbsMpli1bxrhx43jppZe4++67z/oe7du355FHHjnjyF4REbNMmDBBo6tFRETOQztXHujbty92u52srCzXvWbOZOnSpVx99dX86U9/OmewEhFpCEpLS92mHaampvLll1+6TW4TERGR0ylcnUdRUZHblKy0tDSSk5OJiIggISGBKVOmcNttt7kuUcnOzua7776jV69ejBs3jh9++IGrr76ahx9+mOuvv56jR48CxlSkn4ZaVFRUsHPnTtc/Z2RkkJyc7DpgKyJSnzp27Mi0adPo2LEjBw8e5B//+Ac+Pj78v//3/8xuTUREpEHTZYHnsXTpUi699NLT6lOnTmX+/PlUVlby/PPP884775CRkUFUVBRDhgxh1qxZ9OzZk2nTpvH222+f9vykpCTXdfwHDhw443Xzpz5GRKS+3H777fzwww8cPXoUX19fhg4dyh/+8Af69etndmsiIiINmsKViIiIiIhIHdB9rkREREREROqAwpWIiIiIiEgd0ECLM3A4HBw5coTg4GAsFovZ7YiIiIiIiEmcTicnTpygdevWrnvvnY3C1RkcOXKE2NhYs9sQEREREZEG4tChQ7Rt2/acj1G4OoPg4GDA+A0MCQkxuRsRERERETFLYWEhsbGxroxwLgpXZ/DTpYAhISEKVyIiIiIickHHhTTQQkREREREpA4oXImIiIiIiNQBhSsREREREZE6oDNXteR0OqmqqsJut5vdijQxNpsNLy8v3QZAREREpJFRuKqFiooKMjMzKSkpMbsVaaICAgJo1aoVPj4+ZrciIiIiIhdI4aqGHA4HaWlp2Gw2WrdujY+Pj3YYpM44nU4qKirIzs4mLS2N+Pj4896sTkREREQaBoWrGqqoqMDhcBAbG0tAQIDZ7UgT5O/vj7e3NwcPHqSiogI/Pz+zWxIRERGRC6C/Eq8l7SbIxaTvLxEREZHGRz/BiYiIiIiI1AGFKxERERERkTqgcCUead++PbNnzza7DRERERER0ylcNRMWi+Wcv2bOnFmr192wYQN33323R72NGjWKRx55xKPXEBERERExm6YFNhOZmZmuf/7ggw945pln2LNnj6sWFBTk+men04ndbsfL6/zfHtHR0XXbqIiIiIhII6WdqzrgdDopqaiq919Op/OCe2zZsqXrV2hoKBaLxfX17t27CQ4O5quvvqJ///74+vqycuVK9u3bx7XXXkuLFi0ICgpi4MCBLFmyxO11f35ZoMVi4Y033uC6664jICCA+Ph4Pv/8c49+fz/55BO6d++Or68v7du3569//avb+ty5c4mPj8fPz48WLVpwww03uNY+/vhjevbsib+/P5GRkYwePZri4mKP+hERERERORPtXNWB0ko73Z75pt7fd+ezYwjwqbt/hU888QR/+ctf6NixI+Hh4Rw6dIirrrqK//u//8PX15d33nmH8ePHs2fPHtq1a3fW15k1axZ//vOfefHFF5kzZw5Tpkzh4MGDRERE1LinTZs2ceONNzJz5kwmTZrE6tWruf/++4mMjGTatGls3LiRGTNm8O677zJs2DDy8vJYsWIFYOzWTZ48mT//+c9cd911nDhxghUrVtQolIqIiIiIXCiFK3F59tlnueKKK1xfR0RE0Lt3b9fXzz33HAsXLuTzzz/nwQcfPOvrTJs2jcmTJwPwhz/8gZdffpn169dz5ZVX1rinl156icsvv5ynn34agISEBHbu3MmLL77ItGnTSE9PJzAwkKuvvprg4GDi4uLo27cvYISrqqoqJk6cSFxcHAA9e/ascQ8iIiIiIhdC4aoO+Hvb2PnsGFPety4NGDDA7euioiJmzpzJF1984QoqpaWlpKenn/N1evXq5frnwMBAQkJCyMrKqlVPu3bt4tprr3WrDR8+nNmzZ2O327niiiuIi4ujY8eOXHnllVx55ZWuSxJ79+7N5ZdfTs+ePRkzZgy/+MUvuOGGGwgPD69VLyIiIiJST9LXQsFh6HnD+R/bgOjMVR2wWCwE+HjV+y+LxVKnnyMwMNDt68cff5yFCxfyhz/8gRUrVpCcnEzPnj2pqKg45+t4e3uf9vvjcDjqtNefBAcHs3nzZt5//31atWrFM888Q+/evcnPz8dms/Htt9/y1Vdf0a1bN+bMmUNiYiJpaWkXpRcRERER8YDDAbu/hDd/AW+NgS8eg/Iis7uqEYUrOatVq1Yxbdo0rrvuOnr27EnLli05cOBAvfbQtWtXVq1adVpfCQkJ2GzGzp2XlxejR4/mz3/+M9u2bePAgQN8//33gBHshg8fzqxZs9iyZQs+Pj4sXLiwXj+DiIiIiJxDVQVs+Tf8Yyj8ZzIcWgc2H+h2DVSVmd1djeiyQDmr+Ph4Pv30U8aPH4/FYuHpp5++aDtQ2dnZJCcnu9VatWrFY489xsCBA3nuueeYNGkSa9as4ZVXXmHu3LkA/O9//2P//v2MHDmS8PBwvvzySxwOB4mJiaxbt47vvvuOX/ziF8TExLBu3Tqys7Pp2rXrRfkMIiIiIlID5Sdg09uwdi4UZhg13xAYMB2G3AfBLc3trxYUruSsXnrpJaZPn86wYcOIioriN7/5DYWFhRflvRYsWMCCBQvcas899xy/+93v+PDDD3nmmWd47rnnaNWqFc8++yzTpk0DICwsjE8//ZSZM2dSVlZGfHw877//Pt27d2fXrl0sX76c2bNnU1hYSFxcHH/9618ZO3bsRfkMIiIiInIBirJg3Wuw4Q0oKzBqQS2NQDXgdvALNbc/D1icmkt9msLCQkJDQykoKCAkJMRtraysjLS0NDp06ICfn59JHUpTp+8zERERaXLy9sPqOcYlgPZyoxbZGYbNgN43gZevuf2dxbmywc9p50pERERERC6eI1tg5WzY9Tk4q4+YtBkAlzwCiePA2nTGQChciYiIiIhI3XI6Yf8PRqhKW3ay3vkKI1TFDYc6nnzdEChciYiIiIhI3bBXwa7PjFB1dJtRs9igx/Uw/GFo2cPU9i42hSsREREREfFMZSlseQ/WvALHDxg17wDodxsMfQDC2pnaXn1RuBIRERERkdopyYMNbxrT/0pyjJp/BAy+FwbdBQER5vZXzxSuRERERESkZgoOw5q5sGk+VBYbtbB2MPQh6HsL+ASY2p5ZFK5EREREROTCZO2CVX+H7R+Bo8qotehpnKfqfh3Ymne8aN6fXkREREREzu/gGlg1G1K+PllrPwKGPwKdL2+Sk/9qQ+FKRERERERO53BAylfGTtWhddVFC3Qdb4Sqtv3N7K5Bajp37JJ6MWrUKB555BHX1+3bt2f27NnnfI7FYmHRokUev3ddvY6IiIiInENVhTH5b+4Q+M/NRrCy+UC/qfDgRpj0roLVWShcNRPjx4/nyiuvPOPaihUrsFgsbNu2rcavu2HDBu6++25P23Mzc+ZM+vTpc1o9MzOTsWPH1ul7/dz8+fMJCwu7qO8hIiIi0iCVFcLqOfD33vDZA5CzB3xD4JJfwSPb4ZqXIaqz2V02aLossJm44447uP766zl8+DBt27Z1W5s3bx4DBgygV69eNX7d6OjoumrxvFq2bFlv7yUiIiLSbBRlwdp/GCPVywuMWlBLGHo/9L8d/ELM7a8R0c5VXXA6oaK4/n85nRfc4tVXX010dDTz5893qxcVFfHRRx9xxx13kJuby+TJk2nTpg0BAQH07NmT999//5yv+/PLAlNTUxk5ciR+fn5069aNb7/99rTn/OY3vyEhIYGAgAA6duzI008/TWVlJWDsHM2aNYutW7disViwWCyunn9+WeD27du57LLL8Pf3JzIykrvvvpuioiLX+rRp05gwYQJ/+ctfaNWqFZGRkTzwwAOu96qN9PR0rr32WoKCgggJCeHGG2/k2LFjrvWtW7dy6aWXEhwcTEhICP3792fjxo0AHDx4kPHjxxMeHk5gYCDdu3fnyy+/rHUvIiIiIh7J3Qf/fQT+1gNWvmQEq8h4uOYVeGSbMQFQwapGtHNVFypL4A+t6/99f3sEfAIv6KFeXl7cdtttzJ8/n6eeegpL9USXjz76CLvdzuTJkykqKqJ///785je/ISQkhC+++IJbb72VTp06MWjQoPO+h8PhYOLEibRo0YJ169ZRUFDgdj7rJ8HBwcyfP5/WrVuzfft27rrrLoKDg/l//+//MWnSJH788Ue+/vprlixZAkBoaOhpr1FcXMyYMWMYOnQoGzZsICsrizvvvJMHH3zQLUD+8MMPtGrVih9++IG9e/cyadIk+vTpw1133XVBv28//3w/Batly5ZRVVXFAw88wKRJk1i6dCkAU6ZMoW/fvvzjH//AZrORnJyMt7c3AA888AAVFRUsX76cwMBAdu7cSVBQUI37EBEREfFIxmZj8t/Oz4Hqv6xvO9AYUpF4FVi1/1JbClfNyPTp03nxxRdZtmwZo0aNAoxLAq+//npCQ0MJDQ3l8ccfdz3+oYce4ptvvuHDDz+8oHC1ZMkSdu/ezTfffEPr1kbY/MMf/nDaOanf/e53rn9u3749jz/+OP/5z3/4f//v/+Hv709QUBBeXl7nvAxwwYIFlJWV8c477xAYaATMV155hfHjx/OnP/2JFi1aABAeHs4rr7yCzWajS5cujBs3ju+++65W4eq7775j+/btpKWlERsbC8A777xD9+7d2bBhAwMHDiQ9PZ1f//rXdOnSBYD4+HjX89PT07n++uvp2bMnAB07dqxxDyIiIiK14nTCvu+NUJW2/GQ9/hdGqIobpnHqdUDhqi54Bxi7SGa8bw106dKFYcOG8dZbbzFq1Cj27t3LihUrePbZZwGw2+384Q9/4MMPPyQjI4OKigrKy8sJCLiw99m1axexsbGuYAUwdOjQ0x73wQcf8PLLL7Nv3z6KioqoqqoiJKRmW867du2id+/ermAFMHz4cBwOB3v27HGFq+7du2Oz2VyPadWqFdu3b6/Re536nrGxsa5gBdCtWzfCwsLYtWsXAwcO5NFHH+XOO+/k3XffZfTo0fzyl7+kU6dOAMyYMYP77ruPxYsXM3r0aK6//vpanXMTERERuWD2Kti5yAhVR6t/BrJ6QY8bYPgMaNHdzO6aHO351QWLxbg8r75/1eJvF+644w4++eQTTpw4wbx58+jUqRNJSUkAvPjii/z973/nN7/5DT/88APJycmMGTOGioqKOvutWrNmDVOmTOGqq67if//7H1u2bOGpp56q0/c41U+X5P3EYrHgcDguynuBMelwx44djBs3ju+//55u3bqxcOFCAO68807279/Prbfeyvbt2xkwYABz5sy5aL2IiIhIM1ZRAuv/BXP6wSd3GMHKOwAG3wczkmHi6wpWF4HCVTNz4403YrVaWbBgAe+88w7Tp093nb9atWoV1157Lbfccgu9e/emY8eOpKSkXPBrd+3alUOHDpGZmemqrV271u0xq1evJi4ujqeeeooBAwYQHx/PwYMH3R7j4+OD3W4/73tt3bqV4uJiV23VqlVYrVYSExMvuOea+OnzHTp0yFXbuXMn+fn5dOvWzVVLSEjgV7/6FYsXL2bixInMmzfPtRYbG8u9997Lp59+ymOPPca//vWvi9KriIiINFMlebDszzC7B3z5OOQfhIBIuPQp+NUOGPtHCIs9/+tIreiywGYmKCiISZMm8eSTT1JYWMi0adNca/Hx8Xz88cesXr2a8PBwXnrpJY4dO+YWHM5l9OjRJCQkMHXqVF588UUKCwt56qmn3B4THx9Peno6//nPfxg4cCBffPGFa2fnJ+3btyctLY3k5GTatm1LcHAwvr6+bo+ZMmUKv//975k6dSozZ84kOzubhx56iFtvvdV1SWBt2e12kpOT3Wq+vr6MHj2anj17MmXKFGbPnk1VVRX3338/SUlJDBgwgNLSUn79619zww030KFDBw4fPsyGDRu4/vrrAXjkkUcYO3YsCQkJHD9+nB9++IGuXbt61KuIiIgIAPmHYM2rsPkdqKz+y+ewdjBsBvSZAj41O04itaOdq2bojjvu4Pjx44wZM8btfNTvfvc7+vXrx5gxYxg1ahQtW7ZkwoQJF/y6VquVhQsXUlpayqBBg7jzzjv5v//7P7fHXHPNNfzqV7/iwQcfpE+fPqxevZqnn37a7THXX389V155JZdeeinR0dFnHAcfEBDAN998Q15eHgMHDuSGG27g8ssv55VXXqnZb8YZFBUV0bdvX7df48ePx2Kx8NlnnxEeHs7IkSMZPXo0HTt25IMPPgDAZrORm5vLbbfdRkJCAjfeeCNjx45l1qxZgBHaHnjgAbp27cqVV15JQkICc+fO9bhfERERacaO7YRP74GX+8C6fxjBqkVPuP5NeGgLDLpLwaoeWZzOGtwsqZkoLCwkNDSUgoKC0wYtlJWVkZaWRocOHfDz8zOpQ2nq9H0mIiIiZ+V0wsHVsOrvkPrNyXqHkcbkv06XafJfHTpXNvg5XRYoIiIiItIYOByw50tj8t/hDdVFC3S7xrjhb5v+ZnYnKFyJiIiIiDRsVeWw7UNY/TLkVA8bs/lCn5th2EMQ2cnc/sRF4UpEREREpCEqK4RN82DtP+BE9TRm31AYeAcMvheCPRviJXVP4UpEREREpCE5ccwYTrHhLSgvMGrBrWDI/dB/Gvid+9yPmEfhqpY0B0QuJn1/iYiINEO5+4xL/5LfB3u5UYtKMM5T9fwlePme+/liOoWrGvL29gagpKQEf39/k7uRpqqkpAQ4+f0mIiIiTVjGJlg5G3b9F6j+C9a2g+CSRyBhLFh196TGQuGqhmw2G2FhYWRlZQHG/ZYsGnUpdcTpdFJSUkJWVhZhYWHYbDazWxIREZGLwemEfd8ZoerAipP1hCuNnap2QzVOvRFSuKqFli1bArgClkhdCwsLc32fiYiISBNir4IdC417VB3bbtSsXsZlf8NmQItu5vYnHlG4qgWLxUKrVq2IiYmhsrLS7HakifH29taOlYiISFNTUQJb3oM1cyA/3ah5B0L/qcagirBYc/uTOqFw5QGbzaYfgkVERETk7EryYP0/Yd3rUJpn1AKijFHqA++AgAhz+5M6pXAlIiIiIlLX8tNhzauw+R2oNAZVERZn3PS37y3grcFoTZHClYiIiIhIXTm2wzhPtf1jcNqNWstexpCKbhPAph+/mzL92xURERER8YTTCQdXGZP/9n57st4hyRin3vFSTf5rJhSuRERERERqw+GAPV8YoSpjo1GzWKHrNcZOVZt+prYn9U/hSkRERESkJqrKYet/YPXLkLvXqNl8oe8UGPogRHYytz8xjcKViIiIiMiFKCuAjfNg7T+g6KhR8wuFgXca0/+CYsztT0xnNfPNly9fzvjx42ndujUWi4VFixad9znl5eU89dRTxMXF4evrS/v27XnrrbfcHvPRRx/RpUsX/Pz86NmzJ19++eVF+gQiIiIi0uSdOArf/h7+1gOW/N4IVsGt4Rf/B7/aAZc/o2AlgMk7V8XFxfTu3Zvp06czceLEC3rOjTfeyLFjx3jzzTfp3LkzmZmZOBwO1/rq1auZPHkyL7zwAldffTULFixgwoQJbN68mR49elysjyIiIiIiTU3OXlj9d+MSQHuFUYtKNM5T9fwlePmY2580OBan0+k0uwkAi8XCwoULmTBhwlkf8/XXX3PTTTexf/9+IiLOfMO1SZMmUVxczP/+9z9XbciQIfTp04fXXnvtgnopLCwkNDSUgoICQkJCavQ5RERERKSRO7wJVv0Ndv0PqP5ROXYwDH8EEq4Eq6kXf0k9q0k2aFTfGZ9//jkDBgzgz3/+M23atCEhIYHHH3+c0tJS12PWrFnD6NGj3Z43ZswY1qxZc9bXLS8vp7Cw0O2XiIiIiDQjTiekfgvzr4Y3LoNd/wWckDAWpn8DdyyGLlcpWMk5NaqBFvv372flypX4+fmxcOFCcnJyuP/++8nNzWXevHkAHD16lBYtWrg9r0WLFhw9evSsr/vCCy8wa9asi9q7iIiIiDRA9irY8alx499jPxo1qxf0vBGGz4CYrub2J41KowpXDocDi8XCv//9b0JDQwF46aWXuOGGG5g7dy7+/v61et0nn3ySRx991PV1YWEhsbGxddKziIiIiDRAFcWw5T1Y/QoUpBs1nyDoPw2G3AehbU1tTxqnRhWuWrVqRZs2bVzBCqBr1644nU4OHz5MfHw8LVu25NixY27PO3bsGC1btjzr6/r6+uLr63vR+hYRERGRBqI4F9b/0/hVmmfUAqJgyL3GSHX/cHP7k0atUV00Onz4cI4cOUJRUZGrlpKSgtVqpW1b428Xhg4dynfffef2vG+//ZahQ4fWa68iIiIi0oAcPwhf/j+Y3QOW/dEIVuHtYdxf4Vc/wshfK1iJx0zduSoqKmLv3r2ur9PS0khOTiYiIoJ27drx5JNPkpGRwTvvvAPAzTffzHPPPcftt9/OrFmzyMnJ4de//jXTp093XRL48MMPk5SUxF//+lfGjRvHf/7zHzZu3Mg///lPUz6jiIiIiJjo6I+wajb8+Ck47UatVW9j8l+3a8FqM7M7aWJMDVcbN27k0ksvdX3907mnqVOnMn/+fDIzM0lPT3etBwUF8e233/LQQw8xYMAAIiMjufHGG3n++eddjxk2bBgLFizgd7/7Hb/97W+Jj49n0aJFuseViIiISHPhdMKBlUao2rvkZL3jKCNUdRwFFos5vUmT1mDuc9WQ6D5XIiIiIo2Qww67vzBCVcYmo2axQrcJxo1/W/cxsTlprGqSDRrVQAsRERERkdNUlcPW92H1HMitPnLi5Qd9psCwByGio7n9SbOhcCUiIiIijVNZAWx8C9b+A4qqp0X7hcLAu2DwvRAUbW5/0uwoXImIiIhI41KYCWvnwsZ5UHHCqIW0gaEPQL/bwDfY3P6k2VK4EhEREZHGIScVVv0dtn0A9gqjFt3FOE/V4wbw8jG3P2n2FK5EREREpGE7vBFW/s0YVkH1LLbYIXDJIxA/BqyN6tat0oQpXImIiIhIw+N0Quq3xk7VwZUn64lXGTtV7YaY15vIWShciYiIiEjDYa80bvi76u+QtcOoWb2h1yQY9hDEdDG3P5FzULgSEREREfNVFMPmd2DNq1BwyKj5BEH/aTDkfghtY2p7IhdC4UpEREREzFOcC+tfh/X/hNLjRi0w2hilPvAO8A83tz+RGlC4EhEREZH6d/wgrHkFNr8LVaVGLbyDcelfn5vB29/c/kRqQeFKREREROrP0e3GeaofPwWn3ai16mNM/ut6DVhtZnYn4hGFKxERERG5uJxOOLACVs6Gfd+drHe81AhVHZLAYjGrO5E6o3AlIiIiIheHww67/mvsVB3ZbNQsVuh+HQybAa37mNqeSF1TuBIRERGRulVZBlvfh9VzIG+fUfPyg763wNAHIaKDuf2JXCQKVyIiIiJSN0rzYeNbsPYfUJxl1PzCYNBdMOgeCIo2szuRi07hSkREREQ8U3gE1s6FjfOh4oRRC2kLQx+AfreBb5Cp7YnUF4UrEREREamd7BRY/XfY+gE4Ko1adFcY/jD0vAFs3ub2J1LPFK5EREREpGYOrTeGVOz+AnAatXbDjMl/na8Aq9XM7kRMo3AlIiIiIufndELqYmOcevrqk/XEcUaoih1kVmciDYbClYiIiIicnb0SfvzE2KnK2mnUrN7Qe5IxTj060dz+RBoQhSsREREROV15EWx+B9a8CoWHjZpPMAyYBkPuh5DWprYn0hApXImIiIjIScU5sO51WP9PKMs3aoExMOReGHAH+IeZ2Z1Ig6ZwJSIiIiKQl2bsUm15D6pKjVpER+PSv96TwdvP3P5EGgGFKxEREZHmLHMbrJoNOxaC02HUWveF4Y9A1/FgtZnZnUijonAlIiIi0tw4nZC23AhV+74/We90uXGPqg4jwWIxrT2RxkrhSkRERKS5cNhh1+fG5L8jW4yaxQrdJxqhqlUvc/sTaeQUrkRERESausoy2LoAVs+BvP1Gzcsf+t4Cwx6E8PamtifSVChciYiIiDRVpfmw8U1Y+xoUZxk1/3AYdLfxKzDK1PZEmhqFKxEREZGmpvCIMflv03yoKDJqIW2NXaq+t4JvkKntiTRVClciIiIiTUX2Hlj1Mmz7AByVRi2mm3Geqsf1YPM2tz+RJk7hSkRERKSxS19nTP7b8+XJWtxwY5x6/BWa/CdSTxSuRERERBojhwNSFxuhKn1NddECXcYZoSp2oInNiTRPClciIiIijUlVBfz4sXH5X/Yuo2b1ht43wbAZEJ1gbn8izZjClYiIiEhjUF4Em982BlUUZhg1n2AYcDsMuR9CWpnbn4goXImIiIg0aEXZsP51WP8vKMs3aoExMOQ+GDAd/MPM7E5ETqFwJSIiItIQ5aUZN/1N/jdUlRm1iE4wfAb0ugm8/cztT0ROo3AlIiIi0pBkboWVs2HnInA6jFrrfnDJI9DlarDaTGxORM5F4UpERETEbE4n7F8Kq/4O+384We882pj81/4SjVMXaQQUrkRERETM4rDDzs+MUJWZbNQsNugx0bjxb8ueprYnIjWjcCUiIiJS3ypLIXmBcabqeJpR8/KHfrfB0AcgPM7c/kSkVhSuREREROpL6XHY8Aasex2Ks42afzgMugcG3Q2Bkeb2JyIeUbgSERERudgKMmDtXNg0HyqKjFpoLAx9EPrdCj6BprYnInVD4UpERETkYsnaDatfhm0fgqPSqMV0Nyb/db8ObN6mticidUvhSkRERKSupa81xqmnfHWyFneJEao6j9bkP5EmSuFKREREpC44HJD6jRGqDq2tLlqg69XGOPW2A0xsTkTqg8KViIiIiCeqKmD7R8blf9m7jZrNB3rfBMNmQFS8uf2JSL1RuBIRERGpjfITsOltY1BFYYZR8w2BAbfD4PsgpJW5/YlIvVO4EhEREamJoixjlPqGf0FZgVELagFD7jeClV+ouf2JiGkUrkREREQuRN5+46a/yQugqsyoRXY2Lv3rfRN4+Zrbn4iYTuFKRERE5FyObIFVf4edn4HTYdTa9DeGVHQZB1abqe2JSMOhcCUiIiLyc04n7P/BmPyXtuxkvfMVxjj1uOEapy4ip1G4EhEREfmJvQp2fWbsVGVuNWoWG/S4HoY/DC17mNufiDRoClciIiIilaWw5T1Y8wocP2DUvAOg320w9AEIa2dqeyLSOChciYiISPNVkgcb3oR1r0FJjlHzj4DB98DAuyAw0tz+RKRRUbgSERGR5qfgMKyZC5vmQ2WxUQttB8MehL63gE+gqe2JSOOkcCUiIiLNR9Yu4zzV9o/AUWXUWvQwJv91nwA2bzO7E5FGTuFKREREmr6Da2DVbEj5+mSt/QgjVHW+XJP/RKROWM188+XLlzN+/Hhat26NxWJh0aJF53z80qVLsVgsp/06evSo6zEzZ848bb1Lly4X+ZOIiIhIg+NwwO4v4M1fwLwrq4OVBbpeA3d+D9P+B/GjFaxEpM6YunNVXFxM7969mT59OhMnTrzg5+3Zs4eQkBDX1zExMW7r3bt3Z8mSJa6vvby0QSciItJsVFXA9g9h1cuQs8eo2Xyg92QYNgOiOpvbn4g0WaamjrFjxzJ27NgaPy8mJoawsLCzrnt5edGyZUsPOhMREZFGp/yEMaBizVw4ccSo+YbAgOkw5D4I1s8GInJxNcotnT59+lBeXk6PHj2YOXMmw4cPd1tPTU2ldevW+Pn5MXToUF544QXatTv7/SnKy8spLy93fV1YWHjRehcREZE6VpRljFLf8AaUFRi1oJYw9H7ofzv4hZz7+SIidaRRhatWrVrx2muvMWDAAMrLy3njjTcYNWoU69ato1+/fgAMHjyY+fPnk5iYSGZmJrNmzWLEiBH8+OOPBAcHn/F1X3jhBWbNmlWfH0VEREQ8lbsPVs+B5AVgr/5L0sh4GD4Dek0CL19z+xORZsfidDqdZjcBYLFYWLhwIRMmTKjR85KSkmjXrh3vvvvuGdfz8/OJi4vjpZde4o477jjjY860cxUbG0tBQYHb2S4RERFpADI2G5P/dn4OVP8Y02YAXPIIJI4Dq6nzukSkiSksLCQ0NPSCskGj2rk6k0GDBrFy5cqzroeFhZGQkMDevXvP+hhfX198ffW3WyIiIg2W0wn7vjdCVdryk/X4Xxjj1OOGaeqfiJiu0Yer5ORkWrVqddb1oqIi9u3bx6233lqPXYmIiEidsFfBzkXGjX+PbjNqFhv0vAGGPwwtupvanojIqUwNV0VFRW47SmlpaSQnJxMREUG7du148sknycjI4J133gFg9uzZdOjQge7du1NWVsYbb7zB999/z+LFi12v8fjjjzN+/Hji4uI4cuQIv//977HZbEyePLneP5+IiIjUUkUJJP/bOFOVf9CoeQdAv6nGoIqwsw+qEhExi6nhauPGjVx66aWurx999FEApk6dyvz588nMzCQ9Pd21XlFRwWOPPUZGRgYBAQH06tWLJUuWuL3G4cOHmTx5Mrm5uURHR3PJJZewdu1aoqOj6++DiYiISO2U5BlT/9a9BiW5Ri0gEgbdA4PugoAIc/sTETmHBjPQoiGpyaE1ERERqQP5h2DNq7D5HagsNmph7Yyb/vaZAj4B5vYnIs1WsxpoISIiIo3YsZ3GeaofPwZHlVFr0dOY/NdtAtj0o4qINB76E0tERETql9MJ6Wtg5WxI/eZkvf0II1R1ulyT/0SkUVK4EhERkfrhcMCeL42dqsPrq4sW6HaNMfmvTX9T2xMR8ZTClYiIiFxcVeWw7UNY/TLkpBg1my/0mWycqYrsZG5/IiJ1ROFKRERELo6yQtg0H9bOhROZRs03FAbeAYPvheAWprYnIlLXFK5ERESkbp04Buv+ARvegvICoxbcCobcD/2ngZ8m8YpI06RwJSIiInUjd59x6V/y+2AvN2pRCcalf71uBC9fc/sTEbnIFK5ERETEMxmbjMl/u/4LVN8+s+0gY/JfwliwWk1sTkSk/ihciYiISM05nbDvOyNUHVhxsh4/xghV7YZqnLqINDsKVyIiInLh7FWwcxGsmg1Htxs1qxf0/KVx+V+LbmZ2JyJiKoUrEREROb+KEtjyHqyZA/npRs07EPpPNQZVhMWa25+ISAOgcCUiIiJnV5IH6/8F61+HklyjFhBpjFIfeCcERJjbn4hIA6JwJSIiIqfLT4c1r8Lmd6CyxKiFxcGwh6DPFPAJMLc/EZEGSOFKRERETjq2A1b9HbZ/DE67UWvZE4Y/At0mgE0/OoiInI3+hBQREWnunE44uMoIVamLT9Y7JBmT/zpeqsl/IiIXQOFKRESkuXI4YM8Xxjj1jI1GzWKFrtfA8IehTT9T2xMRaWwUrkRERJqbqnLY9gGsehlyU42azRf63GycqYrsZG5/IiKNlMKViIhIc1FWABvnwdp/QNFRo+YXakz9G3wvBMWY25+ISCOncCUiItLUnThqBKqNb0F5oVELbg1D74f+08A32NT2RESaCoUrERGRpipnL6x+Gba+D/YKoxaVaJyn6vlL8PIxtz8RkSZG4UpERKSpObwJVv0Ndv0PcBq12MHGOPWEK8FqNbM7EZEmS+FKRESkKXA6Ye93sGo2HFhxsp4w1tipihtqWmsiIs2FwpWIiEhjZq+CHZ8a96g69qNRs3pBzxth+AyI6WpufyIizYjClYiISGNUUQxb3oPVr0BBulHzDjQGVAy9H0LbmtqeiEhzpHAlIiLSmBTnwoZ/wbrXoTTPqAVEGaPUB94BARHm9ici0owpXImIiDQG+enGLtWWd6GyxKiFtzdu+ttnCnj7m9qeiIgoXImIiDRsR380zlP9+Ak47UatZS+45BHoei3Y9L9yEZGGQn8ii4iINDROJxxYaUz+27vkZL3jKGOcesdRYLGY05uIiJyVwpWIiEhD4bDD7i+MUJWxyahZrNDtWmOceuu+prYnIiLnpnAlIiJitqpy2Po+rJ4DuXuNms0X+k4xzlRFdDS3PxERuSAKVyIiImYpK4CNb8Haf0DRMaPmFwoD74LB90BQjLn9iYhIjShciYiI1LcTR2HtXNg4D8oLjVpwaxj6APSfCr7B5vYnIiK1onAlIiJSX3JSjcl/2z4Ae4VRi+5inKfqcQN4+Zjbn4iIeEThSkRE5GI7vBFW/s0YVoHTqMUOMcapx48Bq9XM7kREpI4oXImIiFwMTiekfmvsVB1cebKeeJWxU9VuiHm9iYjIRaFwJSIiUpfslfDjp0aoytph1Kze0OtGGDYDYrqY25+IiFw0ClciIiJ1oaIYNr8La16BgkNGzScI+k+DIfdDaBtT2xMRkYtP4UpERMQTxbmw/nVY/08oPW7UAqNh8L0w8A7wDze3PxERqTcKVyIiIrVx/KCxS7X5XagqNWrhHYyb/va5Gbz9ze1PRETqncKViIhITRzdbpyn+vFTcNqNWqs+xuS/rteA1WZmdyIiYiKFKxERkfNxOuHAClg5G/Z9d7Le8VIjVHVIAovFrO5ERKSBULgSERE5G4cddv/PCFVHNhs1ixW6TTDGqbfuY2JzIiLS0ChciYiI/FxlGWx9H1bPgbx9Rs3LD/reAkMfhIgO5vYnIiINksKViIjIT0rzYeNbsO41KDpm1PzCYNBdMOgeCIo2szsREWngFK5EREQKM2HtXNg4DypOGLWQNjD0Aeg3FXyDzO1PREQaBYUrERFpvrJTYPXfYesH4Kg0atFdjfNUPW8Am7e5/YmISKOicCUiIs3PofXGOPXdXwBOo9ZuKAx/BOJ/AVarmd2JiEgjpXAlIiLNg9MJqYuNyX/pq0/WE8cZO1XtBpvWmoiINA0KVyIi0rTZK+HHT4ydqqydRs3qDb0mwfAZEJ1obn8iItJkKFyJiEjTVF4Em9+BNa9C4WGj5hMEA26HwfdBaBtz+xMRkSZH4UpERJqW4hxY9zps+BeUHjdqgTEw5F4YcAf4h5nanoiINF0KVyIi0jQcPwCrX4Et70FVqVGL6AjDHoLeN4O3n6ntiYhI06dwJSIijVvmNuM81Y6F4LQbtdZ9jcl/XceD1WZqeyIi0nwoXImISOPjdELaclg1G/Z9f7Le6TIjVHUYCRaLWd2JiEgzpXAlIiKNh8MOuz43dqqObDFqFit0n2hM/mvV29z+RESkWVO4EhGRhq+yDLYugNVzIG+/UfPyg763wrAHIby9qe2JiIiAwpWIiDRkpfmw8U1Y+xoUZxk1vzAYdDcMvgcCo8zsTkRExI3VzDdfvnw548ePp3Xr1lgsFhYtWnTOxy9duhSLxXLar6NHj7o97tVXX6V9+/b4+fkxePBg1q9ffxE/hYiI1LnCI/DNU/C37vDds0awCmkLV/4RfrUDLntKwUpERBocU3euiouL6d27N9OnT2fixIkX/Lw9e/YQEhLi+jomJsb1zx988AGPPvoor732GoMHD2b27NmMGTOGPXv2uD1OREQaoOw9sOpl2PYBOCqNWkw3GP4w9LgebN7m9iciInIOpoarsWPHMnbs2Bo/LyYmhrCwsDOuvfTSS9x1113cfvvtALz22mt88cUXvPXWWzzxxBOetCsiIhdL+jpj8t+eL0/W4oYboSr+F5r8JyIijUKjPHPVp08fysvL6dGjBzNnzmT48OEAVFRUsGnTJp588knXY61WK6NHj2bNmjVnfb3y8nLKy8tdXxcWFl685kVExOBwQOpiI1Sln/JndJerjXHqsQPN6kxERKRWGlW4atWqFa+99hoDBgygvLycN954g1GjRrFu3Tr69etHTk4OdrudFi1auD2vRYsW7N69+6yv+8ILLzBr1qyL3b6IiADYK2H7x8Y49exdRs3qDb0nwbCHITrB3P5ERERqqVGFq8TERBITE11fDxs2jH379vG3v/2Nd999t9av++STT/Loo4+6vi4sLCQ2NtajXkVE5GfKi2Dz27BmLhQeNmo+wTDgdhhyP4S0Mrc/ERERDzWqcHUmgwYNYuXKlQBERUVhs9k4duyY22OOHTtGy5Ytz/oavr6++Pr6XtQ+RUSaraJsWP86rP8XlOUbtcAYGHIfDJgO/mFmdiciIlJnGn24Sk5OplUr4287fXx86N+/P9999x0TJkwAwOFw8N133/Hggw+a2KWISDOUlwZrXoEt70FVmVGL6ATDZ0Cvm8Dbz9z+RERE6pip4aqoqIi9e/e6vk5LSyM5OZmIiAjatWvHk08+SUZGBu+88w4As2fPpkOHDnTv3p2ysjLeeOMNvv/+exYvXux6jUcffZSpU6cyYMAABg0axOzZsykuLnZNDxQRkYsscyusnA07F4HTYdRa94NLHjGGVVhtJjYnIiJy8ZgarjZu3Mill17q+vqnc09Tp05l/vz5ZGZmkp6e7lqvqKjgscceIyMjg4CAAHr16sWSJUvcXmPSpElkZ2fzzDPPcPToUfr06cPXX3992pALERGpQ04npC0zQtX+H07WO11uhKr2IzROXUREmjyL0+l0mt1EQ1NYWEhoaCgFBQVuNysWEZGfcdhh52fG5L/MZKNmsUGPicY9qlr2NLU9ERERT9UkGzT6M1ciImKCylJIXgCr58DxNKPm5Q/9boWhD0B4e1PbExERMYPClYiIXLjS47DhTVj3GhRnGzX/cBh0t/ErMMrc/kREREykcCUiIudXkAFr58Km+VBRZNRCY2Hog8ZulU+gqe2JiIg0BApXIiJydlm7YfXLsO1DcFQatZjuxnmqHhPB5m1ufyIiIg2IwpWIiJwufa0x+S/lq5O1uEuMyX+dR2vyn4iIyBkoXImIiMHhgNRvjFB1aG110QJdxsElv4K2A8zsTkREpMFTuBIRae6qKuDHj41x6tm7jZrNB3pNMi7/i4o3tz8REZFGQuFKRKS5Kj8Bm942BlUUZhg13xAYcDsMvg9CWpnbn4iISCOjcCUi0twUZRuj1Df8C8oKjFpQCxhyHwyYDn6h5vYnIiLSSClciYg0F3n7jZv+Ji+AqjKjFtkZhs2A3jeBl6+5/YmIiDRytQpXhw4dwmKx0LZtWwDWr1/PggUL6NatG3fffXedNigiIh46kgyrZsPOz8DpMGpt+sPwR4xhFVabic2JiIg0HbUKVzfffDN33303t956K0ePHuWKK66ge/fu/Pvf/+bo0aM888wzdd2niIjUhNMJ+38whlTsX3qy3nm0EaraX6Jx6iIiInWsVuHqxx9/ZNCgQQB8+OGH9OjRg1WrVrF48WLuvfdehSsREbPYq2DXZ0aoytxq1Cw26HG9MfmvZQ9z+xMREWnCahWuKisr8fU1rs1fsmQJ11xzDQBdunQhMzOz7roTEZELU1kKyf82zlQdP2DUvPyh320w9AEIjzO1PRERkeagVuGqe/fuvPbaa4wbN45vv/2W5557DoAjR44QGRlZpw2KiMg5lB6H9W8Y0/9KcoyafwQMvgcG3gWB+jNZRESkvtQqXP3pT3/iuuuu48UXX2Tq1Kn07t0bgM8//9x1uaCIiFxEBYdhzVzYNB8qi41aaDsY9iD0vQV8Ak1tT0REpDmyOJ1OZ22eaLfbKSwsJDw83FU7cOAAAQEBxMTE1FmDZigsLCQ0NJSCggJCQkLMbkdE5KSsXbDqZdj+ITiqjFqLHsZ5qu7Xgc3b3P5ERESamJpkg1rtXJWWluJ0Ol3B6uDBgyxcuJCuXbsyZsyY2rykiIicy8E1xjj1lK9P1tqPMCb/db5ck/9EREQagFqFq2uvvZaJEydy7733kp+fz+DBg/H29iYnJ4eXXnqJ++67r677FBFpfhwOI0ytmg2H1lUXLdD1ahj+K2jb38zuRERE5GestXnS5s2bGTFiBAAff/wxLVq04ODBg7zzzju8/PLLddqgiEizU1UBW96DuUPgP5ONYGXzgX5T4cENMOk9BSsREZEGqFY7VyUlJQQHBwOwePFiJk6ciNVqZciQIRw8eLBOGxQRaTbKTxgDKtbMhRNHjJpvCAyYDkPug+CWprYnIiIi51arcNW5c2cWLVrEddddxzfffMOvfvUrALKysjQAQkSkpoqyjFHqG96AsgKjFtTSCFQDbge/UHP7ExERkQtSq3D1zDPPcPPNN/OrX/2Kyy67jKFDhwLGLlbfvn3rtEERkSYrd59x09/kBWAvN2qR8TB8BvSaBF6+5vYnIiIiNVLrUexHjx4lMzOT3r17Y7UaR7fWr19PSEgIXbp0qdMm65tGsYvIRXVkC6ycDbs+B6fDqLUZAJc8AonjwFqr47AiIiJyEVz0UewALVu2pGXLlhw+fBiAtm3b6gbCIiJn43TCvu+NyX9py0/W439hjFOPG6Zx6iIiIo1crf561OFw8OyzzxIaGkpcXBxxcXGEhYXx3HPP4XA46rpHEZHGy14F2z+G10fCexONYGWxGZf93bcapnwE7YcrWImIiDQBtdq5euqpp3jzzTf54x//yPDhwwFYuXIlM2fOpKysjP/7v/+r0yZFRBqdylJjnPrqOZBfPUXVOwD63QZDH4Cwdub2JyIiInWuVmeuWrduzWuvvcY111zjVv/ss8+4//77ycjIqLMGzaAzVyJSayV5xtS/da9DSY5RC4iEQffAoLsgIMLc/kRERKRGLvqZq7y8vDMOrejSpQt5eXm1eUkRkcYt/xCsnQub3obKYqMW1g6GPgR9bwGfAHP7ExERkYuuVuGqd+/evPLKK7z88stu9VdeeYVevXrVSWMiIo3CsZ2w6u/w48fgqDJqLXoak/+6TQBbrecGiYiISCNTq//r//nPf2bcuHEsWbLEdY+rNWvWcOjQIb788ss6bVBEpMFxOiF9jTFOPfWbk/X2I4xQ1elyDagQERFphmo1LTApKYmUlBSuu+468vPzyc/PZ+LEiezYsYN33323rnts1r7ffYx/LN3HziOF1PKWZCJSVxwO2P0FvPkLmDe2OlhZoNu1cNf3MO1/0Hm0gpWIiEgzVeubCJ/J1q1b6devH3a7va5e0hQNaaDFPe9u5JsdxwCICfYlKSGaUYkxXNI5itAAb1N7E2k2qsph24ew+mXISTFqNh/oczMMmwGRncztT0RERC6aermJsNSPy7u2oNLuZM2+XLJOlPPRpsN8tOkwVgv0bRfOqIRokhKj6dE6FKtVf1suUqfKCmHTfGNQxYlMo+YbCgOnw+D7ILiFqe2JiIhIw6KdqzNoSDtXPymrtLPxwHGW7sliWUo2qVlFbuuRgT6MTIgmKSGaEfFRRAb5mtSpSBNw4his+wdseAvKC4xacCsYcj/0nwZ+DePPBREREbn4apINFK7OoCGGq5/LyC9l2Z5slqVksWpvLkXlVa41iwV6tQklKSGapMQY+sSGYdOulsj55e4zLv1Lfh/s5UYtMh6GPwy9bgQv/aWFiIhIc3PRwtXEiRPPuZ6fn8+yZcsUrupZpd3BpoPHWZaSzdI92ezKLHRbD/X35pL4KOMSwoRoYkL8TOpUpIHK2GSMU9/5OVD9R2LbgTD8EUi8Cqy1mv0jIiIiTcBFC1e33377BT1u3rx5F/qSDVJjC1c/d6ywjOUp2SxNyWZFSjaFZVVu611bhTAq0Qha/ePC8bbpB0dphpxO2PedMU79wIqT9fgxxjj1dkM19U9ERETMuyywqWjs4epUVXYHWw8XsKz6rNa2jAJO/Tce5OvF8M6RJCXEkJQYTZswf/OaFakP9irYuQhWzYaj242a1Qt63ADDZ0CL7mZ2JyIiIg2MwpWHmlK4+rnconJWpOawLCWb5SnZ5BZXuK3HxwS5xr0P7BCOr5fNpE5F6lhFCWx5D9bMgfx0o+YdCP2nGoMqwmLN7U9EREQaJIUrDzXlcHUqh8PJj0cKWLbHuIRwS/pxHKd8N/h72xjaKdJ1CWFcZKB5zYrUVkkerP8XrH8dSnKNWkAkDL4XBt4JARHm9iciIiINmsKVh5pLuPq5gpJKVu7NcY17zzpR7rbePjKAUYkxJCVEM6RjJP4+2tWSBiw/Hda8CpvfgcoSoxYWB8Megj5TwCfA3P5ERESkUVC48lBzDVencjqd7D56gqXV4943HjhO1SnbWj5eVgZ3iKi+hDCaTtFBWHT4XxqCYzuMyX/bPwZn9eTSlj2NyX/dJoBN904XERGRC6dw5SGFq9OdKKtk9b5clqVks2xPNhn5pW7rbcL8Saq+fHB45yiCfPUDrNQjpxMOrjaGVKQuPlnvMNIIVZ0u0+Q/ERERqRWFKw8pXJ2b0+lkX3ZR9a5WNuv251Fhd7jWvawWBrQPd11C2KVlsHa15OJwOGDPF8ZO1eEN1UULdLvWuPFvm36mticiIiKNn8KVhxSuaqakoop1+/NcZ7UO5Ja4rbcI8SUpIZqkhBgu6RxFaIC3SZ1Kk1FVDts+gFUvQ26qUbP5Qp+bjTNVkZ3M7U9ERESaDIUrDylceeZATrFx+WBKNqv35VBWeXJXy2a10Dc2zAhbidH0aB2K1apdLblAZYWw8S1Y+w8oOmrU/EKNqX+D7oHgFub2JyIiIk2OwpWHFK7qTlmlnQ0H8lzj3vdmFbmtRwb6MLJ6KMaI+GgiAn1M6lQatBNHjUC18S0oLzRqwa1h6P3Qfxr4BpvanoiIiDRdClceUri6eA4fL2F5ijHufdXeHIor7K41iwV6tQklqfqsVp/YMGza1WrecvbC6pdh6/tgr77hdVSCcZ6q543gpTAuIiIiF5fClYcUrupHRZWDzenHXYMxdmUWuq2H+nszIj6q+rxWNDEhfiZ1KvXu8CZj8t+u/wLVf0TFDjYm/yVcCVaric2JiIhIc6Jw5SGFK3McKyxzndVakZJNYVmV23q3ViGuce/948LxtukH7CbF6YS93xmh6sCKk/WEK41QFTfUrM5ERESkGVO48pDClfmq7A62Hs53ndXadrjAbT3I14vhnSNd495bh/mb1Kl4zF4FOxYa49SPbTdqVi/o+UsYNgNadDO3PxEREWnWFK48pHDV8OQUlbMy1TirtTw1h7ziCrf1+JggRiUa494HdgjH18tmUqdywSqKYct7sPoVKEg3at6BxoCKIfdBWKyp7YmIiIiAwpXHFK4aNofDyY9HClxntbakH8dxynexv7eNYZ0iXZcQxkUGmtesnK4kD9b/E9a9DqV5Ri0gCgbfCwPvgIAIc/sTEREROYXClYcUrhqXgpJKVuzNZll12Mo6Ue623iEq0DUUY0jHSPx9tKtlivx0Y5dqy7tQWX2j6fD2xk1/+0wBb13aKSIiIg2PwpWHFK4aL6fTya7ME9WDMbLYeOA4Vadsa/l4WRncIcJ1VqtTdCAWi8a9X1RHfzTOU/34CTirR++37AWXPAJdrwWbl6ntiYiIiJyLwpWHFK6ajhNllazel8vSPdksT8kmI7/Ubb1tuL9rV2tY5yiCfPWDfp1wOuHgKlg5G/Z+e7LeIckIVR0vNW5sJiIiItLANZpwtXz5cl588UU2bdpEZmYmCxcuZMKECRf03FWrVpGUlESPHj1ITk521WfOnMmsWbPcHpuYmMju3bsvuC+Fq6bJ6XSyL7vIdVZr3f48KuwO17q3zcKAuAjXWa0uLYO1q1VTDjvs/sIYp56xyahZrNDtWuPGv637mtqeiIiISE3VJBuY+tf0xcXF9O7dm+nTpzNx4sQLfl5+fj633XYbl19+OceOHTttvXv37ixZssT1tZeXdiMELBYLnWOC6RwTzJ0jOlJSUcXa/bmus1oHcktYsz+XNftz+eNXu2kR4ktSQjSjEmMY3jmKUH9vsz9Cw1VVDlv/A6tfhty9Rs3mC32nwNAHIbKTuf2JiIiI1ANTU8fYsWMZO3ZsjZ937733cvPNN2Oz2Vi0aNFp615eXrRs2bIOOpSmLMDHi8u6tOCyLi0AOJBT7LqJ8ep9ORwrLOfDjYf5cONhbFYLfWPDXOPeu7cOwWrVrhZlBbDxLVj7Dyiq/osOv1AYeBcMvgeCYsztT0RERKQeNbotnXnz5rF//37ee+89nn/++TM+JjU1ldatW+Pn58fQoUN54YUXaNeu3Vlfs7y8nPLykxPmCgsL67xvafjaRwXSPiqQqcPaU1ZpZ8OBPNclhHuzith48DgbDx7nL4tTiAryYWR8NEmJ0YyIjyYi0Mfs9uvXiaOwdi5snAfl1f+9BLeGoQ9A/6ngG2xufyIiIiImaFThKjU1lSeeeIIVK1ac9VK/wYMHM3/+fBITE8nMzGTWrFmMGDGCH3/8keDgM//A98ILL5x2TkuaNz9vGyPijeD0NHD4eImxq7Unm1V7c8gpquDTLRl8uiUDiwV6tQ1zDcboExuGranuauWkGpf+bf0P2Ktv5ByVaJyn6vlL8GpmIVNERETkFA1mWqDFYjnnQAu73c6QIUO44447uPfeewFjeMWiRYvcBlr8XH5+PnFxcbz00kvccccdZ3zMmXauYmNjNdBCzqiiysGmg8ddlxDuynTf6Qz192ZEfBSjEmMYmRBFTLCfSZ3WocMbYeXfjGEVVP+RETvEmPwXPwasVjO7ExEREbloGs20wFOdL1zl5+cTHh6OzXbyBrAOhwOn04nNZmPx4sVcdtllZ3zuwIEDGT16NC+88MIF9aJpgVITxwrLXEFrRUo2hWVVbuvdWoVUn9WKpl9cON62RhJEnE7Yu8QYp35w5cl6wlgjVLUbYlZnIiIiIvWm0UwLrImQkBC2b9/uVps7dy7ff/89H3/8MR06dDjj84qKiti3bx+33nprfbQpzVCLED9uHBDLjQNiqbI72Ho4n2V7slmaks22wwXszCxkZ2Yhc5fuI9jXi+Gdo1zj3luH+Zvd/unslfDjp8aNf7N2GDWrF/SaBMMegpiu5vYnIiIi0kCZGq6KiorYu3ev6+u0tDSSk5OJiIigXbt2PPnkk2RkZPDOO+9gtVrp0aOH2/NjYmLw8/Nzqz/++OOMHz+euLg4jhw5wu9//3tsNhuTJ0+ut88lzZeXzUr/uAj6x0Xw6C8SySkqZ0WqcVZreWoOecUVfL3jKF/vOApAQoug6rNaMQzsEI6vl+0873ARVRTD5ndhzStQcMio+QRB/2kw5H4IbWNebyIiIiKNgKnhauPGjVx66aWurx999FEApk6dyvz588nMzCQ9Pb1Gr3n48GEmT55Mbm4u0dHRXHLJJaxdu5bo6Og67V3kQkQF+XJd37Zc17ctDoeT7RkFrksIt6QfJ+VYESnHivjXijT8vW0M6xRJUmI0oxJiaBcZUD9NFufC+n8av0rzjFpAFAy5FwbeCf7h9dOHiIiISCPXYM5cNSQ6cyX1Ib+kgpV7c1w3Mc46Ue623iEq0NjVSoxmSIdI/H3qeFfr+EFjl2rzu1BVatTCOxiX/vW5Gbwb4CWLIiIiIvWsUQ60aEgUrqS+OZ1OdmWeYFlKNkv3ZLHp4HGqHCf/0/T1sjK4Y6Rr3Hun6EAsllqOez+63ThP9eOn4LQbtVa9Yfgj0O1asJp4aaKIiIhIA6Nw5SGFKzHbibJKVu/LNW5ivCeLIwVlbuttw/1dQWtY5yiCfM9zha/TCQdWGJP/9n13st7xUuMeVR1HQW3DmoiIiEgTpnDlIYUraUicTid7s4pcZ7XW7c+jwu5wrXvbLAyIizDOaiVGk9gi+OSulsMOu/9nhKojm42axQrdJhihqnWf+v44IiIiIo2KwpWHFK6kISupqGLt/lzXuPeDuSVu6y1D/Li8cwg3+a2mW9p8bMf3GwteftBnCgx7ECI6mtC5iIiISOPTJO9zJSKGAB8vLuvSgsu6tADgQE6x66zW9v2HuK74c27f8TUxlnwAiixB7IqdROCIB+jSqSNWqy7/ExEREbkYFK5EGrn2UYG09ylkatF/cWa+haWiCIAsSxSvV1zJ+/bLKEnxg5TdRAXtZ2S8MYFwRHw0EYE+JncvIiIi0nTossAz0GWB0mhkp8Dqv8PWD8BRadSiuxrnqXpcz6HCKpZX38R41d4ciivsrqdaLNCrbRijqse9924bhk27WiIiIiJudObKQwpX0uAd2gCrZsPuL4Dq/4TbDTXGqcf/AqzW055SUeVg08HjrksIdx894bYeFuDNiHhjAuHIhChigv0u+scQERERaegUrjykcCUNktMJqYuNe1QdXHWynniVEaraDa7Ryx0rLDMmEO7JZkVqNoVlVW7r3VuHuMa994sLx9t2emATERERaeoUrjykcCUNir0SfvzECFVZO42a1Rt6TYLhMyA60eO3qLI7SD6U7xr3vu1wgdt6sK8XwztHkZRohK3WYf4ev6eIiIhIY6Bw5SGFK2kQyotgy7uw5lUoOGTUfIKg/zQYcj+Etrlob51TVM6K6rNay1NzyCuucFtPaBHEqMQYkhKiGdA+HF8v20XrRURERMRMClceUrgSUxXnwLrXYcO/oPS4UQuMgSH3woDp4B9er+3YHU5+zChwndVKPpSP45Q/NQJ8bAzrFFl9CWEM7SID6rU/ERERkYtJ4cpDCldiiuMHYPUrsOU9qCo1auEdjEv/et8M3g1jwER+SQUr9+awdI9xCWH2iXK39Y5RgYysnkA4tGMkft7a1RIREZHGS+HKQwpXUq8ytxnnqXYsBGf1qPRWfeCSR6DrNWBtuOHE6XSyM7PQNRhj08HjVJ2yreXrZWVwR2NXa1RiNB2jArFYNO5dREREGg+FKw8pXMlF53RC2nJjnPq+70/WO11mTP7rMNK4EVUjU1hWyeq9udVhK4sjBWVu623D/RmVaFw+OKxTJIG+uo+5iIiINGwKVx5SuJKLxmGHXf81QtWRLUbNYoXu1xk3/m3V29T26pLT6WRvVlH1Wa1s1qflUWF3uNa9bRYGto8wzmolRpPYIli7WiIiItLgKFx5SOFK6lxlGWxdAKvnQN5+o+blB31vhaEPQEQHc/urByUVVazZl+sKW+l5JW7rLUP8XEFreOcoQv29TepURERE5CSFKw8pXEmdKc2HjW/C2tegOMuo+YXBoLth8D0QGGVmd6ZKyylm2Z4slqVks2Z/LmWVJ3e1bFYL/dqFuca9d2sVgtWqXS0RERGpfwpXHlK4Eo8VHoG1c2HjfKg4YdRC2hq7VP1uA98gU9traMoq7axPy3ONe9+XXey2HhXky8iEKJISohkRH01EoI9JnYqIiEhzo3DlIYUrqbXsPbDqZdj2ATgqjVpMN+M8VY/rwaZL3S7EobwSlqcalw+u3ptDcYXdtWaxQO+2Ya5LCHu3DcOmXS0RERG5SBSuPKRwJTV2aD2snA17vjhZazfMGKce/4tGOfmvoaiocrDp4HGWpmSxbE82u4+ecFsPC/BmRHw0SQnRjEyIIia4YdwPTERERJoGhSsPKVzJBXE4IHWxMfkvfc3JeperjZ2q2EGmtdaUHS0oY3mKcQPj5anZnCirclvv3jrENe69b7swvG1WkzoVERGRpkDhykMKV3JO9krY/rFx49/sXUbN6g29J8GwhyE6wdz+mpEqu4PkQ/muCYTbMwrc1oN9vbgkPsp1CWGrUH+TOhUREZHGSuHKQwpXckblRbD5HVjzKhQeNmo+wTBgGgy5H0Jam9qeQE5ROSuqz2otT8nmeEml23pii2CSEo1LCAe0D8fXy2ZSpyIiItJYKFx5SOFK3BRlw/rXYf2/oCzfqAXGwJD7YMB08A8zszs5C7vDyY8ZBSzdk82ylCySD+XjOOVPuwAfG8M6RZKUEM2oxBhiIwLMa1ZEREQaLIUrDylcCQB5abDmFdjyHlSVGbWIjjBsBvSeDN4anNCYHC+uYOXeHJZVn9fKPlHutt4xKtC1qzWkYyR+3trVEhEREYUrjylcNXOZW43JfzsXgbP6xrat+8LwR6DreLDqh+7GzuFwsutooeus1uaDx6k6ZVvL18vKkI6RrrNaHaMCsWjio4iISLOkcOUhhatmyOmEtGVGqNr/w8l6p8uNcertR2icehNWWFbJ6r25LEvJYumebDILytzWYyP8jaCVEMOwTpEE+nqZ1KmIiIjUN4UrDylcNSMOO+z8zJj8l5ls1Cw26DHRuPyvVS9T25P653Q6Sc0qYtke4/LB9Wl5VNgdrnVvm4WB7SNcZ7USWgRpV0tERKQJU7jykMJVM1BZCskLYPUcOJ5m1Lz8od+tMPQBCG9vanvScBSXV7F2f67rEsL0vBK39ZYhftVBK5phnaMI9fc2qVMRERG5GBSuPKRw1YSVHocNb8K616A426j5h8Ogu41fgVHm9icNmtPp5EBuCcv2ZLE0JZs1+3Iprzq5q2WzWujXLoxRiTEkJUTTrVUIVqt2tURERBozhSsPKVw1QQUZsHYubJoPFUVGLTTW2KXqdxv4BJranjROZZV21qfluca978sudluPCvJlZIJxE+OR8dGEB/qY1KmIiIjUlsKVhxSumpCs3bD6Zdj2ITiqbygb0x2GP2ycq7LpEi6pO4fySlyj3lfvzaG4wu5as1igd9sw1yWEvdqGYdOuloiISIOncOUhhasmIH2tMfkv5auTtbjhxjj1+Cs0+U8uuooqBxsP5hlha082u4+ecFsPC/BmRHw0oxKiGZkQTXSwr0mdioiIyLkoXHlI4aqRcjgg9RsjVB1aW120QJdxRqiKHWhic9LcHS0oY3lKNktTsliRmsOJsiq39e6tQxiVaIx779suDG+b1aRORURE5FQKVx5SuGpkqirgx4+NcerZu42azQd6TTIu/4uKN7c/kZ+psjtIPpRffVYrm+0ZBW7rwX5eXNI5ynUT41ah/iZ1KiIiIgpXHlK4aiTKT8Cmt41BFYUZRs0nGAZOh8H3QUgrc/sTuUDZJ8pZkWoEreUp2RwvqXRbT2wRTFKicQlh//bh+HrZTOpURESk+VG48pDCVQNXlG2MUt/wLyir/hv/oBYw5D4YMB38Qs3tT8QDdoeT7RkFLNtjXEK49VA+jlP+lA7wsTGsUyRJiTGMSogmNiLAvGZFRESaAYUrDylcNVB5+42b/iYvgKoyoxbRCYbPgF43gbefuf2JXATHiytYuTfHdQlhTlG523rHqECSEqNJSohmSMdI/Ly1qyUiIlKXFK48pHDVwBxJhlWzYedn4Ky+YWub/saQii7jwKofJqV5cDic7Dpa6Apamw4ex37Ktpavl5UhHSNd4947RAVi0WRMERERjyhceUjhqgFwOmH/UiNU7V96st55tBGq2l+icerS7BWWVbJ6bw7LUrJZuiebzIIyt/XYCH8jaCXEMLRTJIG+XiZ1KiIi0ngpXHlI4cpE9irY9Zkx+S9zq1Gz2KDH9cblfy17mtufSAPldDpJzSpyndXakHacCrvDte5tszCwfYRr3HtCiyDtaomIiFwAhSsPKVyZoLIUkv9tnKk6fsCoeflDv9tg6AMQHmdqeyKNTXF5FWv357K0Omwdyit1W28V6meMek+IZnh8FCF+3iZ1KiIi0rApXHlI4aoelR6HDW/AutehONuo+YfDoHtg0N0QGGlufyJNgNPpJC2nmGUpxlmtNftyKa86uatls1ro3y7cNRijW6sQrFbtaomIiIDClccUrupBwWFYMxc2zYfKYqMW2g6GPQh9bwGfQFPbE2nKyirtrEvLc11CuD+72G09KsiXkQlRjEqMYUTnKMIDfUzqVERExHwKVx5SuLqIsnYb56m2fwiOKqMW0x0ueQS6Xwc2XZokUt8O5ZW4hmKs3pdDSYXdtWaxQO+2YdVntaLp1TYMm3a1RESkGVG48pDC1UVwcI0RqlK+OllrPwKGP2xMANTBepEGoaLKwcaDxq7WspRsdh894bYeHuDNiHgjaI1MiCY62NekTkVEROqHwpWHFK7qiMMBKV8b49QPrasuWqDr1cY49bYDTGxORC5EZkEpy6vPaq1IzeFEWZXbeo82IdX31Yqhb2wYXjarSZ2KiIhcHApXHlK48lBVBWz/CFa/DNm7jZrNB3rfBMNmQFS8uf2JSK1U2R1sOZTvOqv1Y0ah23qwnxeXdI5iVKKxq9Uq1N+kTkVEROqOwpWHFK5qqfyEMaBizVw4ccSo+YbAgOkw5D4IbmlqeyJSt7JPlLMi1TirtSI1m+MllW7riS2CXWe1+rcPx9fLZlKnIiIitadw5SGFqxoqyoJ1rxkj1csKjFpQSyNQDbgd/ELN7U9ELjq7w8n2jAKW7sliWUo2yYfyOfX/LgE+NoZ1iiIpMZpRCdHERgSY16yIiEgNKFx5SOHqAuXuM276m7wA7OVGLbKzcelf75vASwfdRZqr48UVrNib4xqMkVNU7rbeMTrQdVZrcIcI/Ly1qyUiIg2TwpWHFK7O48gWWDkbdn0OzuobkbYZYIxTTxwHVh1oF5GTHA4nOzMLjZsY78lmU/px7I6T/+vx9bIypGOk6xLCDlGBWDRBVEREGgiFKw8pXJ2B0wn7fzBCVdqyk/XOVxihKm64xqmLyAUpLKtk9d4cllbvamUWlLmtx0b4MyohhqSEaIZ2iiTQ18ukTkVERBSuPKZwdQp7FexcZNyj6ug2o2axQc8bjMv/WvYwtT0RadycTiepWUWus1rr0/KotJ/835KPzcrADuGuSwjjY4K0qyUiIvVK4cpDCldAZSlseQ/WvALHDxg17wDodxsMfQDC2pnanog0TcXlVazZl8uyFGPc+6G8Urf1VqF+1UErmmGdowjx8zapUxERaS4UrjzUrMNVSZ4x9W/d61CSY9T8I2DwvTDoLgiIMLc/EWk2nE4naTnFxlmtlGzW7MulvMrhWrdZLfRvF05S9Vmtbq1CsFq1qyUiInWr0YSr5cuX8+KLL7Jp0yYyMzNZuHAhEyZMuKDnrlq1iqSkJHr06EFycrLb2quvvsqLL77I0aNH6d27N3PmzGHQoEEX3FezDFcFh2HNq7DpbagsNmph7WDoQ9D3FvDR2GQRMVdZpZ11aXmuSwj3Zxe7rUcF+ZKUEE1SYjQj46MIC/AxqVMREWlKapINTD0lXFxcTO/evZk+fToTJ0684Ofl5+dz2223cfnll3Ps2DG3tQ8++IBHH32U1157jcGDBzN79mzGjBnDnj17iImJqeuP0Pgd2wmrX4btH4Gjyqi16AnDH4bu14FNB8lFpGHw87YZ4SkhGoBDeSUsrZ5AuHpfDjlF5Xyy+TCfbD6M1QK9Y8NcZ7V6tgnFpl0tERG5yBrMZYEWi+WCd65uuukm4uPjsdlsLFq0yG3navDgwQwcOJBXXnkFAIfDQWxsLA899BBPPPHEBfXS5HeunE5IX2NM/kv95mS9/Qhj8l+nyzX5T0QalfIqO5sOHHddQrj76Am39fAAb0bEG2e1RsRHEx2s+/CJiMiFaTQ7V7Uxb9489u/fz3vvvcfzzz/vtlZRUcGmTZt48sknXTWr1cro0aNZs2bNWV+zvLyc8vKTN7gsLCys+8YbAocDUr4yQtXh9dVFC3Qdb4SqNv1NbE5EpPZ8vWwM6xzFsM5RPHlVVzILSlleHbRWpOZwvKSSz7ce4fOtRwDo0SbEGPeeGE3f2DC8bLo/n4iIeK5RhavU1FSeeOIJVqxYgZfX6a3n5ORgt9tp0aKFW71Fixbs3r37rK/7wgsvMGvWrDrvt8GoqoBtHxiX/+WkGDWbD/S52ThTFdXZ3P5EROpYq1B/Jg1sx6SB7ai0O0g+lO86q/VjRqHr1ys/7CXYz4sR8VHVlxzG0DLUz+z2RUSkkWo04cput3PzzTcza9YsEhIS6vS1n3zySR599FHX14WFhcTGxtbpe5iirBA2zYe1c+FEplHzDYGBd8Dg+yC4xTmfLiLSFHjbrAxsH8HA9hH8ekwXsk+Uu3a1lqdmk19SyZfbj/Ll9qMAdGkZ7BqMMSAuAh8v7WqJiMiFaTTh6sSJE2zcuJEtW7bw4IMPAsZ5KqfTiZeXF4sXL+aSSy7BZrOdNuTi2LFjtGzZ8qyv7evri69vE7r+/sQxWPcabHgTyguMWlBLGHo/9L8d/JrgOTIRkQsUHezL9f3bcn3/ttgdTrYdzned1Uo+lM/uoyfYffQEry/fT6CPjaGdohhVPe49NkKTU0VE5OwaTbgKCQlh+/btbrW5c+fy/fff8/HHH9OhQwd8fHzo378/3333nWswhsPh4LvvvnMFsiYtd59x6V/y+2CvPkMWGW9M/ut1I3g1oQApIlIHbFYLfduF07ddOI+MTuB4cQUr9uawdE8Wy1OyySmqYMmuYyzZZfylXcfoQNdZrcEdIvDztpn8CUREpCExNVwVFRWxd+9e19dpaWkkJycTERFBu3btePLJJ8nIyOCdd97BarXSo0cPt+fHxMTg5+fnVn/00UeZOnUqAwYMYNCgQcyePZvi4mJuv/32evtc9S5jM6yaDTs/B6qHP7YdCMMfgcSrwKpLWkRELkR4oA/X9G7NNb1b43A42ZlZaOxq7clmU/px9mcXsz87jbdWpeHnbWVIx0jXuPf2kQFYNGlVRKRZMzVcbdy4kUsvvdT19U/nnqZOncr8+fPJzMwkPT29Rq85adIksrOzeeaZZzh69Ch9+vTh66+/Pm3IRaPndMK+74zJfwdWnKzHjzF2quKGaZy6iIgHrFYLPdqE0qNNKA9c2pnCskpWpeawLCWbpXuyOVpYxtI9xj/P+u9O2kUEVAetaIZ2iiTAp9FcHCIiInWkwdznqiFp0Pe5slfBzkXGTtXR6sskrV7Q4wYYPgNadDezOxGRZsHpdJJyrIhlKcYEwvVpeVTaT/7v1MdmZWCHcNclhPExQdrVEhFppGqSDRSuzqBBhquKEtjyHqyZA/nVu3neAdBvKgx9AMKawHRDEZFGqri8ijX7clmaksXSPdkcPl7qtt461I+k6qEYwztHEeznbVKnIiJSUwpXHmpQ4aokD9b/C9a/DiW5Ri0gEgbfCwPvhIAIc/sTERE3TqeTtJxilu4xJhCu3Z9LeZXDte5ltdAvLtx1CWG3ViHa1RIRacAUrjzUYMKV0wmvXQLHfjS+DmsHw2ZAnyngo3HAIiKNQVmlnbX7c13j3vdnF7utRwf7MjLeCFoj4qMIC/AxqVMRETkThSsPNZhwBbDhDdg4Hy55BLpNAJsOSIuINGbpuSUsSzUmEK7el0NJhd21ZrVA79gw11mtnm1CsVm1qyUiYiaFKw81qHDlsIPFqsl/IiJNUHmVnU0HjrO0etz7nmMn3NbDA7wZmWCc1RoRH010sO5XKCJS3xSuPNSgwpWIiDQbmQWlLKs+q7UyNYcT5VVu6z3bhLrOavWJDcPLpvsYiohcbApXHlK4EhERs1XaHWxJz3eNe/8xo9BtPdjPixHxUYxKiGFkQjQtQ/1M6lREpGlTuPKQwpWIiDQ0WSfKWJFi3MR4eWo2+SWVbutdWga7xr0PiIvAx0u7WiIidUHhykMKVyIi0pDZHU62Hc53jXvfejifU/9vHuhjY1jnKJKqz2vFRmjCrIhIbSlceUjhSkREGpPjxRUsTzWC1vKUbHKKKtzWO0UHkpQQw6jEaAZ1iMDP22ZSpyIijY/ClYcUrkREpLFyOJzszCw07qu1J5tN6cexO07+r97P28qQjpGMSogmKTGG9pEBuomxiMg5KFx5SOFKRESaioLSSlbvzXFdQni0sMxtvV1EAKOqz2oN7RRJgI/upygiciqFKw8pXImISFPkdDpJOVbE0j3GBMINB/KotJ/8McDHZmVQhwjXuPfOMUHa1RKRZk/hykMKVyIi0hwUl1exel8uy1KyWLonm8PHS93WW4f6VU8gjGF450iC/bxN6lRExDwKVx5SuBIRkebG6XSyP6fYdRPjtftzKa9yuNa9rBb6xYW7LiHs1ipEu1oi0iwoXHlI4UpERJq7sko7a/fnsnSPMYFwf06x23p0sK9r1PuI+CjCAnxM6lRE5OJSuPKQwpWIiIi79NwSlqUYZ7VW78ulpMLuWrNaoE9sGEkJMSQlRtOrTShWq3a1RKRpULjykMKViIjI2ZVX2dl44Lhr3PueYyfc1iMCfRgRH8WoxGhGxEcTFeRrUqciIp5TuPKQwpWIiMiFO5JfyvIU46zWytQcTpRXua33bBPqOqvVJzYML5vVpE5FRGpO4cpDClciIiK1U2l3sCU93zXufceRQrf1ED8vRsQbQWtkQjQtQ/1M6lRE5MIoXHlI4UpERKRuZJ0oY3lKDstSslmRmk1+SaXbepeWwdXj3qMZEBeBj5d2tUSkYVG48pDClYiISN2zO5xsPZzvGve+9XA+p/4UEuhjY1jnKNdNjNuGB5jXrIhINYUrDylciYiIXHx5xRWsSDWGYixPzSanqMJtvVN0IKMSY0hKiGZQhwj8vG0mdSoizZnClYcUrkREROqXw+FkZ2ah66zW5vR87I6TP6L4eVsZ2jHSuLdWYgwdogJN7FZEmhOFKw8pXImIiJiroLSSVXtzXJcQHi0sc1uPiwxw3cR4aKdIAny8TOpURJo6hSsPKVyJiIg0HE6nkz3HTriC1oYDeVTaT/744mOzMqhDhOusVueYICwW3cRYROqGwpWHFK5EREQarqLyKtbsy2VZShZL92Rz+Hip23qbMH9GVu9qDe8cSbCft0mdikhToHDlIYUrERGRxsHpdLI/p5il1btaa/fnUlHlcK17WS30jwt3jXvv1ipEu1oiUiMKVx5SuBIREWmcSivsrE3LNSYQpmSzP6fYbT062Nd1VmtEfBRhAT4mdSoijYXClYcUrkRERJqG9NwSlqUYEwhX7c2ltNLuWrNaoE9sGEkJMYxKjKZnm1CsVu1qiYg7hSsPKVyJiIg0PeVVdjYeOM6ylGyW7ski5ViR23pEoA8j46NISoxmRHw0UUG+JnUqIg2JwpWHFK5ERESaviP5pSxLMW5ivGpvDifKq1xrFgv0bBPquoSwT2wYXjarid2KiFkUrjykcCUiItK8VNodbD5o7GotS8lmx5FCt/UQPy9GxEdX38Q4mhYhfiZ1KiL1TeHKQwpXIiIizVtWYRnLU3NYlpLNitRs8ksq3da7tAxmVGIMSQnR9I8Lx8dLu1oiTZXClYcUrkREROQndoeTrYfzXePetx3O59SfnoJ8vRjWKdI17r1teIB5zYpInVO48pDClYiIiJxNXnEFK1KNs1rLU7PJKapwW+8cE+Q6qzWoQwR+3jaTOhWRuqBw5SGFKxEREbkQDoeTHUcKXePeN6fnY3ec/NHKz9vK0I6RJCVEMyoxhvZRgSZ2KyK1oXDlIYUrERERqY2C0kpW7c1h2Z5slqZkcayw3G09LjKgOmhFM6RjJAE+XiZ1KiIXSuHKQwpXIiIi4imn08meYyeMoLUnm40H86i0n/yxy8fLyuAOEa5LCDvHBGGx6CbGIg2NwpWHFK5ERESkrhWVV7F6b071TYyzycgvdVtvE+bPyOqgNbxzJMF+3iZ1KiKnUrjykMKViIiIXExOp5N92cWu+2qt3Z9LRZXDte5ltdA/LpykxGhGJcTQtVWwdrVETKJw5SGFKxEREalPpRV21qblsqx63HtaTrHbekywLyOrz2qN6BxNaIB2tUTqi8KVhxSuRERExEwHc4tZXn354Op9uZRW2l1rVgv0bRfuOqvVs00oVqt2tUQuFoUrDylciYiISENRXmVn44HjLN1jjHtPOVbkth4R6MPI+CiSEqMZGR9NZJCvSZ2KNE0KVx5SuBIREZGGKiO/lOUpxk2MV+7Noai8yrVmsUDPNqGuce+924bhZbOa2K1I46dw5SGFKxEREWkMKu0ONh887ppAuDOz0G091N+bS+KjXJcQtgjxM6lTkcZL4cpDClciIiLSGGUVlrE8NYele7JYkZpDQWml23rXViGuoNU/LhwfL+1qiZyPwpWHFK5ERESksbM7nGw9nM/S6gmE2w7nc+pPfUG+XgzrFElSohG22oYHmNesSAOmcOUhhSsRERFpanKLylm5N8c17j23uMJtvXNMkOus1sD2Efh520zqVKRhUbjykMKViIiINGUOh5MdRwpZlpLF0j3ZbE4/juOUnwj9vW0M7RTpuoSwfVSgec2KmEzhykMKVyIiItKcFJRUsmpfjmvc+7HCcrf19pEBRtBKjGZoxyj8fbSrJc2HwpWHFK5ERESkuXI6new5dsI4q7Unm40H86i0n/xx0cfLyuAOEa5LCDtFB2Gx6CbG0nQpXHlI4UpERETEUFRexeq9Oa5x7xn5pW7rbcL8GVkdtIZ1iiTYz9ukTkUuDoUrDylciYiIiJzO6XSyL7u4OmhlsS4tj4oqh2vdy2phQPtwkhJiSEqIpmurYO1qSaOncOUhhSsRERGR8yutsLM2Ldc1gTAtp9htPSbY13VWa0TnaEIDtKsljY/ClYcUrkRERERq7mCusau1bE82q/flUlppd61ZLdC3XbjrrFaP1qFYrdrVkoZP4cpDClciIiIinimvsrMh7bhr3HtqVpHbekSgDyPjoxiVGMOI+Cgig3xN6lTk3BSuPKRwJSIiIlK3MvJLWV59VmvV3lyKyqtcaxYL9GoT6rqEsHfbMLxsVhO7FTmpJtnA1O/a5cuXM378eFq3bo3FYmHRokXnfPzKlSsZPnw4kZGR+Pv706VLF/72t7+5PWbmzJlYLBa3X126dLmIn0JEREREzqdNmD+TB7Xj9VsHsOWZK/jg7iHcN6oT3VqF4HTC1sMFvPz9Xq7/xxr6P7+EBxZs5sONh8gqLDO7dZEL5mXmmxcXF9O7d2+mT5/OxIkTz/v4wMBAHnzwQXr16kVgYCArV67knnvuITAwkLvvvtv1uO7du7NkyRLX115epn5MERERETmFt83K4I6RDO4YyW+u7EJWYZlxVislmxWpORSUVvLFtky+2JYJQNdWIa6zWv3jwvHWrpY0UA3mskCLxcLChQuZMGFCjZ43ceJEAgMDeffddwFj52rRokUkJydf8GuUl5dTXn7yTuSFhYXExsbqskARERGRelZld7D1cEH1YIwstmUUcOpPq0G+XgzvHGmMe0+Mpk2Yv3nNSrNQk8sCG/WWzpYtW1i9ejXPP/+8Wz01NZXWrVvj5+fH0KFDeeGFF2jXrt1ZX+eFF15g1qxZF7tdERERETkPL5uV/nHh9I8L59ErEsgtKmfl3hyW7slmeUo2ucUVfLPjGN/sOAZAfEyQ66zWwPYR+HnbTP4E0pw1yp2rtm3bkp2dTVVVFTNnzuTpp592rX311VcUFRWRmJhIZmYms2bNIiMjgx9//JHg4OAzvp52rkREREQaPofDyY4jhSzdk8WylGw2px/HccpPsv7eNoZ2inRdQhgXGWhes9JkNMppgTUJV2lpaRQVFbF27VqeeOIJXnnlFSZPnnzGx+bn5xMXF8dLL73EHXfccUG9aFqgiIiISMNXUFLJyr05LEsxwtaxwnK39faRAdVBK4YhHSPx99GultRck78ssEOHDgD07NmTY8eOMXPmzLOGq7CwMBISEti7d299tigiIiIiF1logDfjerViXK9WOJ1Odh89wbLqce8bDxznQG4JB9Yc5O01B/HxsjK4Q4RrV6tTdBAWi25iLHWrUYarUzkcDrdL+n6uqKiIffv2ceutt9ZjVyIiIiJSnywWC11bhdC1VQj3JnWiqLyK1XtzWJqSzbI92WTkl7IiNYcVqTk8/8Uu2oT5k5QYTVJCNMM7RxHk2+h/LJYGwNTvoqKiIrcdpbS0NJKTk4mIiKBdu3Y8+eSTZGRk8M477wDw6quv0q5dO9d9q5YvX85f/vIXZsyY4XqNxx9/nPHjxxMXF8eRI0f4/e9/j81mO+vOloiIiIg0PUG+Xvyie0t+0b0lTqeTfdnFrrNa69LyyMgvZcG6dBasS8fLamFA+3CSEmIYlRhNl5bB2tWSWjE1XG3cuJFLL73U9fWjjz4KwNSpU5k/fz6ZmZmkp6e71h0OB08++SRpaWl4eXnRqVMn/vSnP3HPPfe4HnP48GEmT55Mbm4u0dHRXHLJJaxdu5bo6Oj6+2AiIiIi0mBYLBY6xwTROSaIO0d0pLTCztr9ua5LCA/klrB2fx5r9+fxp693ExPs6zqrdUnnKEIDvM3+CNJINJiBFg2JBlqIiIiINB8HcopZnprN0j3ZrNmXS2ml3bVmtUC/duGuce89WoditWpXqzlplNMCGxKFKxEREZHmqazSzsYDx12XEKZmFbmtRwb6MDLBOKs1Ij6KyCBfkzqV+qJw5SGFKxEREREByMgvZdmebJalZLFqby5F5VWuNYsFerUJrd7ViqFPbBg27Wo1OQpXHlK4EhEREZGfq6hysDn9ePVZrWx2ZRa6rYf6e3NJfBSjqne2YkL8TOpU6pLClYcUrkRERETkfI4VlrE8JZulKdmsSMmmsKzKbb1rqxBGVY977x8XjrfNalKn4gmFKw8pXImIiIhITVTZHWw9XMCy6rNa2zIKOPWn7CBfL4Z3jiQpIYakxGjahPmb16zUiMKVhxSuRERERMQTuUXlrEjNYVlKNstTssktrnBbj48Jco17H9ghHF8vm0mdyvkoXHlI4UpERERE6orD4eTHIwUs22NcQrgl/TiOU34C9/e2MbRTpOsSwrjIQPOaldMoXHlI4UpERERELpaCkkpW7s1xjXvPOlHutt4+MoBRiTEkJUQzpGMk/j7a1TKTwpWHFK5EREREpD44nU52Hz3B0upx7xsPHKfqlG0tHy8rgztEVF9CGE2n6CAsFo17r08KVx5SuBIRERERM5woq2T1vlyWpWSzbE82GfmlbuttwvxJSoxmVEI0wzpHEeTrZVKnzYfClYcUrkRERETEbE6nk33ZRdW7Wtms259Hhd3hWve2WegfF+66hLBLy2Dtal0EClceUrgSERERkYampKKKdfvzXGe1DuSWuK23CPElKSGapIQYLukcRWiAt0mdNi0KVx5SuBIRERGRhu5ATrFx+WBKNqv35VBWeXJXy2a10Dc2zDXuvXvrEKxW7WrVhsKVhxSuRERERKQxKau0s+FAnmvc+96sIrf1yEAfRlYPxRgRH01EoI9JnTY+ClceUrgSERERkcbs8PESlqcY495X7c2huMLuWrNYoFebUJKqz2r1iQ3Dpl2ts1K48pDClYiIiIg0FRVVDjanH3cNxtiVWei2HurvzYj4qOrzWtHEhPiZ1GnDpHDlIYUrEREREWmqjhWWuc5qrUjJprCsym29W6sQ17j3fnHheNusJnXaMChceUjhSkRERESagyq7g62H811ntbYdLnBbD/L1YnjnSNe499Zh/iZ1ah6FKw8pXImIiIhIc5RTVM7KVOOs1vLUHPKKK9zW42OCGJVojHsf2CEcXy+bSZ3WH4UrDylciYiIiEhz53A4+fFIgeus1pb04zhOSQ7+3jaGdYokKdE4qxUXGWhesxeRwpWHFK5ERERERNwVlFSyYm82y6rDVtaJcrf1DlGBxlCMxGiGdIjE36dp7GopXHlI4UpERERE5OycTie7Mk+wLCWbpXuy2HTwOFWnbGv5eFkZ3CHCdVarU3QgFkvjHPeucOUhhSsRERERkQt3oqyS1ftyWbonm+Up2WTkl7qttw33d416H9Y5iiBfL5M6rTmFKw8pXImIiIiI1I7T6WRfdpHrrNa6/XlU2B2udW+bhQFxEa6zWl1aBjfoXS2FKw8pXImIiIiI1I2SiirW7s91jXs/mFvitt4ixJekhGhGJcYwvHMUof7eJnV6ZgpXHlK4EhERERG5OA7kFLvOaq3Zn0tZ5cldLZvVQt/YMNe49+6tQ7Bazd3VUrjykMKViIiIiMjFV1ZpZ8OBPNclhHuzilxrFgtsfGo0kUG+JnZYs2zQeE6SiYiIiIhIk+LnbWNEfDQj4qN5Gjh8vIRlKca49+KKKtODVU1p5+oMtHMlIiIiImIup9PZIAZd1CQbWOupJxERERERkQvWEIJVTSlciYiIiIiI1AGFKxER+f/t3X9MVfUfx/HXRfJyIXCagFe0zImElDjzF5Exf/8ajUZTkgX+HglO1zTTSmS6uZrZ72g2pZoIExfGFDHDkiKdaWKaRD/UppGmlgn4YxXn+0fz7nunotcO99yLz8d2N+6558Lrsveunxfn3CMAADAB5QoAAAAATEC5AgAAAAATUK4AAAAAwASUKwAAAAAwAeUKAAAAAExAuQIAAAAAE1CuAAAAAMAElCsAAAAAMAHlCgAAAABMQLkCAAAAABNQrgAAAADABJQrAAAAADAB5QoAAAAATEC5AgAAAAATUK4AAAAAwASBVgfwRYZhSJLOnz9vcRIAAAAAVrrSCa50hJZQrq6hoaFBktS9e3eLkwAAAADwBQ0NDerQoUOL+9iMm6lgt5nm5mbV19crNDRUNpvN0iznz59X9+7ddfz4cYWFhVmaBf6BmYGnmBl4ipmBp5gZeMqXZsYwDDU0NKhr164KCGj5U1UcubqGgIAAdevWzeoYbsLCwiwfLPgXZgaeYmbgKWYGnmJm4ClfmZkbHbG6ggtaAAAAAIAJKFcAAAAAYALKlY+z2+3Kzc2V3W63Ogr8BDMDTzEz8BQzA08xM/CUv84MF7QAAAAAABNw5AoAAAAATEC5AgAAAAATUK4AAAAAwASUKwAAAAAwAeXKYlVVVUpOTlbXrl1ls9m0adOmGz7ns88+U//+/WW329WrVy+99957rZ4TvsPTmfnwww81atQohYeHKywsTAkJCdq2bZt3wsIn3Mr7zBXV1dUKDAxUv379Wi0ffM+tzMzly5f13HPP6Z577pHdblePHj20du3a1g8Ln3ArM1NYWKj4+HgFBwfL6XRq2rRpOnv2bOuHheVWrFihgQMHKjQ0VBEREUpJSVFdXd0Nn1dSUqL77rtPQUFBeuCBB1ReXu6FtJ6hXFmsqalJ8fHxeuutt25q/6NHj2rChAkaNmyYampqNG/ePM2YMYPF8m3E05mpqqrSqFGjVF5ern379mnYsGFKTk7W/v37WzkpfIWnM3PFuXPnlJGRoREjRrRSMviqW5mZiRMnqrKyUmvWrFFdXZ2KiooUExPTiinhSzydmerqamVkZGj69On69ttvVVJSoj179mjmzJmtnBS+YOfOncrOztbu3bu1fft2/fXXXxo9erSampqu+5wvv/xSTzzxhKZPn679+/crJSVFKSkpOnTokBeT3xiXYvchNptNpaWlSklJue4+Cxcu1JYtW9wGKS0tTefOnVNFRYUXUsKX3MzMXEtcXJwmTZqkJUuWtE4w+CxPZiYtLU3R0dFq166dNm3apJqamlbPB99zMzNTUVGhtLQ0HTlyRJ06dfJeOPikm5mZlStXKj8/Xz/99JNr2xtvvKEXX3xRJ06c8EJK+JLTp08rIiJCO3fu1COPPHLNfSZNmqSmpiZt3rzZtW3IkCHq16+f3nnnHW9FvSGOXPmZXbt2aeTIkW7bxowZo127dlmUCP6mublZDQ0NLIDQooKCAh05ckS5ublWR4EfKCsr04ABA/TSSy8pKipKvXv31vz583Xx4kWro8FHJSQk6Pjx4yovL5dhGDp16pQ2btyo8ePHWx0NFvjzzz8lqcW1ib+sgQOtDgDPnDx5UpGRkW7bIiMjdf78eV28eFEOh8OiZPAXK1euVGNjoyZOnGh1FPioH374Qc8++6w+//xzBQbyzwRu7MiRI/riiy8UFBSk0tJSnTlzRrNnz9bZs2dVUFBgdTz4oMTERBUWFmrSpEm6dOmS/v77byUnJ3t8+jL8X3Nzs+bNm6fExETdf//9193vemvgkydPtnZEj3DkCriNrF+/Xnl5edqwYYMiIiKsjgMf9M8//2jy5MnKy8tT7969rY4DP9Hc3CybzabCwkINGjRI48eP16pVq/T+++9z9ArXdPjwYc2dO1dLlizRvn37VFFRoWPHjikrK8vqaPCy7OxsHTp0SMXFxVZHMQV/kvQzXbp00alTp9y2nTp1SmFhYRy1QouKi4s1Y8YMlZSUXHVYHbiioaFBe/fu1f79+5WTkyPp34WzYRgKDAzUxx9/rOHDh1ucEr7G6XQqKipKHTp0cG2LjY2VYRg6ceKEoqOjLUwHX7RixQolJiZqwYIFkqS+ffsqJCREQ4cO1fLly+V0Oi1OCG/IycnR5s2bVVVVpW7durW47/XWwF26dGnNiB7jyJWfSUhIUGVlpdu27du3KyEhwaJE8AdFRUWaOnWqioqKNGHCBKvjwIeFhYXp4MGDqqmpcd2ysrIUExOjmpoaDR482OqI8EGJiYmqr69XY2Oja9v333+vgICAGy6YcHu6cOGCAgLcl6Ht2rWTJHGttbbPMAzl5OSotLRUO3bs0L333nvD5/jLGpgjVxZrbGzUjz/+6Lp/9OhR1dTUqFOnTrr77ru1aNEi/fLLL/rggw8kSVlZWXrzzTf1zDPPaNq0adqxY4c2bNigLVu2WPUS4GWezsz69euVmZmp1157TYMHD3adm+xwONz+yoy2y5OZCQgIuOqc94iICAUFBbV4LjzaFk/fZyZPnqxly5Zp6tSpysvL05kzZ7RgwQJNmzaNsypuE57OTHJysmbOnKn8/HyNGTNGv/76q+bNm6dBgwapa9euVr0MeEl2drbWr1+vjz76SKGhoa61SYcOHVzvGRkZGYqKitKKFSskSXPnzlVSUpJefvllTZgwQcXFxdq7d69Wr15t2eu4JgOW+vTTTw1JV90yMzMNwzCMzMxMIykp6arn9OvXz2jfvr3Rs2dPo6CgwOu5YR1PZyYpKanF/dH23cr7zP/Lzc014uPjvZIVvuFWZqa2ttYYOXKk4XA4jG7duhlPP/20ceHCBe+HhyVuZWZef/11o0+fPobD4TCcTqeRnp5unDhxwvvh4XXXmhVJbmvapKSkq9YqGzZsMHr37m20b9/eiIuLM7Zs2eLd4DeB/+cKAAAAAEzAZ64AAAAAwASUKwAAAAAwAeUKAAAAAExAuQIAAAAAE1CuAAAAAMAElCsAAAAAMAHlCgAAAABMQLkCAAAAABNQrgAA+I9sNps2bdpkdQwAgMUoVwAAvzZlyhTZbLarbmPHjrU6GgDgNhNodQAAAP6rsWPHqqCgwG2b3W63KA0A4HbFkSsAgN+z2+3q0qWL261jx46S/j1lLz8/X+PGjZPD4VDPnj21ceNGt+cfPHhQw4cPl8Ph0F133aVZs2apsbHRbZ+1a9cqLi5OdrtdTqdTOTk5bo+fOXNGjz32mIKDgxUdHa2ysjLXY3/88YfS09MVHh4uh8Oh6Ojoq8ogAMD/Ua4AAG3eCy+8oNTUVB04cEDp6elKS0tTbW2tJKmpqUljxoxRx44d9dVXX6mkpESffPKJW3nKz89Xdna2Zs2apYMHD6qsrEy9evVy+xl5eXmaOHGivvnmG40fP17p6en6/fffXT//8OHD2rp1q2pra5Wfn6/OnTt77xcAAPAKm2EYhtUhAAC4VVOmTNG6desUFBTktn3x4sVavHixbDabsrKylJ+f73psyJAh6t+/v95++229++67WrhwoY4fP66QkBBJUnl5uZKTk1VfX6/IyEhFRUVp6tSpWr58+TUz2Gw2Pf/881q2bJmkfwvbnXfeqa1bt2rs2LF69NFH1blzZ61du7aVfgsAAF/AZ64AAH5v2LBhbuVJkjp16uT6OiEhwe2xhIQE1dTUSJJqa2sVHx/vKlaSlJiYqObmZtXV1clms6m+vl4jRoxoMUPfvn1dX4eEhCgsLEy//fabJOmpp55Samqqvv76a40ePVopKSl66KGHbum1AgB8F+UKAOD3QkJCrjpNzywOh+Om9rvjjjvc7ttsNjU3N0uSxo0bp59//lnl5eXavn27RowYoezsbK1cudL0vAAA6/CZKwBAm7d79+6r7sfGxkqSYmNjdeDAATU1Nbker66uVkBAgGJiYhQaGqoePXqosrLyP2UIDw9XZmam1q1bp1dffVWrV6/+T98PAOB7OHIFAPB7ly9f1smTJ922BQYGui4aUVJSogEDBujhhx9WYWGh9uzZozVr1kiS0tPTlZubq8zMTC1dulSnT5/WnDlz9OSTTyoyMlKStHTpUmVlZSkiIkLjxo1TQ0ODqqurNWfOnJvKt2TJEj344IOKi4vT5cuXtXnzZle5AwC0HZQrAIDfq6iokNPpdNsWExOj7777TtK/V/IrLi7W7Nmz5XQ6VVRUpD59+kiSgoODtW3bNs2dO1cDBw5UcHCwUlNTtWrVKtf3yszM1KVLl/TKK69o/vz56ty5sx5//PGbzte+fXstWrRIx44dk8Ph0NChQ1VcXGzCKwcA+BKuFggAaNNsNptKS0uVkpJidRQAQBvHZ64AAAAAwASUKwAAAAAwAZ+5AgC0aZz9DgDwFo5cAQAAAIAJKFcAAAAAYALKFQAAAACYgHIFAAAAACagXAEAAACACShXAAAAAGACyhUAAAAAmIByBQAAAAAm+B86ln6wtwZXYwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HvrTuj0fP8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}