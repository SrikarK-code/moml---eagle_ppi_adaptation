{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "k-OWRYMLzzG0",
        "lCVCpLwsz2ZY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59ed8f1d4fe94cdf83cc60e4e7d6a0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4be8115b18e840e0a05e1009ceefb0e6",
              "IPY_MODEL_edc70fe4d2c54bec91efa89bb78069dd"
            ],
            "layout": "IPY_MODEL_0f31192bb77445bca1882cf032c1f55c"
          }
        },
        "4be8115b18e840e0a05e1009ceefb0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_385e44dcf6614f9d9d26ada507ded607",
            "placeholder": "​",
            "style": "IPY_MODEL_959c050500ec4c43a87b38341120497c",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "edc70fe4d2c54bec91efa89bb78069dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5946bc0d271c43b6bf00227170c00689",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daa3c4b6c17641039e1c4330529ee834",
            "value": 1
          }
        },
        "0f31192bb77445bca1882cf032c1f55c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385e44dcf6614f9d9d26ada507ded607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959c050500ec4c43a87b38341120497c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5946bc0d271c43b6bf00227170c00689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daa3c4b6c17641039e1c4330529ee834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pip installs"
      ],
      "metadata": {
        "id": "k-OWRYMLzzG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install black\n",
        "!pip install fair-esm\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlB53XWXSmW",
        "outputId": "3ba7bd08-8d5d-4d65-e997-de9aa8ae23ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 rapidfuzz-3.9.6\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting einops_exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops_exts) (0.8.0)\n",
            "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: einops_exts\n",
            "Successfully installed einops_exts-0.0.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting black\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, mypy-extensions, black\n",
            "Successfully installed black-24.8.0 mypy-extensions-1.0.0 pathspec-0.12.1\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.12.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "lCVCpLwsz2ZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYIaRxvBMU0G",
        "outputId": "e4bd68e1-aabd-47a1-bea8-9a1ac388d454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import re\n",
        "import esm\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import wandb\n",
        "wandb.login()\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/')\n",
        "\n",
        "# ESM Model Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model = esm_model.to(device)\n",
        "esm_model.eval()\n",
        "for param in esm_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[dataset['protein_RCSB'] != dataset['peptide_source_RCSB']]\n",
        "\n",
        "# Dataset Class\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.mismatched_lengths = 0\n",
        "        self.total_samples = len(dataframe)\n",
        "        self.check_lengths()\n",
        "\n",
        "    def check_lengths(self):\n",
        "        for idx in range(self.total_samples):\n",
        "            row = self.dataframe.iloc[idx]\n",
        "            peptide_seq = row['peptide_derived_sequence']\n",
        "            energy_scores = row['energy_scores']\n",
        "\n",
        "            energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "            energy_scores = [float(score) for score in energy_scores]\n",
        "\n",
        "            if len(energy_scores) != len(peptide_seq):\n",
        "                self.mismatched_lengths += 1\n",
        "\n",
        "        print(f\"Total samples: {self.total_samples}\")\n",
        "        print(f\"Mismatched lengths: {self.mismatched_lengths}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        peptide_seq = row['peptide_derived_sequence']\n",
        "        protein_seq = row['protein_derived_sequence']\n",
        "        energy_scores = row['energy_scores']\n",
        "\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = self.one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        # Convert energy scores to tensor\n",
        "        energy_scores = torch.tensor(energy_scores, dtype=torch.float32)\n",
        "\n",
        "        return energy_scores, peptide_seq, protein_seq # energy scores are aligned with the peptide (we will keep peptide as protien)\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hot_encode_energy_scores(scores):\n",
        "        return [1 if score <= -1 else 0 for score in scores]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "train_snp = filter_datasets(train_snp)\n",
        "val_snp = filter_datasets(val_snp)\n",
        "test_snp = filter_datasets(test_snp)\n",
        "\n",
        "train_snp, val_snp, test_snp = train_snp[:16], val_snp[16:24], test_snp[24:32] # subset code\n",
        "\n",
        "# Calculate max_length\n",
        "all_seqs = pd.concat([\n",
        "    train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "    val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence'],\n",
        "    test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence']\n",
        "])\n",
        "max_length = max(len(seq) for seq in all_seqs)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinInteractionDataset(train_snp)\n",
        "val_dataset = ProteinInteractionDataset(val_snp)\n",
        "test_dataset = ProteinInteractionDataset(test_snp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydU34BngfUeT",
        "outputId": "b787729d-9806-4239-f5f4-223c2d300520"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 16\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):  # Adjust range to view more samples\n",
        "    energy_scores, protein_seq, peptide_seq = train_dataset[i]\n",
        "    print(f\"Sample {i}:\")\n",
        "\n",
        "    # Print energy scores and their length\n",
        "    print(f\"Energy Scores: {energy_scores}\")\n",
        "    print(f\"Length of Energy Scores: {energy_scores.shape[0]}\")\n",
        "\n",
        "    # Print protein sequence and its length\n",
        "    print(f\"Protein Sequence: {protein_seq}\")\n",
        "    print(f\"Length of Protein Sequence: {len(protein_seq)}\")\n",
        "\n",
        "    # Print peptide sequence and its length\n",
        "    print(f\"Peptide Sequence: {peptide_seq}\")\n",
        "    print(f\"Length of Peptide Sequence: {len(peptide_seq)}\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDhIUkdKfXey",
        "outputId": "403c3b93-9322-4672-b016-b7593b74b7ae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Energy Scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 221\n",
            "Protein Sequence: EVQLVQSGAEVKKPGESLNISCKASGYSFTIYWIAWVRQLPGKGLEWMGIIYPGDSDTRYSPSFQGQVTISADKSISTAYLQWRSLKASDSAVYYCARGVAVDWYFDLWGRGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKS\n",
            "Length of Protein Sequence: 221\n",
            "Peptide Sequence: QSVLTQPPSVSGAPGQRVTISCAGSSSNIGAGFDVYWYQQLPGTAPKLLIYGNNNRPSGVPDRFSGSKSGTSASLAITGLQAEDEADYYCQSSGSVLSDLYVFGTGTKVTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAPTE\n",
            "Length of Peptide Sequence: 216\n",
            "\n",
            "\n",
            "Sample 1:\n",
            "Energy Scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 270\n",
            "Protein Sequence: NLTCDFNDVYKLEFHPNQQTSVTKLCNLTPNVLEKVTIKCGSDKLNYNLYPPTCFEEVYASRNMMHLKKIKEFVIGSSMFMRRSLTPNKINEVSFRIPPNMMPEKPIYCFCENKKTITINGSNGNPSSKKDIINRGIVEIIIPSLNEKVKGCDFTTSESTIFSKGYSINEISQDIVCTVKAHANDLIGFKCPSNYSVEPHDCFVSAFNLSGKNENLENKLKLTNIIMDHYNNTFYSRLPSLISDNWKFFCVCSKDNEKKLVFTVEASISS\n",
            "Length of Protein Sequence: 270\n",
            "Peptide Sequence: LQESGGGLVQAGGSLRLSCAASGRTFSSYGMGWFRQAPGTEREFVAAISWSGDSTYYADSVKGRFTISIDKAKNTVYLQMNSLKPEDTAVYYCAADHALVVGGTYNYWGQGTQVTVSS\n",
            "Length of Peptide Sequence: 118\n",
            "\n",
            "\n",
            "Sample 2:\n",
            "Energy Scores: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Length of Energy Scores: 110\n",
            "Protein Sequence: NMFKSKHKLDFSLVSMDQRGKHILGYELVNMGGYDLVHYDDLAYVASAHQELLKTGASGMIAYRYQKKDGEWQWLQTSSRLVYKNSKPDFVICTHRQLMDEEGHDLLGKR\n",
            "Length of Protein Sequence: 110\n",
            "Peptide Sequence: TEFISRHNIEGIFTFVDHRCVATVGYQPQELLGKNIVEFCHPEDQQLLRDSFQQVVKLKGQVLSVMFRFRSKTREWLWMRTSSFTFQNPYSDEIEYIICTNTNV\n",
            "Length of Peptide Sequence: 104\n",
            "\n",
            "\n",
            "Sample 3:\n",
            "Energy Scores: tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 132\n",
            "Protein Sequence: ANAFLLRPGSLRCKQCSFARIFKDARTKLFWISYSDGDQCASSPCQNGGSCKDQLQSYICFCLPAFEGRNCETHKDDQLICVNENGGCEQYCSDHTGTKRSCRCHEGYSLLADGVSCTPTVEYPCGKIPILE\n",
            "Length of Protein Sequence: 132\n",
            "Peptide Sequence: EPLYENSPEFTPYLETNLGQPTIQSFEQVGTKVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTVNRKSTDSPVECM\n",
            "Length of Peptide Sequence: 116\n",
            "\n",
            "\n",
            "Sample 4:\n",
            "Energy Scores: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 154\n",
            "Protein Sequence: SMAASRRLMKELEEIRKCGMKNFRNIQVDEANLLTWQGLIVPDNPPYDKGAFRIEINFPAEYPFKPPKITFKTKIYHPNIDEKGQVCLPVISAENWKPATKTDQVIQSLIALVNDPQPEHPLRADLAEEYSKDRKKFCKNAEEFTKKYGEKRPV\n",
            "Length of Protein Sequence: 154\n",
            "Peptide Sequence: GSEFQECAVCGWALPHNRMQALTSCECTICPDCFRQHFTIALKEKHITDMVCPACGRPDLTDDTQLLSYFSTLDIQLRESLEPDAYALFHKKLTEGVLMRD\n",
            "Length of Peptide Sequence: 101\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "b5j2iJtxz8rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ESM Encoder-Decoder (reduce latent to 128 as paper says its easier to diffuse on)\n",
        "class RefinedESMEncoderDecoder(nn.Module):\n",
        "    def __init__(self, esm_dim=1280, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(esm_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=latent_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.final_layer = nn.Linear(latent_dim, len(esm_model.alphabet))  # 20 amino acids + unknown + ESM stuff. could also be simple and just make it 21 = 20+unk\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = torch.tanh(self.encoder(x))\n",
        "        decoded = self.decoder(latent, latent)\n",
        "        return self.final_layer(decoded)\n",
        "\n",
        "# CLIP Model with classifier free guidance schedule (90/10)\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self, embed_dim, projection_dim=128):\n",
        "        super().__init__()\n",
        "        self.antibody_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.protein_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.project_antibody = nn.Linear(embed_dim, projection_dim)\n",
        "        self.project_protein = nn.Linear(embed_dim, projection_dim)\n",
        "\n",
        "    def forward(self, antibody_emb, protein_emb):\n",
        "        antibody_vec = self.antibody_encoder(antibody_emb)\n",
        "        protein_vec = self.protein_encoder(protein_emb)\n",
        "\n",
        "        antibody_embed = F.normalize(self.project_antibody(antibody_vec[:, 0]), dim=-1)\n",
        "        protein_embed = F.normalize(self.project_protein(protein_vec[:, 0]), dim=-1)\n",
        "\n",
        "        # Calculate CLIP loss\n",
        "        clip_loss = -torch.sum(antibody_embed * protein_embed, dim=-1).mean()\n",
        "\n",
        "        return antibody_embed, protein_embed, clip_loss\n",
        "\n",
        "# One hot encoding of antigen sequence + Binary motif (epitope) representation\n",
        "class RefinedRepresentation(nn.Module):\n",
        "    def __init__(self, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, tokens, energy_scores):\n",
        "      # One-hot encoding\n",
        "      one_hot = F.one_hot(tokens, num_classes=len(esm_model.alphabet))\n",
        "      # Binary motif channel\n",
        "      motif_channel = (energy_scores <= -1).float().unsqueeze(-1)\n",
        "      # Combine representations\n",
        "      combined = torch.cat([one_hot, motif_channel], dim=-1)\n",
        "\n",
        "      return combined\n",
        "\n",
        "\n",
        "# Updated Denoiser Model\n",
        "class RefinedDenoiser(nn.Module):\n",
        "    def __init__(self, latent_dim, protein_dim, clip_dim, alphabet_size):\n",
        "        super().__init__()\n",
        "        self.protein_binder_transformer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=8)\n",
        "        self.target_protein_transformer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=8)\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=8)\n",
        "        self.onehot_projection = nn.Linear(alphabet_size, latent_dim)\n",
        "        self.final_layer = nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, protein_emb, clip_emb, motif_emb, onehot_seq, t, use_classifier_free=False):\n",
        "        if use_classifier_free:\n",
        "            protein_emb = torch.zeros_like(protein_emb)\n",
        "            clip_emb = torch.zeros_like(clip_emb)\n",
        "            motif_emb = torch.zeros_like(motif_emb)\n",
        "            onehot_seq = torch.zeros_like(onehot_seq)\n",
        "\n",
        "        # Ensure all inputs have the same sequence length\n",
        "        seq_len = x.size(1)\n",
        "        protein_emb = F.interpolate(protein_emb.transpose(1, 2), size=seq_len, mode='linear', align_corners=False).transpose(1, 2)\n",
        "\n",
        "        x = self.protein_binder_transformer(x)\n",
        "        protein_emb = self.target_protein_transformer(protein_emb)\n",
        "\n",
        "        # Reshape for cross attention\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, latent_dim)\n",
        "        protein_emb = protein_emb.permute(1, 0, 2)  # (seq_len, batch_size, latent_dim)\n",
        "\n",
        "        x, _ = self.cross_attention(x, protein_emb, protein_emb)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, latent_dim)\n",
        "\n",
        "        onehot_proj = self.onehot_projection(onehot_seq)\n",
        "        x = x + clip_emb + motif_emb + onehot_proj\n",
        "        return self.final_layer(x)\n",
        "\n",
        "# Updated LatentDiffusion Model\n",
        "class RefinedLatentDiffusion(nn.Module):\n",
        "    def __init__(self, esm_model, num_steps, latent_dim, protein_dim, clip_dim, device):\n",
        "        super().__init__()\n",
        "        self.esm_model = esm_model\n",
        "        self.num_steps = num_steps\n",
        "        self.latent_dim = latent_dim\n",
        "        self.protein_dim = protein_dim\n",
        "        self.clip_dim = clip_dim\n",
        "        self.device = device\n",
        "        self.alphabet_size = len(esm_model.alphabet)\n",
        "\n",
        "        self.esm_encoder_decoder = RefinedESMEncoderDecoder(esm_dim=1280, latent_dim=latent_dim)\n",
        "        self.refined_representation = RefinedRepresentation(seq_len=1000)  # Adjust seq_len as needed\n",
        "        self.clip_model = CLIPModel(embed_dim=clip_dim)\n",
        "        self.denoiser = RefinedDenoiser(latent_dim=latent_dim, protein_dim=protein_dim, clip_dim=clip_dim, alphabet_size=self.alphabet_size)\n",
        "\n",
        "        # Define beta schedule\n",
        "        self.beta = torch.linspace(1e-4, 0.02, num_steps).to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0).to(self.device)\n",
        "        return (\n",
        "            self.sqrt_alpha_bar[t, None, None] * x0 +\n",
        "            self.sqrt_one_minus_alpha_bar[t, None, None] * noise\n",
        "        )\n",
        "\n",
        "    def p_losses(self, peptide_latent, protein_emb, protein_repr, clip_emb, motif_emb, onehot_seq, t, target_seq, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(peptide_latent).to(self.device)\n",
        "\n",
        "        x_noisy = self.q_sample(peptide_latent, t, noise=noise)\n",
        "\n",
        "        # Ensure all inputs have the correct shape\n",
        "        batch_size, seq_len, _ = x_noisy.shape\n",
        "        protein_emb = F.interpolate(protein_emb.transpose(1, 2), size=seq_len, mode='linear', align_corners=False).transpose(1, 2)\n",
        "        clip_emb = clip_emb.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "        motif_emb = motif_emb.expand(-1, seq_len, -1)\n",
        "        onehot_seq = F.interpolate(onehot_seq.float().transpose(1, 2), size=seq_len, mode='linear', align_corners=False).transpose(1, 2)\n",
        "\n",
        "        # Classifier-free guidance: 10% of the time, use unconditional generation\n",
        "        use_classifier_free = random.random() < 0.1\n",
        "        predicted_noise = self.denoiser(x_noisy, protein_emb, clip_emb, motif_emb, onehot_seq, t, use_classifier_free)\n",
        "\n",
        "        loss = F.mse_loss(predicted_noise, noise)\n",
        "\n",
        "        # CLIP loss (only when not using classifier-free guidance)\n",
        "        if not use_classifier_free:\n",
        "            peptide_clip, protein_clip, clip_loss = self.clip_model(peptide_latent, protein_emb)\n",
        "        else:\n",
        "            clip_loss = torch.tensor(0.0).to(self.device)\n",
        "\n",
        "        # Decoder loss\n",
        "        decoded_seq = self.esm_encoder_decoder(peptide_latent)\n",
        "        ce_loss = F.cross_entropy(decoded_seq.view(-1, decoded_seq.size(-1)), target_seq.view(-1))\n",
        "\n",
        "        total_loss = loss + 0.1 * clip_loss + ce_loss\n",
        "        return total_loss, clip_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x, protein_emb, clip_emb, motif_emb, onehot_seq, t, guidance_scale=3.0):\n",
        "        betas_t = self.beta[t][:, None, None]\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alpha_bar[t][:, None, None]\n",
        "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alpha[t])[:, None, None]\n",
        "\n",
        "        # Generate both conditional and unconditional predictions\n",
        "        noise_pred_cond = self.denoiser(x, protein_emb, clip_emb, motif_emb, t, use_classifier_free=False)\n",
        "        noise_pred_uncond = self.denoiser(x, protein_emb, clip_emb, motif_emb, t, use_classifier_free=True)\n",
        "\n",
        "        # Apply classifier-free guidance\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
        "\n",
        "        model_mean = sqrt_recip_alphas_t * (\n",
        "            x - betas_t * noise_pred / sqrt_one_minus_alphas_cumprod_t\n",
        "        )\n",
        "\n",
        "        if t[0] > 0:\n",
        "            noise = torch.randn_like(x).to(self.device)\n",
        "            return model_mean + torch.sqrt(betas_t) * noise\n",
        "        else:\n",
        "            return model_mean\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, num_samples, sequence_length, protein_emb, clip_emb, motif_emb, onehot_seq, guidance_scale=3.0):\n",
        "        device = next(self.parameters()).device\n",
        "        shape = (num_samples, sequence_length, self.latent_dim)\n",
        "        x = torch.randn(shape, device=device)\n",
        "        protein_emb = protein_emb.to(device)\n",
        "        clip_emb = clip_emb.to(device)\n",
        "        motif_emb = motif_emb.to(device)\n",
        "\n",
        "        for t in reversed(range(0, self.num_steps)):\n",
        "            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "            x = self.p_sample(x, protein_emb, clip_emb, motif_emb, t_batch, guidance_scale)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_without_guidance(self, num_samples, sequence_length):\n",
        "        device = next(self.parameters()).device\n",
        "        shape = (num_samples, sequence_length, self.latent_dim)\n",
        "        x = torch.randn(shape, device=device)\n",
        "\n",
        "        for t in reversed(range(0, self.num_steps)):\n",
        "            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "            betas_t = self.beta[t][:, None, None]\n",
        "            sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alpha_bar[t][:, None, None]\n",
        "            sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alpha[t])[:, None, None]\n",
        "\n",
        "            # Unconditional prediction\n",
        "            noise_pred = self.denoiser(x, None, None, None, t_batch, use_classifier_free=True)\n",
        "\n",
        "            model_mean = sqrt_recip_alphas_t * (\n",
        "                x - betas_t * noise_pred / sqrt_one_minus_alphas_cumprod_t\n",
        "            )\n",
        "\n",
        "            if t > 0:\n",
        "                noise = torch.randn_like(x).to(device)\n",
        "                x = model_mean + torch.sqrt(betas_t) * noise\n",
        "            else:\n",
        "                x = model_mean\n",
        "\n",
        "        return x\n",
        "\n",
        "def generate_protein_binders(model, protein_seq, motif, num_samples=1, guidance_scale=3.0):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Generate protein embedding\n",
        "    with torch.no_grad():\n",
        "        protein_tokens = model.esm_model.encode(protein_seq)\n",
        "        protein_embedding = model.esm_model(protein_tokens.to(device), repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "    # Process motif based on input type\n",
        "    if isinstance(motif, str):\n",
        "        # Motif is a sequence\n",
        "        motif_tokens = model.esm_model.encode(motif)\n",
        "        with torch.no_grad():\n",
        "            motif_embedding = model.esm_model(motif_tokens.to(device), repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "        motif_repr = model.refined_representation(motif_tokens, torch.ones_like(motif_embedding[:, :, 0]))\n",
        "    elif isinstance(motif, torch.Tensor):\n",
        "        # Motif is a binary tensor\n",
        "        motif_embedding = motif.float().to(device)\n",
        "        motif_repr = model.refined_representation(protein_tokens, motif_embedding)\n",
        "    else:\n",
        "        raise ValueError(\"Motif must be either a string (sequence) or a torch.Tensor (binary representation)\")\n",
        "\n",
        "    # Create refined representations\n",
        "    protein_repr = model.refined_representation(protein_tokens, torch.zeros_like(protein_embedding[:, :, 0]))\n",
        "\n",
        "    # Process through ESM encoder-decoder\n",
        "    protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "    motif_latent = model.esm_encoder_decoder.encoder(motif_embedding)\n",
        "\n",
        "    # Sample from the model\n",
        "    latent_samples = model.sample(num_samples, protein_latent.shape[1], protein_latent, motif_latent, guidance_scale)\n",
        "\n",
        "    # Decode the latent samples to amino acid sequences\n",
        "    generated_sequences = model.esm_encoder_decoder.decoder(latent_samples)\n",
        "\n",
        "    return generated_sequences\n",
        "\n",
        "def generate_protein_binders_without_guidance(model, sequence_length, num_samples=1):\n",
        "    # Sample from the model without guidance\n",
        "    latent_samples = model.sample_without_guidance(num_samples, sequence_length)\n",
        "\n",
        "    # Decode the latent samples to amino acid sequences\n",
        "    generated_sequences = model.esm_encoder_decoder.decoder(latent_samples)\n",
        "\n",
        "    return generated_sequences\n",
        "\n",
        "def pad_or_truncate(tensor, target_length, pad_value=0):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < target_length:\n",
        "        padding = torch.full((tensor.size(0), target_length - current_length, *tensor.size()[2:]), pad_value, device=tensor.device)\n",
        "        return torch.cat([tensor, padding], dim=1)\n",
        "    else:\n",
        "        return tensor[:, :target_length]\n",
        "\n",
        "def pad_or_truncate(tensor, target_length, pad_value=0):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < target_length:\n",
        "        padding = torch.full((tensor.size(0), target_length - current_length, *tensor.size()[2:]), pad_value, device=tensor.device)\n",
        "        return torch.cat([tensor, padding], dim=1)\n",
        "    else:\n",
        "        return tensor[:, :target_length]\n",
        "\n",
        "def train_clip(clip_model, train_loader, val_loader, optimizer, num_epochs, device):\n",
        "    wandb.init(project=\"eagle_adaptation_clip\", entity=\"vskavi2003\")\n",
        "    wandb.config.update({\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": train_loader.batch_size\n",
        "    })\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        clip_model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"CLIP Epoch {epoch+1}/{num_epochs}\"):\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            padded_energy_scores = F.pad(energy_scores, (1, 1), value=0)\n",
        "\n",
        "            batch_converter = esm_model.alphabet.get_batch_converter()\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "\n",
        "            max_seq_len = max(protein_tokens.size(1), peptide_tokens.size(1), padded_energy_scores.size(1))\n",
        "            protein_tokens = pad_or_truncate(protein_tokens, max_seq_len, pad_value=esm_model.alphabet.padding_idx)\n",
        "            peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_len, pad_value=esm_model.alphabet.padding_idx)\n",
        "            padded_energy_scores = pad_or_truncate(padded_energy_scores, max_seq_len)\n",
        "\n",
        "            protein_tokens, peptide_tokens = protein_tokens.to(device), peptide_tokens.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                protein_embedding = esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "                peptide_embedding = esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "            peptide_latent = model.esm_encoder_decoder.encoder(peptide_embedding)\n",
        "\n",
        "            _, _, clip_loss = clip_model(peptide_latent, protein_latent)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            clip_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += clip_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        clip_model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                energy_scores, protein_seq, peptide_seq = batch\n",
        "                padded_energy_scores = F.pad(energy_scores, (1, 1), value=0)\n",
        "\n",
        "                _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "                _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "\n",
        "                max_seq_len = max(protein_tokens.size(1), peptide_tokens.size(1), padded_energy_scores.size(1))\n",
        "                protein_tokens = pad_or_truncate(protein_tokens, max_seq_len, pad_value=esm_model.alphabet.padding_idx)\n",
        "                peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_len, pad_value=esm_model.alphabet.padding_idx)\n",
        "                padded_energy_scores = pad_or_truncate(padded_energy_scores, max_seq_len)\n",
        "\n",
        "                protein_tokens, peptide_tokens = protein_tokens.to(device), peptide_tokens.to(device)\n",
        "\n",
        "                protein_embedding = esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "                peptide_embedding = esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "                protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "                peptide_latent = model.esm_encoder_decoder.encoder(peptide_embedding)\n",
        "\n",
        "                _, _, val_clip_loss = clip_model(peptide_latent, protein_latent)\n",
        "                total_val_loss += val_clip_loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"clip_train_loss\": avg_train_loss,\n",
        "            \"clip_val_loss\": avg_val_loss\n",
        "        })\n",
        "\n",
        "        print(f\"CLIP Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_diff_loss = 0\n",
        "    total_clip_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            padded_energy_scores = F.pad(energy_scores, (1, 1), value=0)\n",
        "\n",
        "            batch_converter = model.esm_model.alphabet.get_batch_converter()\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "\n",
        "            max_seq_len = max(protein_tokens.size(1), peptide_tokens.size(1), padded_energy_scores.size(1))\n",
        "            protein_tokens = pad_or_truncate(protein_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            padded_energy_scores = pad_or_truncate(padded_energy_scores, max_seq_len)\n",
        "\n",
        "            protein_tokens = protein_tokens.to(device)\n",
        "            peptide_tokens = peptide_tokens.to(device)\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(model.esm_model.alphabet)).float()\n",
        "\n",
        "            protein_embedding = model.esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "            peptide_embedding = model.esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            protein_repr = model.refined_representation(protein_tokens, padded_energy_scores)\n",
        "            protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "            peptide_latent = model.esm_encoder_decoder.encoder(peptide_embedding)\n",
        "            motif_emb = (padded_energy_scores <= -1).float().unsqueeze(-1)\n",
        "\n",
        "            peptide_clip, protein_clip, clip_loss = model.clip_model(peptide_embedding, protein_embedding)\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            diff_loss, _ = model.p_losses(peptide_latent, protein_latent, protein_repr, protein_clip, motif_emb, protein_onehot, t, peptide_tokens)\n",
        "\n",
        "            loss = diff_loss + clip_loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_diff_loss += diff_loss.item()\n",
        "            total_clip_loss += clip_loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_diff_loss = total_diff_loss / len(dataloader)\n",
        "    avg_clip_loss = total_clip_loss / len(dataloader)\n",
        "\n",
        "    return avg_loss, avg_diff_loss, avg_clip_loss\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
        "    wandb.init(project=\"eagle_adaptation_diffusion\", entity=\"vskavi2003\")\n",
        "    wandb.config.update({\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": train_loader.batch_size\n",
        "    })\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_diff_loss = 0\n",
        "        total_train_clip_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            padded_energy_scores = F.pad(energy_scores, (1, 1), value=0)\n",
        "\n",
        "            batch_converter = model.esm_model.alphabet.get_batch_converter()\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "\n",
        "            max_seq_len = max(protein_tokens.size(1), peptide_tokens.size(1), padded_energy_scores.size(1))\n",
        "            protein_tokens = pad_or_truncate(protein_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            padded_energy_scores = pad_or_truncate(padded_energy_scores, max_seq_len)\n",
        "\n",
        "            protein_tokens = protein_tokens.to(device)\n",
        "            peptide_tokens = peptide_tokens.to(device)\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(model.esm_model.alphabet)).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                protein_embedding = model.esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "                peptide_embedding = model.esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            protein_repr = model.refined_representation(protein_tokens, padded_energy_scores)\n",
        "            protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "            peptide_latent = model.esm_encoder_decoder.encoder(peptide_embedding)\n",
        "            motif_emb = (padded_energy_scores <= -1).float().unsqueeze(-1)\n",
        "\n",
        "            peptide_clip, protein_clip, clip_loss = model.clip_model(peptide_embedding, protein_embedding)\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            diff_loss, _ = model.p_losses(peptide_latent, protein_latent, protein_repr, protein_clip, motif_emb, protein_onehot, t, peptide_tokens)\n",
        "\n",
        "            loss = diff_loss + clip_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_diff_loss += diff_loss.item()\n",
        "            total_train_clip_loss += clip_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_train_diff_loss = total_train_diff_loss / len(train_loader)\n",
        "        avg_train_clip_loss = total_train_clip_loss / len(train_loader)\n",
        "\n",
        "        val_loss, val_diff_loss, val_clip_loss = validate(model, val_loader, device)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"train_diff_loss\": avg_train_diff_loss,\n",
        "            \"train_clip_loss\": avg_train_clip_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_diff_loss\": val_diff_loss,\n",
        "            \"val_clip_loss\": val_clip_loss\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss_plot.png')\n",
        "    wandb.log({\"loss_plot\": wandb.Image('loss_plot.png')})\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "    val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "    test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "    train_snp = filter_datasets(train_snp)\n",
        "    val_snp = filter_datasets(val_snp)\n",
        "    test_snp = filter_datasets(test_snp)\n",
        "\n",
        "    train_snp, val_snp, test_snp = train_snp[:16], val_snp[16:24], test_snp[24:32] # subset code\n",
        "\n",
        "    # Calculate max_length\n",
        "    all_seqs = pd.concat([\n",
        "        train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "        val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence'],\n",
        "        test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence']\n",
        "    ])\n",
        "    max_length = max(len(seq) for seq in all_seqs)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ProteinInteractionDataset(train_snp)\n",
        "    val_dataset = ProteinInteractionDataset(val_snp)\n",
        "    test_dataset = ProteinInteractionDataset(test_snp)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize LatentDiffusion model\n",
        "    latent_dim = 128\n",
        "    protein_dim = esm_model.embed_dim\n",
        "    clip_dim = 128\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_steps = 1000\n",
        "\n",
        "    # Initialize models\n",
        "    clip_model = CLIPModel(embed_dim=128, projection_dim=128).to(device)\n",
        "    latent_diffusion_model = RefinedLatentDiffusion(esm_model, num_steps, latent_dim, protein_dim, clip_dim, device).to(device)\n",
        "\n",
        "    # Train CLIP model\n",
        "    clip_optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-4)\n",
        "    train_clip(clip_model, train_loader, val_loader, clip_optimizer, num_epochs=2, device=device)\n",
        "\n",
        "    # Save trained CLIP model\n",
        "    torch.save(clip_model.state_dict(), 'trained_clip_model.pth')\n",
        "\n",
        "    # Update LatentDiffusion model with trained CLIP\n",
        "    latent_diffusion_model.clip_model = clip_model\n",
        "\n",
        "    # Train LatentDiffusion model\n",
        "    diffusion_optimizer = torch.optim.AdamW(latent_diffusion_model.parameters(), lr=1e-4)\n",
        "    train_losses, val_losses = train(latent_diffusion_model, train_loader, val_loader, diffusion_optimizer, num_epochs=2, device=device)\n",
        "\n",
        "    # Plot losses\n",
        "    plot_losses(train_losses, val_losses)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(latent_diffusion_model.state_dict(), 'protein_binder_model.pth')\n",
        "\n",
        "    # Generation (dummy examples)\n",
        "    latent_diffusion_model.eval()\n",
        "\n",
        "    # Example usage of generation with guidance (sequence-based motif)\n",
        "    protein_seq = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n",
        "    motif_seq = \"LRSLGY\"\n",
        "    generated_binders_seq = generate_protein_binders(latent_diffusion_model, protein_seq, motif_seq, num_samples=3, guidance_scale=3.0)\n",
        "\n",
        "    print(\"Generated protein binders with guidance (sequence-based motif):\")\n",
        "    for i, seq in enumerate(generated_binders_seq):\n",
        "        print(f\"Binder {i+1}: {seq}\")\n",
        "\n",
        "    # Example usage of generation with guidance (binary motif)\n",
        "    binary_motif = torch.zeros(len(protein_seq))\n",
        "    binary_motif[30:36] = 1  # Assuming the motif is in this region\n",
        "    generated_binders_binary = generate_protein_binders(latent_diffusion_model, protein_seq, binary_motif, num_samples=3, guidance_scale=3.0)\n",
        "\n",
        "    print(\"\\nGenerated protein binders with guidance (binary motif):\")\n",
        "    for i, seq in enumerate(generated_binders_binary):\n",
        "        print(f\"Binder {i+1}: {seq}\")\n",
        "\n",
        "    # Example usage of generation without guidance\n",
        "    generated_binders_no_guidance = generate_protein_binders_without_guidance(latent_diffusion_model, sequence_length=100, num_samples=3)\n",
        "\n",
        "    print(\"\\nGenerated protein binders without guidance:\")\n",
        "    for i, seq in enumerate(generated_binders_no_guidance):\n",
        "        print(f\"Binder {i+1}: {seq}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "59ed8f1d4fe94cdf83cc60e4e7d6a0b5",
            "4be8115b18e840e0a05e1009ceefb0e6",
            "edc70fe4d2c54bec91efa89bb78069dd",
            "0f31192bb77445bca1882cf032c1f55c",
            "385e44dcf6614f9d9d26ada507ded607",
            "959c050500ec4c43a87b38341120497c",
            "5946bc0d271c43b6bf00227170c00689",
            "daa3c4b6c17641039e1c4330529ee834"
          ]
        },
        "id": "HbV8-2KMz-CL",
        "outputId": "fdb6aa90-73d2-49eb-cc0a-4cc1907203c4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 16\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:w2o3by1z) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59ed8f1d4fe94cdf83cc60e4e7d6a0b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">serene-terrain-2</strong> at: <a href='https://wandb.ai/vskavi2003/eagle_adaptation_diffusion/runs/w2o3by1z' target=\"_blank\">https://wandb.ai/vskavi2003/eagle_adaptation_diffusion/runs/w2o3by1z</a><br/> View project at: <a href='https://wandb.ai/vskavi2003/eagle_adaptation_diffusion' target=\"_blank\">https://wandb.ai/vskavi2003/eagle_adaptation_diffusion</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240811_222011-w2o3by1z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:w2o3by1z). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1EBYzKC5cdf8cu4kkYeHgWcmUZAPc6Fli/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/wandb/run-20240811_223227-zhutvjqe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vskavi2003/eagle_adaptation_clip/runs/zhutvjqe' target=\"_blank\">vocal-dragon-4</a></strong> to <a href='https://wandb.ai/vskavi2003/eagle_adaptation_clip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vskavi2003/eagle_adaptation_clip' target=\"_blank\">https://wandb.ai/vskavi2003/eagle_adaptation_clip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vskavi2003/eagle_adaptation_clip/runs/zhutvjqe' target=\"_blank\">https://wandb.ai/vskavi2003/eagle_adaptation_clip/runs/zhutvjqe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIP Epoch 1/2: 100%|██████████| 16/16 [00:05<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Epoch 1, Train Loss: -0.8849, Val Loss: -0.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIP Epoch 2/2: 100%|██████████| 16/16 [00:05<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Epoch 2, Train Loss: -0.9874, Val Loss: -0.9950\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for CLIPModel:\n\tsize mismatch for antibody_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for antibody_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for antibody_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for antibody_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for antibody_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for antibody_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for antibody_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for antibody_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for antibody_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for antibody_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for protein_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for protein_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for protein_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for protein_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for protein_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for protein_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for project_antibody.weight: copying a param with shape torch.Size([128, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for project_protein.weight: copying a param with shape torch.Size([128, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3ed0f977ccc5>\u001b[0m in \u001b[0;36m<cell line: 610>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-3ed0f977ccc5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Initialize the latent CLIP model with the trained weights, adjusting for the new input size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0mlatent_clip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m     latent_clip_model.antibody_encoder = nn.TransformerEncoder(\n\u001b[1;32m    561\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CLIPModel:\n\tsize mismatch for antibody_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for antibody_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for antibody_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for antibody_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for antibody_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for antibody_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for antibody_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for antibody_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for antibody_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for antibody_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for antibody_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for antibody_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for antibody_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for protein_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for protein_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for protein_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for protein_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for protein_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for protein_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([3840, 1280]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([3840]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for protein_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 1280]) from checkpoint, the shape in current model is torch.Size([2048, 128]).\n\tsize mismatch for protein_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([1280, 2048]) from checkpoint, the shape in current model is torch.Size([128, 2048]).\n\tsize mismatch for protein_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for protein_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for project_antibody.weight: copying a param with shape torch.Size([128, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for project_protein.weight: copying a param with shape torch.Size([128, 1280]) from checkpoint, the shape in current model is torch.Size([128, 128])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HvrTuj0fP8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
