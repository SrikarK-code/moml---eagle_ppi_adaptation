{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92fbea0b6d674c5ca4ef59b9d1ae1810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_264000c0411746f0b415e64851e9e5c8",
              "IPY_MODEL_d8d9138f99fa457d9c2e7bdd73631dc0"
            ],
            "layout": "IPY_MODEL_3f4366e0937c4bc09de80d7f3104dcfa"
          }
        },
        "264000c0411746f0b415e64851e9e5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60059a92fe57482c9e1c8f8066240ae1",
            "placeholder": "​",
            "style": "IPY_MODEL_f58d4f4aa192440c90e8fbfa605bcae4",
            "value": "0.543 MB of 0.543 MB uploaded\r"
          }
        },
        "d8d9138f99fa457d9c2e7bdd73631dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5063535f3ab24947aba875cb16e3a1ba",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d73238ad659842e189726eddaf22694a",
            "value": 1
          }
        },
        "3f4366e0937c4bc09de80d7f3104dcfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60059a92fe57482c9e1c8f8066240ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58d4f4aa192440c90e8fbfa605bcae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5063535f3ab24947aba875cb16e3a1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d73238ad659842e189726eddaf22694a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install black\n",
        "!pip install fair-esm\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlB53XWXSmW",
        "outputId": "2d658957-b087-4497-bcf8-e32471986e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 rapidfuzz-3.9.4\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n",
            "Collecting einops_exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops_exts) (0.8.0)\n",
            "Installing collected packages: einops_exts\n",
            "Successfully installed einops_exts-0.0.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting black\n",
            "  Downloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Installing collected packages: pathspec, mypy-extensions, black\n",
            "Successfully installed black-24.4.2 mypy-extensions-1.0.0 pathspec-0.12.1\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.10.0-py2.py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.10.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "92fbea0b6d674c5ca4ef59b9d1ae1810",
            "264000c0411746f0b415e64851e9e5c8",
            "d8d9138f99fa457d9c2e7bdd73631dc0",
            "3f4366e0937c4bc09de80d7f3104dcfa",
            "60059a92fe57482c9e1c8f8066240ae1",
            "f58d4f4aa192440c90e8fbfa605bcae4",
            "5063535f3ab24947aba875cb16e3a1ba",
            "d73238ad659842e189726eddaf22694a"
          ]
        },
        "id": "zYIaRxvBMU0G",
        "outputId": "5c92c5fe-1813-41e4-ec24-4ec09d0bf90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:a9azws65) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92fbea0b6d674c5ca4ef59b9d1ae1810"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss training</td><td>█▇▆▅▄▃▃▃▂▂▁▁▁▂▃▂▂▁▂▁▁▁▁▃▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss training</td><td>4.73197</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-cosmos-11</strong> at: <a href='https://wandb.ai/vskavi2003/latent-diffusion-protein/runs/a9azws65' target=\"_blank\">https://wandb.ai/vskavi2003/latent-diffusion-protein/runs/a9azws65</a><br/> View project at: <a href='https://wandb.ai/vskavi2003/latent-diffusion-protein' target=\"_blank\">https://wandb.ai/vskavi2003/latent-diffusion-protein</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240723_140430-a9azws65/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:a9azws65). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1EBYzKC5cdf8cu4kkYeHgWcmUZAPc6Fli/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/wandb/run-20240723_141102-eo7k0jw5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vskavi2003/latent-diffusion-protein/runs/eo7k0jw5' target=\"_blank\">mild-meadow-12</a></strong> to <a href='https://wandb.ai/vskavi2003/latent-diffusion-protein' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vskavi2003/latent-diffusion-protein' target=\"_blank\">https://wandb.ai/vskavi2003/latent-diffusion-protein</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vskavi2003/latent-diffusion-protein/runs/eo7k0jw5' target=\"_blank\">https://wandb.ai/vskavi2003/latent-diffusion-protein/runs/eo7k0jw5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial energy score shape torch.Size([1, 106])\n",
            "intiial prot emb shape torch.Size([1, 106, 1280])\n",
            "tensor([[ 1.2002e-01,  2.3128e-01,  3.5025e-02,  1.4320e-02,  3.1932e-01,\n",
            "         -1.3154e-01,  2.2804e-03, -6.3823e-02, -2.9063e-02, -1.8486e-01,\n",
            "          3.0451e-02, -9.3639e-02,  4.2829e-04,  3.8379e-01,  5.3006e-01,\n",
            "          3.6579e-01,  5.5800e-01,  4.8924e-01,  1.6760e-01,  2.8062e-01,\n",
            "          2.4693e-01,  4.8707e-01,  7.5809e-01,  7.3354e-01,  1.4617e-01,\n",
            "          4.2820e-01,  3.0235e-01,  3.3622e-02, -3.4350e-01, -4.0311e-01,\n",
            "         -4.8136e-01, -3.0632e-01, -3.4722e-01, -4.3481e-01, -8.3017e-02,\n",
            "          6.1324e-01,  1.7082e-01, -3.5690e-01, -1.1254e-01, -2.3791e-02,\n",
            "          3.6737e-01,  7.7707e-01, -1.2616e-01, -3.2163e-01, -2.4831e-02,\n",
            "         -2.5484e-02,  5.0863e-01,  8.0236e-01,  1.8826e-01, -6.3469e-01,\n",
            "         -3.6920e-01,  7.7913e-02, -3.7937e-01, -1.3128e-01, -3.0516e-01,\n",
            "         -1.5366e-01, -3.8302e-01, -3.9545e-01, -6.3183e-01, -3.5832e-01,\n",
            "         -3.0056e-02,  4.3331e-01,  3.8502e-01, -3.8575e-01, -2.6279e-01,\n",
            "          2.2668e-01,  2.2599e-01,  2.8735e-02,  2.8805e-01,  2.4624e-01,\n",
            "          2.8935e-01,  8.1898e-02,  3.3144e-02,  1.1993e-01,  2.8018e-02,\n",
            "         -4.5276e-02, -2.1681e-01,  7.6011e-02, -2.0051e-01, -2.8783e-01,\n",
            "         -3.4513e-01, -4.8667e-01, -8.9820e-03,  5.4471e-01,  2.4646e-01,\n",
            "         -4.0062e-01, -3.9726e-01, -5.4778e-01, -5.0201e-01, -2.9594e-01,\n",
            "         -7.5817e-02, -2.4773e-01,  1.9519e-01,  1.3279e-01,  1.1670e-01,\n",
            "          2.4827e-01,  1.2896e-01, -2.8296e-01,  2.3215e-01,  8.2795e-02,\n",
            "         -3.9065e-01, -1.2540e-02,  5.9311e-03, -2.0810e-01, -3.5435e-01,\n",
            "         -5.0500e-01]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([[[ -2.1571,   3.6740,   2.3252,  ..., -13.2491, -14.4200,   3.4160],\n",
            "         [ -0.7384,   3.5303,   1.0816,  ..., -13.4978, -14.8131,   3.3459],\n",
            "         [ -2.0532,   3.4686,  -1.1015,  ..., -13.4281, -14.1919,   3.3520],\n",
            "         ...,\n",
            "         [ -3.6591,   3.1453,  -3.5123,  ..., -12.7367, -13.4957,   3.0571],\n",
            "         [ -2.1527,   5.9926,  -3.7587,  ..., -14.0849, -13.1589,   5.9593],\n",
            "         [ -3.0126,   1.7402,  -3.8723,  ..., -14.3653, -15.0160,   1.7753]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([[ 0,  5, 10, 23,  8, 10, 15,  5,  4, 21,  7, 17, 18, 15, 13, 20,  6, 22,\n",
            "         13, 13, 22, 12, 12,  5, 14,  4,  9, 19,  9,  5, 18, 21, 23,  9,  6,  4,\n",
            "         23,  9, 18, 14,  4, 10,  8, 21,  4,  9, 14, 11, 17, 21,  5,  7, 12, 16,\n",
            "         11,  4, 20, 17,  8, 20, 13, 14,  9,  8, 11, 14, 14, 11, 23, 23,  7, 14,\n",
            "         11, 10,  4,  8, 14, 12,  8, 12,  4, 18, 12, 13,  8,  5, 17, 17,  7,  7,\n",
            "         19, 15, 16, 19,  9, 13, 20,  7,  7,  9,  8, 23,  6, 23, 10,  2]],\n",
            "       device='cuda:0')\n",
            "total loss: tensor(6.5911, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "l2 loss: tensor(2.1454, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "ce loss: tensor(4.4254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "binding loss: tensor(0.0203, device='cuda:0', grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial energy score shape torch.Size([1, 832])\n",
            "intiial prot emb shape torch.Size([1, 832, 1280])\n",
            "tensor([[-0.1935, -0.2765,  0.1024, -0.3244,  0.3450,  0.2760,  0.0469,  0.2213,\n",
            "          0.1074, -0.1403, -0.0733, -0.1097,  0.0970,  0.4372,  0.1955, -0.3777,\n",
            "         -0.6190, -0.2874, -0.0095, -0.0575, -0.2048, -0.0598, -0.0693, -0.0195,\n",
            "         -0.0125,  0.0016, -0.0750, -0.0030,  0.0187, -0.0070, -0.2340, -0.4295,\n",
            "         -0.5722, -0.3098, -0.3754, -0.1196, -0.1609, -0.2162,  0.4624, -0.0108,\n",
            "         -0.2095, -0.2012,  0.1101,  0.4210,  0.2809,  0.5657,  0.2896,  0.1954,\n",
            "          0.0812, -0.0912, -0.1507,  0.4404, -0.1040,  0.1874,  0.6212,  0.3431,\n",
            "          0.2971,  0.3953,  0.2242,  0.5737,  0.3037,  0.2091,  0.3751,  0.2270,\n",
            "          0.0649,  0.7942, -0.0145,  0.0502, -0.0555, -0.4537, -0.0287, -0.0169,\n",
            "          0.4150,  0.5071,  0.3787,  0.3341,  0.1871, -0.1711, -0.1807,  0.0339,\n",
            "          0.2227,  0.2923,  0.4040,  0.5692,  0.2746,  0.0082,  0.0932,  0.3323,\n",
            "          0.0998, -0.0126, -0.1784, -0.3186, -0.3536, -0.4428,  0.5357,  0.4355,\n",
            "         -0.1574, -0.0015,  0.1644,  0.3062,  0.0731, -0.3172, -0.1393, -0.2407,\n",
            "         -0.1428,  0.0790,  0.1647,  0.4536,  0.0788,  0.4168,  0.6165, -0.2725,\n",
            "         -0.1129, -0.0934, -0.1482,  0.1407,  0.1247, -0.3779, -0.7622, -0.3875,\n",
            "         -0.3578,  0.0609,  0.0847,  0.2272,  0.2879,  0.3475,  0.4588, -0.0111,\n",
            "          0.1190,  0.1592,  0.2552,  0.2492, -0.3191, -0.5110, -0.2548, -0.1593,\n",
            "          0.2985, -0.0620, -0.2185,  0.1446, -0.0344,  0.1597,  0.1977,  0.0727,\n",
            "          0.5167,  0.2177, -0.0509, -0.4209, -0.4909, -0.5033, -0.3550, -0.4062,\n",
            "         -0.6581, -0.4438,  0.3379,  0.4766,  0.4847,  0.3433, -0.1218,  0.1470,\n",
            "         -0.0988,  0.1788, -0.1498, -0.1001, -0.2042, -0.3189, -0.1252, -0.0633,\n",
            "          0.0346,  0.1690,  0.3944,  0.5091, -0.3061, -0.0240, -0.1415,  0.1122,\n",
            "          0.2252,  0.6594,  0.0983, -0.0427, -0.0742,  0.0114,  0.0644,  0.3399,\n",
            "          0.2978,  0.2006, -0.3555, -0.2907, -0.1537,  0.3859, -0.3306, -0.5883,\n",
            "         -0.2137,  0.2193,  0.3371, -0.3058,  0.0709, -0.1363, -0.0445, -0.0735,\n",
            "         -0.2625, -0.3027,  0.2305,  0.1486,  0.2815,  0.2684,  0.0319, -0.1140,\n",
            "         -0.5783, -0.3713, -0.2784, -0.1811,  0.1957,  0.2927,  0.3368,  0.3574,\n",
            "          0.7136,  0.3455, -0.2635, -0.4437,  0.1493,  0.3563,  0.2384,  0.4430,\n",
            "         -0.0893, -0.0813, -0.3290,  0.2298, -0.0273,  0.0435, -0.0465,  0.0348,\n",
            "          0.1630,  0.1003, -0.4658, -0.1594,  0.1940,  0.4671,  0.4894, -0.1035,\n",
            "         -0.2312, -0.3413, -0.2706,  0.1218, -0.0163,  0.4899,  0.1231, -0.0476,\n",
            "         -0.1379, -0.4821, -0.5582,  0.0480,  0.2061,  0.4452,  0.5599,  0.4937,\n",
            "          0.2703, -0.3473, -0.1967, -0.2711, -0.2678,  0.3780, -0.0333,  0.1800,\n",
            "          0.0803, -0.4139, -0.4898, -0.3041, -0.2349, -0.3411, -0.3431,  0.1856,\n",
            "         -0.0718, -0.2123, -0.5912, -0.6402, -0.2554, -0.0099,  0.4374,  0.3823,\n",
            "          0.2498,  0.0725, -0.0253,  0.3474,  0.0576,  0.0580, -0.0329, -0.1048,\n",
            "         -0.4924, -0.0878, -0.2281, -0.0237, -0.0580,  0.4820,  0.3249,  0.0900,\n",
            "          0.1402, -0.0038, -0.2097, -0.0558,  0.1038,  0.2306,  0.0360, -0.0744,\n",
            "         -0.0343, -0.0819, -0.4829,  0.0497, -0.1461,  0.4843,  0.2354,  0.3141,\n",
            "          0.0122, -0.1762, -0.1786, -0.0103,  0.4545,  0.3381,  0.0070, -0.4042,\n",
            "          0.1644, -0.0235,  0.3707,  0.4341,  0.0901,  0.0104,  0.2031,  0.1983,\n",
            "          0.2343,  0.2035,  0.5285,  0.3135,  0.0410,  0.3837, -0.3572, -0.0597,\n",
            "          0.2136,  0.1974,  0.3050,  0.1572,  0.2801, -0.0053,  0.1526,  0.1662,\n",
            "         -0.1347,  0.0914,  0.2534,  0.0029, -0.3979, -0.2395, -0.1584, -0.4370,\n",
            "          0.0182,  0.0898,  0.1569,  0.4889,  0.4690,  0.2052, -0.2560,  0.1068,\n",
            "          0.2193, -0.0273, -0.1527,  0.1715, -0.1868,  0.1834, -0.1125, -0.0796,\n",
            "          0.1199, -0.1854,  0.0184,  0.2739, -0.5122, -0.0737,  0.1627,  0.2128,\n",
            "          0.2704,  0.2958, -0.0416,  0.2390, -0.5646, -0.3715, -0.1107,  0.1287,\n",
            "         -0.0399,  0.0349, -0.2286, -0.5054, -0.1967,  0.2709,  0.0645, -0.2149,\n",
            "         -0.0483,  0.2518,  0.2460,  0.2968,  0.9185,  0.3337, -0.0136, -0.5732,\n",
            "         -0.2911, -0.1042, -0.2323, -0.0840,  0.2047, -0.0727, -0.0668,  0.3467,\n",
            "          0.0163,  0.0616, -0.1849, -0.4898, -0.3854,  0.4503, -0.3857, -0.2431,\n",
            "          0.2002, -0.4128, -0.2693, -0.1133, -0.3007, -0.4400, -0.1137,  0.2551,\n",
            "         -0.2790,  0.0705,  0.0637, -0.4963, -0.0162, -0.5086, -0.4423, -0.2309,\n",
            "         -0.0408, -0.0447,  0.6206,  0.1285, -0.1951,  0.1662,  0.0343, -0.1716,\n",
            "         -0.3377, -0.1128,  0.0739,  0.1266,  0.1071,  0.1981,  0.1924, -0.3407,\n",
            "         -0.0096,  0.0152, -0.3603,  0.2578,  0.8980,  0.4969, -0.4115, -0.2750,\n",
            "          0.0118,  0.0397,  0.3718,  0.2139, -0.3042, -0.3681, -0.2728, -0.2428,\n",
            "         -0.3812, -0.4457, -0.3745, -0.2270, -0.4331, -0.6246, -0.5263, -0.3566,\n",
            "         -0.0849,  0.1771,  0.6876, -0.1238, -0.3365, -0.0274, -0.4679, -0.0612,\n",
            "          0.6741,  0.3433,  0.1982,  0.2798,  0.1769,  0.1684,  0.0388, -0.1127,\n",
            "         -0.3533, -0.4793, -0.0997,  0.0214, -0.0996,  0.2254,  0.2200, -0.4559,\n",
            "         -0.2869,  0.1357,  0.2205,  0.2588, -0.2303, -0.2044,  0.0356,  0.1572,\n",
            "         -0.2774, -0.1582, -0.1565, -0.0833,  0.1470,  0.3742,  0.4757,  0.1666,\n",
            "         -0.1135,  0.2301, -0.0827,  0.0960,  0.1135, -0.2557, -0.3386, -0.2350,\n",
            "         -0.2130,  0.1049, -0.0120,  0.1076, -0.3997, -0.4544, -0.2837, -0.0113,\n",
            "          0.7414,  0.4965, -0.1512,  0.0972,  0.2042,  0.3521,  0.3755,  0.0229,\n",
            "          0.4729,  0.8325,  0.6371,  0.1202, -0.3017,  0.0358,  0.2619,  0.4252,\n",
            "          0.4018,  0.1640,  0.2343, -0.0305, -0.5764,  0.1503, -0.0075,  0.2428,\n",
            "          0.0664,  0.2189, -0.1356,  0.3867,  0.1681, -0.1726, -0.1956, -0.0802,\n",
            "          0.0387,  0.2760,  0.1661,  0.2653, -0.2659, -0.4637,  0.0542,  0.0332,\n",
            "         -0.1430,  0.1681, -0.1462, -0.3781,  0.0792, -0.0565, -0.2638, -0.1109,\n",
            "         -0.0985, -0.5712, -0.4023,  0.0590,  0.1304,  0.1483,  0.1478,  0.2244,\n",
            "          0.0483, -0.0685, -0.2484, -0.2471,  0.2706, -0.0683, -0.2558,  0.3968,\n",
            "          0.3102, -0.0842, -0.0862, -0.0575,  0.0886,  0.5691, -0.0214, -0.0413,\n",
            "          0.0924, -0.1581, -0.1513,  0.2663,  0.2726,  0.3116,  0.1294, -0.1686,\n",
            "         -0.0948,  0.1002,  0.0421, -0.1445,  0.0032, -0.2445, -0.2827,  0.1412,\n",
            "          0.0767, -0.4314, -0.3093, -0.0989, -0.0651,  0.0041, -0.2401, -0.4534,\n",
            "         -0.4483, -0.1141, -0.2962, -0.5722, -0.3316,  0.1796,  0.2502,  0.2392,\n",
            "          0.1186, -0.4288, -0.0726, -0.3711, -0.2260,  0.0752,  0.2745,  0.2996,\n",
            "          0.2147,  0.1594, -0.0090,  0.2230,  0.3142, -0.3889, -0.1017, -0.2202,\n",
            "         -0.1011, -0.0359,  0.0340, -0.2326, -0.5858, -0.4935,  0.0221,  0.3225,\n",
            "          0.1463,  0.1057,  0.1618,  0.2670, -0.0799, -0.3743, -0.2272,  0.2071,\n",
            "          0.0464, -0.0485, -0.1525, -0.0850, -0.3002, -0.5970,  0.1436, -0.2183,\n",
            "          0.1650,  0.4606,  0.1738,  0.3707,  0.2501,  0.0574, -0.4404, -0.2036,\n",
            "          0.0108,  0.2191, -0.0824,  0.2401, -0.0931,  0.0080,  0.6261,  0.4589,\n",
            "         -0.0880, -0.1206,  0.1316,  0.0992,  0.3877,  0.2248, -0.4912, -0.3986,\n",
            "         -0.1572, -0.1645,  0.0787, -0.2355,  0.1046,  0.2380,  0.6174,  0.1316,\n",
            "         -0.0330, -0.0420, -0.0858, -0.1536, -0.1826, -0.1738, -0.4523, -0.4657,\n",
            "         -0.0481, -0.1692,  0.1012,  0.2487, -0.1619, -0.1279,  0.0930,  0.1978,\n",
            "          0.3165,  0.4199,  0.3812,  0.2541,  0.1655, -0.0777, -0.2120, -0.4772,\n",
            "         -0.1800, -0.3124, -0.1822, -0.2874, -0.1862, -0.1767, -0.5237,  0.1327,\n",
            "          0.2034,  0.0938,  0.1672,  0.3080,  0.3174,  0.3444,  0.0522, -0.1596,\n",
            "          0.1199,  0.2516,  0.1428,  0.0797,  0.0049,  0.4884,  0.1831,  0.3342,\n",
            "          0.1933, -0.4008, -0.2513, -0.0449,  0.0384, -0.2142, -0.2801, -0.3960,\n",
            "          0.1633,  0.2691,  0.2110, -0.1016,  0.1379, -0.0538, -0.1642, -0.3114,\n",
            "          0.1102,  0.1879,  0.2343,  0.5210,  0.6707,  0.2149,  0.0074, -0.3711,\n",
            "         -0.2353, -0.6143, -0.4623,  0.0196,  0.1066,  0.0784, -0.4061, -0.1077,\n",
            "          0.1626,  0.5829,  0.4474,  0.4486, -0.0538, -0.1693,  0.1189, -0.1712,\n",
            "         -0.5330, -0.5127, -0.1939,  0.5783,  0.4350, -0.1016, -0.1305, -0.4076,\n",
            "          0.1155,  0.1515,  0.1764, -0.1936, -0.4358, -0.6698, -0.2990, -0.0032,\n",
            "         -0.3331, -0.2905, -0.3765, -0.1516,  0.0973,  0.0037, -0.0976,  0.2877,\n",
            "          0.5937,  0.0878, -0.1192, -0.0241, -0.1183, -0.6966, -0.6329, -0.1050,\n",
            "         -0.1407, -0.2083,  0.0677,  0.0155,  0.3527,  0.5984, -0.0392, -0.2321]],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([[[  0.9615,  -5.0968,  -3.9147,  ..., -14.4906, -14.6502,  -5.1284],\n",
            "         [  2.1060,  -3.5784,  -2.0129,  ..., -13.2415, -13.0223,  -3.6232],\n",
            "         [ -1.2318,  -6.3241,  -5.8225,  ..., -14.8093, -13.7966,  -6.3635],\n",
            "         ...,\n",
            "         [ -3.5030,  -3.3600,  -5.4687,  ..., -14.2843, -13.2276,  -3.3606],\n",
            "         [-12.3059, -21.2894, -13.6212,  ..., -15.5459, -14.9149, -21.1596],\n",
            "         [-14.8389, -22.0606, -13.8916,  ..., -16.1340, -15.6712, -21.9105]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([[ 0, 13,  8, 12,  9, 13, 15, 17,  5, 10,  7, 12,  9,  4, 12,  5,  5, 19,\n",
            "         10, 17, 10,  6, 21,  4, 20,  5, 13, 12, 13, 14,  4, 10,  4, 13, 17, 11,\n",
            "         10, 18, 13,  7, 17,  8, 21,  6,  4, 11,  4, 22, 13,  4, 13, 10,  9, 18,\n",
            "         15,  7, 13,  7, 16, 10, 15, 15,  4, 10, 13, 12,  4,  8,  7,  4, 10, 13,\n",
            "          5, 19, 23, 10, 21,  7,  6,  7,  9, 19, 11, 21, 12,  4,  9, 14,  9, 16,\n",
            "         16, 10, 22, 12, 16,  9, 10,  7,  9, 11, 15, 21, 13, 15, 14, 11,  7,  5,\n",
            "          9, 16, 15, 19, 12,  4,  8, 15,  4, 17,  5,  5,  9,  5, 18,  9, 11, 18,\n",
            "          4, 16, 11, 15, 19,  7,  6, 16, 15, 10, 18,  8,  4,  9,  6,  5,  9, 11,\n",
            "          7, 12, 14, 20, 20, 13,  5,  7, 12, 13, 16, 23,  5,  9, 21,  6,  4, 13,\n",
            "          9,  7,  7, 12,  5, 20, 14, 21, 10,  6, 10,  4, 17,  7,  4,  5, 17, 12,\n",
            "          7,  6, 15, 14, 19,  8, 16, 12, 18,  8,  9, 18,  9, 13,  7, 15, 19, 21,\n",
            "          4,  6,  5, 11,  6, 11, 19, 12, 16, 20, 18,  6, 13, 17, 13, 12,  9,  7,\n",
            "          8,  4, 11,  5, 17, 14,  8, 21,  4,  9,  5,  7, 13, 14,  7,  4,  9,  6,\n",
            "          4,  7, 10,  5, 15, 16, 13,  4,  4, 13, 11,  6,  9,  9,  6,  8, 13, 17,\n",
            "         10, 18,  8,  7,  7, 14,  4, 20,  4, 21,  6, 13,  5,  5, 18,  5,  6, 16,\n",
            "          6,  7,  7,  5,  9, 11,  4, 17,  4,  5,  4,  4, 10,  6, 19, 10, 11,  6,\n",
            "          6, 11, 12, 21, 12,  7,  7, 17, 17, 16, 12,  6, 18, 11, 11,  5, 14, 11,\n",
            "         13,  8, 10,  8,  8,  9, 19, 23, 11, 13,  7,  5, 15, 20, 12,  6,  5, 14,\n",
            "         12, 18, 21,  7, 17,  6, 13, 13, 14,  9,  5, 23,  5, 22,  7,  5, 10,  4,\n",
            "          5,  7, 13, 18, 10, 16,  5, 18, 15, 15, 13,  7,  7, 12, 13, 20,  4, 23,\n",
            "         19, 10, 10, 10,  6, 21, 17,  9,  6, 13, 13, 14,  8, 20, 11, 16, 14, 19,\n",
            "         20, 19, 13,  7, 12, 13, 11, 15, 10,  6,  8, 10, 15,  5, 19, 11,  9,  5,\n",
            "          4, 12,  6, 10,  6, 13, 12,  8, 20, 15,  9,  5,  9, 13,  5,  4, 10, 13,\n",
            "         19, 16,  6, 16,  4,  9,  5,  7, 18, 17,  9,  7, 10,  9,  4,  9, 12, 14,\n",
            "          8, 15,  4,  5, 11,  5,  7, 13, 15,  5, 20,  4, 16, 10, 12,  6, 13,  5,\n",
            "         21,  4,  5,  4, 14,  9,  6, 18, 11,  7, 21, 14, 10,  7, 10, 14,  7,  4,\n",
            "          9, 15, 10, 10,  9, 20,  5, 19,  9,  6, 10, 12, 13, 22,  5, 18,  5,  9,\n",
            "          4,  4,  5,  4,  6,  8,  4, 12,  5,  9,  6, 15,  4,  7, 10,  4,  8,  6,\n",
            "         16, 13, 11, 16, 10,  6, 11, 18, 11, 16, 10, 21,  5,  7, 12,  7, 13, 10,\n",
            "         15, 11,  6,  9,  9, 18, 11, 14,  4, 16,  4,  4,  5, 11, 17, 14, 13,  6,\n",
            "         11, 14, 11,  6,  6, 15, 18,  4,  7, 19, 17,  8,  5,  4,  8,  9, 18,  5,\n",
            "          5,  7,  6, 18,  9, 19,  6, 19,  8,  7,  6, 17, 14, 13,  5, 20,  7,  4,\n",
            "         22,  9,  5, 16, 18,  6, 13, 18,  7, 17,  6,  5, 16,  8, 12, 12, 13,  9,\n",
            "         18, 12,  8,  8,  6,  9,  5, 15, 22,  6, 16,  4,  8, 13,  7,  7,  4,  4,\n",
            "          4, 14, 21,  6, 21,  9,  6, 16,  6, 14, 13, 21, 11,  8,  6, 10, 12,  9,\n",
            "         10, 18,  4, 16,  4, 22,  5,  9,  6,  8, 20, 11, 12,  5, 20, 14,  8, 11,\n",
            "         14,  5, 17, 19, 18, 21,  4,  4, 10, 10, 21,  6, 15, 13,  6, 12, 16, 10,\n",
            "         14,  4, 12,  7, 18, 11, 14, 15,  8, 20,  4, 10, 17, 15,  5,  5,  7,  8,\n",
            "         13, 12, 10, 13, 18, 11,  9,  8, 15, 18, 10,  8,  7,  4,  9,  9, 14, 20,\n",
            "         19, 11, 13,  6,  9,  6, 13, 10, 17, 15,  7, 11, 10,  4,  4,  4, 11,  8,\n",
            "          6, 15, 12, 19, 19,  9,  4,  5,  5, 10, 15,  5, 15,  9, 17, 10,  9, 13,\n",
            "          7,  5, 12,  7, 10, 12,  9, 16,  4,  5, 14,  4, 14, 10, 10, 10,  4,  5,\n",
            "          9, 11,  4, 13, 10, 19, 14, 17,  7, 15,  9, 15, 18, 22,  7, 16,  9,  9,\n",
            "         14,  5, 17, 16,  6,  5, 22, 14,  8, 18,  6,  4, 11,  4, 14,  9, 12,  4,\n",
            "         14, 13, 21, 18, 11,  6,  4, 15, 10, 12,  8, 10, 10,  5, 20,  8,  5, 14,\n",
            "          8,  8,  6,  8,  8, 15,  7, 21,  5,  7,  9, 16, 16,  9, 12,  4, 13, 11,\n",
            "          5, 18,  6,  2]], device='cuda:0')\n",
            "total loss: tensor(8.9043, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "l2 loss: tensor(2.1794, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "ce loss: tensor(6.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "binding loss: tensor(0.0233, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "initial energy score shape torch.Size([1, 553])\n",
            "intiial prot emb shape torch.Size([1, 553, 1280])\n",
            "tensor([[-1.8237e-01,  6.6085e-02,  4.9869e-02,  9.2284e-02, -5.5180e-03,\n",
            "          4.0524e-01,  5.0929e-01,  1.8416e-01,  2.4035e-01,  1.6123e-01,\n",
            "         -1.8142e-01, -1.9232e-01,  8.3940e-03,  5.6424e-01,  4.2540e-01,\n",
            "          1.3210e-01,  2.1493e-01,  2.0216e-01,  3.6448e-01,  3.5529e-01,\n",
            "          2.7276e-01,  4.2467e-01,  2.9368e-01, -2.6995e-01, -1.2433e-01,\n",
            "          1.6514e-01,  2.4740e-01,  5.1255e-01, -2.5263e-01,  2.3155e-01,\n",
            "         -4.6119e-02, -1.7194e-01,  9.6114e-02,  1.4535e-01, -1.2407e-01,\n",
            "         -2.1281e-01,  4.3373e-02, -2.9320e-02,  2.0952e-01,  1.5631e-01,\n",
            "          1.8787e-01,  2.7222e-01,  1.3297e-02, -2.9797e-01, -4.7048e-01,\n",
            "         -2.0900e-01, -3.8138e-01, -1.4260e-01, -5.4380e-02, -4.0423e-01,\n",
            "         -7.9245e-02,  8.9548e-02,  1.6996e-01, -1.3374e-01,  4.3301e-01,\n",
            "          5.0146e-01,  1.4989e-01,  3.0321e-01,  2.2596e-01,  6.1127e-02,\n",
            "          2.4891e-02,  7.3699e-02,  2.4442e-01,  5.6084e-01,  5.7891e-02,\n",
            "          3.7448e-02,  1.4063e-01, -4.6557e-02,  1.1884e-01, -1.3458e-01,\n",
            "          4.3176e-02, -3.3923e-02, -1.5668e-01, -4.6099e-01,  1.2391e-01,\n",
            "          5.0924e-01,  6.0932e-01, -7.4946e-02, -2.0565e-01,  5.3120e-02,\n",
            "         -2.2947e-02, -3.8027e-01, -7.2570e-01, -4.5924e-01, -1.7866e-01,\n",
            "          1.4662e-01,  1.0397e-03,  2.6310e-01, -3.3615e-01, -3.7168e-02,\n",
            "          1.2994e-01,  2.2847e-01, -6.3738e-02,  2.1378e-01, -1.2620e-01,\n",
            "         -2.1907e-01, -1.5763e-01,  5.6981e-01,  5.8075e-01,  6.5519e-01,\n",
            "          3.2779e-01,  4.8909e-01,  2.2757e-01,  5.0888e-01,  2.5505e-01,\n",
            "          4.9016e-01, -6.3734e-01,  1.5604e-01,  4.5534e-01,  3.0073e-01,\n",
            "          2.6437e-01, -4.1226e-01,  1.4513e-01,  3.7872e-01, -1.2848e-01,\n",
            "         -2.7332e-01,  2.5696e-01,  5.7352e-01, -8.8561e-02,  2.0143e-01,\n",
            "          9.6786e-02,  1.7153e-01,  3.3765e-02, -2.1745e-01,  3.9933e-02,\n",
            "          3.8908e-02,  3.3245e-01,  1.0554e-01,  4.3727e-02, -6.0890e-03,\n",
            "          3.7145e-01,  3.3060e-01, -2.2266e-01, -8.5148e-02, -4.7045e-02,\n",
            "          1.4878e-01,  5.3144e-01,  4.7764e-01,  1.5158e-01,  4.1455e-01,\n",
            "          6.2076e-01,  3.9239e-01,  2.4439e-01,  6.3939e-04, -1.1571e-01,\n",
            "          4.7691e-02, -1.5299e-01, -1.5030e-01,  6.1868e-02,  1.4851e-01,\n",
            "          3.1733e-01,  3.1587e-01,  2.9465e-01, -1.2236e-01, -2.9069e-01,\n",
            "         -9.4746e-01, -5.2089e-01, -2.6411e-01, -2.1444e-01, -3.9626e-01,\n",
            "         -2.9315e-01,  1.2512e-01,  8.5748e-02, -6.0348e-01, -5.1946e-01,\n",
            "         -5.4717e-01,  8.4369e-02,  1.7925e-01,  1.4867e-01,  2.5028e-01,\n",
            "         -3.9657e-02, -7.8711e-02,  4.7295e-02, -9.4445e-02, -2.5818e-01,\n",
            "          1.6872e-01,  1.7288e-01,  1.5128e-01, -5.0442e-02,  1.4171e-01,\n",
            "          5.7138e-02,  2.1980e-01, -3.0779e-02,  6.7781e-01,  3.2195e-01,\n",
            "         -1.2720e-01,  1.9714e-02, -2.1935e-01,  1.4934e-01,  2.5228e-01,\n",
            "          2.7180e-01,  1.9197e-01, -6.2314e-02, -7.1509e-02,  6.3401e-03,\n",
            "          2.0573e-01,  5.0321e-01,  4.6238e-01, -1.7874e-01, -2.5992e-01,\n",
            "         -1.3058e-01, -6.7927e-02,  2.3029e-01,  9.4134e-01,  4.6814e-01,\n",
            "          7.2758e-02,  3.7324e-01,  8.5775e-02, -1.2713e-01, -2.9366e-01,\n",
            "         -3.8434e-01,  3.3461e-01,  4.2892e-01,  2.3288e-01,  2.3745e-02,\n",
            "         -7.8122e-03,  2.1267e-01,  4.5483e-01,  3.3295e-01,  3.9080e-01,\n",
            "         -6.3613e-03, -2.2002e-01, -3.6411e-02,  1.1576e-01,  3.3678e-02,\n",
            "          8.6135e-02, -3.7116e-03, -9.4614e-02, -3.6246e-02, -1.4856e-01,\n",
            "         -1.3880e-01,  2.8484e-01,  6.1359e-01,  4.6079e-01,  7.6884e-02,\n",
            "         -2.6214e-01,  2.1067e-01,  1.5808e-01,  8.8888e-02, -1.5239e-01,\n",
            "         -6.7345e-02,  1.0585e-01,  1.4531e-01,  1.0151e-01,  2.5107e-01,\n",
            "         -1.3966e-01, -8.2252e-02, -5.1859e-02,  2.1782e-01, -8.0097e-02,\n",
            "         -8.9774e-02,  1.6362e-01,  1.8468e-01,  5.4805e-01,  4.7801e-01,\n",
            "          7.2154e-01,  2.5223e-01, -2.1551e-01,  3.4550e-01,  1.4089e-01,\n",
            "          2.7729e-02, -4.7455e-02,  3.1730e-01,  2.8283e-01, -4.7470e-02,\n",
            "         -1.1059e-01,  1.3289e-01,  1.5399e-01,  2.7867e-01, -1.4201e-01,\n",
            "         -7.7660e-01, -6.4565e-01, -2.4982e-01,  1.4713e-01,  3.6490e-01,\n",
            "          2.0744e-01,  1.1249e-01, -4.8499e-02,  2.3675e-02,  1.5362e-01,\n",
            "         -3.1578e-01, -2.9887e-01,  2.1224e-02,  9.7544e-02,  7.8628e-02,\n",
            "          2.5252e-01, -6.8705e-02,  4.1119e-01, -2.1198e-01,  2.8254e-01,\n",
            "         -2.0531e-01, -3.1534e-01, -6.1064e-02,  1.1490e-01, -2.8812e-01,\n",
            "         -2.7293e-01, -6.4714e-02,  2.3562e-01,  1.1466e-01,  2.0367e-01,\n",
            "          2.7570e-01,  3.6562e-01,  1.1528e-01,  9.3921e-02,  1.9649e-01,\n",
            "          1.5165e-01,  4.9974e-01,  5.4244e-01,  3.1994e-01,  1.2220e-01,\n",
            "          2.8794e-01,  3.0150e-01,  3.0401e-01,  2.1462e-01, -2.8959e-01,\n",
            "         -2.2199e-01, -1.5886e-01,  3.0647e-02, -3.2863e-02, -5.0604e-01,\n",
            "         -2.6672e-01, -7.1556e-01,  1.7888e-01,  6.7146e-01,  5.3152e-01,\n",
            "          9.1383e-02,  3.3232e-01,  3.0905e-01,  3.4753e-01,  3.5493e-01,\n",
            "          2.3580e-01, -2.6888e-01, -1.6482e-01, -5.0036e-01, -4.8038e-01,\n",
            "         -5.1184e-01, -5.5091e-01, -2.5133e-01, -2.5695e-01,  5.9976e-01,\n",
            "          1.2405e-01,  2.2901e-01, -1.8809e-01, -2.3350e-01, -7.8042e-02,\n",
            "         -3.3201e-01, -3.0340e-01, -3.5698e-01, -9.0326e-02,  1.7951e-02,\n",
            "         -1.4309e-01, -3.3195e-01,  7.9985e-02,  1.6105e-01, -6.1523e-02,\n",
            "          2.4459e-01,  1.9704e-01,  2.0310e-01,  4.4069e-01,  1.6215e-01,\n",
            "          2.5824e-01,  6.9651e-01,  1.3281e-01, -1.9868e-01, -3.0575e-01,\n",
            "         -3.1371e-02, -9.7268e-02,  7.0136e-02,  2.1388e-01,  5.0886e-01,\n",
            "         -3.5420e-02, -1.4729e-02,  3.8784e-01,  3.9472e-01, -4.9998e-01,\n",
            "          2.2422e-01, -2.5141e-01, -1.3361e-01,  4.6026e-02,  2.7391e-02,\n",
            "         -4.2395e-01, -3.6858e-01, -4.5691e-01, -2.7985e-01, -4.2518e-01,\n",
            "         -7.0371e-01, -5.1956e-01,  4.8555e-01,  4.4817e-01,  1.9985e-01,\n",
            "          1.5983e-01,  4.8059e-01,  5.6481e-02,  1.8090e-02,  5.4374e-02,\n",
            "         -1.4270e-01,  1.9089e-01, -1.5807e-01, -1.4051e-01, -5.9720e-01,\n",
            "         -2.5014e-01, -7.4695e-01, -1.7493e-01, -4.4611e-02, -2.2124e-01,\n",
            "          5.4875e-02,  2.2440e-01,  4.6160e-01,  3.8794e-01,  1.8588e-01,\n",
            "         -5.4084e-01,  2.3276e-01,  2.4768e-01,  4.4125e-01,  1.6653e-01,\n",
            "         -3.2104e-01, -6.4440e-01, -7.7535e-01, -1.5520e-01, -4.4186e-02,\n",
            "          2.2352e-01,  2.7902e-01, -1.3659e-01, -2.3729e-01, -1.9967e-01,\n",
            "          1.1932e-02,  1.6459e-01, -2.7790e-01, -5.2571e-01, -4.3678e-01,\n",
            "         -2.9288e-01, -9.6403e-01, -4.2927e-01, -5.0038e-01, -5.4055e-01,\n",
            "          3.6440e-01,  7.8857e-02, -5.2689e-01, -2.2413e-01,  1.2240e-01,\n",
            "         -3.2173e-02, -1.0554e-01,  2.0287e-01,  3.6428e-01, -2.5279e-01,\n",
            "         -1.2071e-01, -2.1903e-01, -2.0687e-01, -1.2145e-01, -1.4978e-01,\n",
            "         -4.4247e-01, -4.1418e-01, -4.7172e-01, -2.1330e-01, -1.8987e-01,\n",
            "         -4.3573e-01, -3.6347e-01, -7.4638e-01, -4.6536e-01, -3.6506e-01,\n",
            "         -8.5526e-02, -7.3637e-02,  3.1537e-01,  1.8051e-01,  1.8794e-01,\n",
            "         -1.4880e-01, -5.1006e-01, -2.3076e-01,  8.4732e-02, -3.7700e-01,\n",
            "         -5.1501e-02,  3.4773e-01,  2.5113e-01, -8.6241e-02, -1.1503e-01,\n",
            "         -1.3386e-01, -6.6251e-02, -4.0669e-01, -1.0504e-01, -3.3284e-01,\n",
            "         -2.0111e-01, -5.0341e-02, -1.3977e-01,  1.9072e-01,  1.9412e-01,\n",
            "          2.7856e-01, -2.4023e-01, -5.2434e-01, -1.9477e-01,  2.6747e-01,\n",
            "          1.2371e-01, -1.5429e-01,  1.5019e-01, -1.1203e-01, -4.8899e-01,\n",
            "         -7.4566e-01, -6.0648e-01, -2.5652e-02,  2.9664e-01,  2.4364e-01,\n",
            "         -2.4868e-01, -4.0166e-01,  5.7961e-01,  7.3892e-01,  2.5329e-01,\n",
            "          5.1217e-01,  3.0893e-01,  3.5582e-01, -3.9718e-01, -3.4643e-01,\n",
            "         -3.8404e-01, -2.6929e-02,  8.5139e-02, -3.4774e-02, -3.2724e-01,\n",
            "         -5.7963e-01,  3.0449e-02,  2.9282e-02,  2.2805e-02, -2.1500e-02,\n",
            "          8.8022e-02,  1.7043e-01, -6.6587e-01, -5.8994e-01, -2.4263e-01,\n",
            "          1.3471e-01, -1.4267e-01, -6.7173e-01, -1.9108e-02,  2.4818e-01,\n",
            "         -2.8607e-01, -9.0752e-02, -5.2992e-02, -2.4994e-01, -3.9355e-01,\n",
            "         -3.4059e-01,  1.1515e-01,  1.0883e-01, -4.9056e-01, -7.5661e-01,\n",
            "         -6.1204e-01, -2.5962e-01, -2.1669e-01, -6.3049e-01, -4.7727e-01,\n",
            "         -4.7824e-01, -1.8106e-01,  8.5771e-02,  2.9508e-01,  2.7637e-01,\n",
            "         -3.5240e-01, -2.8473e-01, -1.4310e-01]], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "tensor([[[  5.0907,   2.8673,  -1.3512,  ..., -13.1065, -12.8772,   2.7786],\n",
            "         [  3.7745,   2.6813,  -2.8187,  ..., -13.3812, -12.7335,   2.5765],\n",
            "         [  0.8833,  -1.4378,  -5.3195,  ..., -14.4783, -12.4165,  -1.5199],\n",
            "         ...,\n",
            "         [ -0.2588,  -2.9862,  -3.2042,  ..., -13.9177, -15.6310,  -3.0680],\n",
            "         [ -2.8424,  -2.2524,  -1.5102,  ..., -13.6558, -14.6985,  -2.2814],\n",
            "         [ -9.0007, -14.8246,  -8.2498,  ..., -15.5751, -15.3608, -14.8913]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([[ 0, 20, 10,  8, 15, 10, 18,  9,  5,  4,  5, 15, 10, 14,  7, 17, 16, 13,\n",
            "          6, 18,  7, 15,  9, 22, 12,  9,  9,  6, 18, 12,  5, 20,  9,  8, 14, 17,\n",
            "         13, 14, 15, 14,  8, 12, 15, 12,  7, 17,  6,  5,  7, 11,  9,  4, 13,  6,\n",
            "         15, 14,  7,  8, 13, 18, 13,  4, 12, 13, 21, 18, 12,  5, 10, 19,  6, 12,\n",
            "         17,  4, 17, 10,  5,  9,  9,  7, 20,  5, 20, 13,  8,  7, 15,  4,  5, 17,\n",
            "         20,  4, 23, 13, 14, 17,  7, 15, 10,  8,  9, 12,  7, 14,  4, 11, 11,  5,\n",
            "         20, 11, 14,  5, 15, 12,  7,  9,  7,  7,  8, 21, 20, 17,  7,  7,  9, 20,\n",
            "         20, 20,  5, 20, 16, 15, 20, 10,  5, 10, 10, 11, 14,  8, 16, 16,  5, 21,\n",
            "          7, 11, 17,  7, 15, 13, 17, 14,  7, 16, 12,  5,  5, 13,  5,  5,  9,  6,\n",
            "          5, 22, 10,  6, 18, 13,  9, 16,  9, 11,  5,  7,  5,  7,  5, 10, 19,  5,\n",
            "         14, 18, 17,  5, 12,  5,  4,  4,  7,  6,  8, 16,  7,  6, 10, 14,  6,  7,\n",
            "          4, 11, 16, 23,  8,  4,  9,  9,  5, 11,  9,  4, 15,  4,  6, 20,  4,  6,\n",
            "         21, 11, 23, 19,  5,  9, 11, 12,  8,  7, 19,  6, 11,  9, 14,  7, 18, 11,\n",
            "         13,  6, 13, 13, 11, 14, 22,  8, 15,  6, 18,  4,  5,  8,  8, 19,  5,  8,\n",
            "         10,  6,  4, 15, 20, 10, 18, 11,  8,  6,  8,  6,  8,  9,  7, 16, 20,  6,\n",
            "         19,  5,  9,  6, 15,  8, 20,  4, 19,  4,  9,  5, 10, 23, 12, 19, 12, 11,\n",
            "         15,  5,  5,  6,  7, 16,  6,  4, 16, 17,  6,  8,  7,  8, 23, 12,  6,  7,\n",
            "         14,  8,  5,  7, 14,  8,  6, 12, 10,  5,  7,  4,  5,  9, 17,  4, 12, 23,\n",
            "          8,  8,  4, 13,  4,  9, 23,  5,  8,  8, 17, 13, 16, 11, 18, 11, 21,  8,\n",
            "         13, 20, 10, 10, 11,  5, 10,  4,  4, 20, 16, 18,  4, 14,  6, 11, 13, 18,\n",
            "         12,  8,  8,  6, 19,  8,  5,  7, 14, 17, 19, 13, 17, 20, 18,  5,  6,  8,\n",
            "         17,  9, 13,  5,  9, 13, 18, 13, 13, 19, 17,  7, 12, 16, 10, 13,  4, 15,\n",
            "          7, 13,  6,  6,  4, 10, 14,  7, 10,  9,  9, 13,  7, 12,  5, 12, 10, 17,\n",
            "         15,  5,  5, 10,  5,  4, 16,  5,  7, 18,  5,  6, 20,  6,  4, 14, 14, 12,\n",
            "         11, 13,  9,  9,  7,  9,  5,  5, 11, 19,  5, 21,  6,  8, 15, 13, 20, 14,\n",
            "          9, 10, 17, 12,  7,  9, 13, 12, 15, 18,  5, 16,  9, 12, 12, 17, 15, 17,\n",
            "         10, 17,  6,  4,  9,  7,  7, 15,  5,  4,  5, 16,  6,  6, 18, 11, 13,  7,\n",
            "          5, 16, 13, 20,  4, 17, 12, 16, 15,  5, 15,  4, 11,  6, 13, 19,  4, 21,\n",
            "         11,  8,  5, 12, 12,  7,  6, 13,  6, 16,  7,  4,  8,  5,  7, 17, 13,  7,\n",
            "         17, 13, 19,  5,  6, 14,  5, 11,  6, 19, 10,  4, 16,  6,  9, 10, 22,  9,\n",
            "          9, 12, 15, 17, 12, 14,  6,  5,  4, 13, 14, 17,  2]], device='cuda:0')\n",
            "total loss: tensor(6.4046, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "l2 loss: tensor(2.2545, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "ce loss: tensor(4.1283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "binding loss: tensor(0.0218, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "initial energy score shape torch.Size([1, 243])\n",
            "intiial prot emb shape torch.Size([1, 243, 1280])\n",
            "tensor([[0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
            "         0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034]],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([[[-13.3715, -11.5830, -11.0213,  ..., -16.1062, -15.1647, -11.5491],\n",
            "         [-13.3715, -11.5830, -11.0213,  ..., -16.1062, -15.1647, -11.5491],\n",
            "         [-13.3715, -11.5830, -11.0213,  ..., -16.1062, -15.1647, -11.5491],\n",
            "         ...,\n",
            "         [-13.3715, -11.5830, -11.0213,  ..., -16.1062, -15.1647, -11.5491],\n",
            "         [-13.3715, -11.5830, -11.0213,  ..., -16.1062, -15.1647, -11.5491],\n",
            "         [-13.3715, -11.5830, -11.0213,  ..., -16.1062, -15.1647, -11.5491]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([[ 0,  6, 19, 13, 10, 21, 12, 11, 12, 18,  8, 14,  9,  6, 10,  4, 19, 16,\n",
            "          7,  9, 19,  5, 18, 15,  5, 11, 17, 16, 11, 17, 12, 17,  8,  4,  5,  7,\n",
            "         10,  6, 15, 13, 23, 11,  7,  7, 12,  8, 16, 15, 15,  7, 14, 13, 15,  4,\n",
            "          4, 13, 14, 11, 11,  7,  8, 19, 12, 18, 23, 12,  8, 10, 11, 12,  6, 20,\n",
            "          7,  7, 17,  6, 14, 12, 14, 13,  5, 10, 17,  5,  5,  4, 10,  5, 15,  5,\n",
            "          9,  5,  5,  9, 18, 10, 19, 15, 19,  6, 19, 13, 20, 14, 23, 13,  7,  4,\n",
            "          5, 15, 10, 20,  5, 17,  4,  8, 16, 12, 19, 11, 16, 10,  5, 19, 20, 10,\n",
            "         14,  4,  6,  7, 12,  4, 11, 18,  7,  8,  7, 13,  9,  9,  4,  6, 14,  8,\n",
            "         12, 19, 15, 11, 13, 14,  5,  6, 19, 19,  7,  6, 19, 15,  5, 11,  5, 11,\n",
            "          6, 14, 15, 16, 16,  9, 12, 11, 11, 17,  4,  9, 17, 21, 18, 15, 15,  8,\n",
            "         15, 12, 13, 21, 12, 17,  9,  9,  8, 22,  9, 15,  7,  7,  9, 18,  5, 12,\n",
            "         11, 21, 20, 12, 13,  5,  4,  6, 11,  9, 18,  8, 15, 17, 13,  4,  9,  7,\n",
            "          6,  7,  5, 11, 15, 13, 15, 18, 18, 11,  4,  8,  5,  9, 17, 12,  9,  9,\n",
            "         10,  4,  7,  5, 12,  5,  9, 16,  2]], device='cuda:0')\n",
            "total loss: tensor(5.0680, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "l2 loss: tensor(1.4989, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "ce loss: tensor(3.5527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "binding loss: tensor(0.0164, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "initial energy score shape torch.Size([1, 248])\n",
            "intiial prot emb shape torch.Size([1, 248, 1280])\n",
            "tensor([[0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031, 0.0031,\n",
            "         0.0031, 0.0031, 0.0031, 0.0031, 0.0031]], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "tensor([[[-13.3829, -11.6233, -11.0312,  ..., -16.1097, -15.1764, -11.5895],\n",
            "         [-13.3829, -11.6233, -11.0312,  ..., -16.1097, -15.1764, -11.5895],\n",
            "         [-13.3829, -11.6233, -11.0312,  ..., -16.1097, -15.1764, -11.5895],\n",
            "         ...,\n",
            "         [-13.3829, -11.6233, -11.0312,  ..., -16.1097, -15.1764, -11.5895],\n",
            "         [-13.3829, -11.6233, -11.0312,  ..., -16.1097, -15.1764, -11.5895],\n",
            "         [-13.3829, -11.6233, -11.0312,  ..., -16.1097, -15.1764, -11.5895]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([[ 0, 20, 11,  5,  5,  9, 21,  6,  4, 21,  5, 14,  5, 19,  5, 22,  8, 21,\n",
            "         17,  6, 14, 18,  9, 11, 18, 13, 21,  5,  8, 12, 10, 10,  6, 19, 16,  7,\n",
            "         19, 10,  9,  7, 23,  5,  5, 23, 21,  8,  4, 13, 10,  7,  5, 22, 10, 11,\n",
            "          4,  7,  6,  7,  8, 21, 11, 17,  9,  9,  7, 10, 17, 20,  5,  9,  9, 18,\n",
            "          9, 19, 13, 13,  9, 14, 13,  9, 16,  6, 17, 14, 15, 15, 10, 14,  6, 15,\n",
            "          4,  8, 13, 19, 12, 14,  6, 14, 19, 14, 17,  9, 16,  5,  5, 10,  5,  5,\n",
            "         17, 16,  6,  5,  4, 14, 14, 13,  4,  8,  4, 12,  7, 15,  5, 10, 21,  6,\n",
            "          6, 23, 13, 19, 12, 18,  8,  4,  4, 11,  6, 19, 14, 13,  9, 14, 14,  5,\n",
            "          6,  7,  5,  4, 14, 14,  6,  8, 17, 19, 17, 14, 19, 18, 14,  6,  6,  8,\n",
            "         12,  5, 20,  5, 10,  7,  4, 18, 13, 13, 20,  7,  9, 19,  9, 13,  6, 11,\n",
            "         14,  5, 11, 11,  8, 16, 20,  5, 15, 13,  7, 11, 11, 18,  4, 17, 22, 23,\n",
            "          5,  9, 14,  9, 21, 13,  9, 10, 15, 10,  4,  6,  4, 15, 11,  7, 12, 12,\n",
            "          4,  8,  8,  4, 19,  4,  4,  8, 12, 22,  7, 15, 15, 18, 15, 22,  5,  6,\n",
            "         12, 15, 11, 10, 15, 18,  7, 18, 17, 14, 14, 15, 14,  2]],\n",
            "       device='cuda:0')\n",
            "total loss: tensor(5.1918, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "l2 loss: tensor(1.4951, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "ce loss: tensor(3.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "binding loss: tensor(0.0164, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "initial energy score shape torch.Size([1, 220])\n",
            "intiial prot emb shape torch.Size([1, 220, 1280])\n",
            "tensor([[-0.0127, -0.1606, -0.2270, -0.1179,  0.0377,  0.2398,  0.2236,  0.1125,\n",
            "          0.3436,  0.1938, -0.2107, -0.1303, -0.0199,  0.0555, -0.1342, -0.3208,\n",
            "         -0.2541, -0.4577, -0.0558,  0.0264, -0.0296, -0.0754,  0.2191,  0.1963,\n",
            "          0.3258,  0.3020, -0.1795, -0.3657, -0.0588,  0.1874,  0.1354, -0.0349,\n",
            "         -0.3116,  0.1107,  0.0699,  0.3286,  0.3982,  0.2222, -0.3168, -0.1095,\n",
            "          0.0128,  0.1036, -0.3829, -0.0347, -0.0905, -0.0569,  0.2104,  0.0102,\n",
            "         -0.1721,  0.0505,  0.1416,  0.1872,  0.3314,  0.3123,  0.3401,  0.3575,\n",
            "          0.1722,  0.0219, -0.0475, -0.0813,  0.1879,  0.1291, -0.0792,  0.0025,\n",
            "          0.0013,  0.1271,  0.0531,  0.0840,  0.2143,  0.0871, -0.0898,  0.1801,\n",
            "         -0.1321, -0.0281, -0.2140,  0.0953, -0.1539, -0.0927,  0.0793,  0.0790,\n",
            "          0.1544, -0.0227, -0.0409,  0.0996, -0.0576,  0.0863,  0.1075,  0.1839,\n",
            "          0.2750,  0.1159, -0.5474,  0.0373,  0.1747,  0.0356,  0.0079, -0.0555,\n",
            "          0.0445, -0.2068, -0.0403, -0.0516, -0.1757, -0.3701, -0.1971,  0.0046,\n",
            "          0.0717,  0.0194, -0.0634,  0.1844,  0.3594,  0.0738, -0.1093, -0.1986,\n",
            "          0.1530,  0.4713,  0.3614,  0.1684, -0.1718, -0.3048,  0.0408,  0.0469,\n",
            "          0.0995,  0.0420, -0.0456,  0.0756, -0.1384, -0.1453, -0.0651, -0.0618,\n",
            "          0.0739, -0.1046, -0.2996, -0.0837,  0.0044, -0.0574,  0.0359,  0.0187,\n",
            "          0.0119, -0.0907, -0.1131,  0.0622,  0.1017, -0.3710,  0.0702,  0.1364,\n",
            "          0.1952, -0.1960, -0.3186, -0.4422,  0.1875, -0.0099, -0.0579,  0.2542,\n",
            "          0.5400,  0.5134, -0.1026, -0.5259, -0.5043, -0.4688, -0.2020, -0.1204,\n",
            "          0.1782, -0.0590,  0.1834,  0.0530,  0.1920,  0.2035,  0.0140, -0.0467,\n",
            "          0.0814,  0.1448, -0.1073, -0.3176, -0.1601, -0.2278, -0.0695, -0.1125,\n",
            "         -0.1949, -0.2729,  0.1320, -0.0173, -0.0729,  0.0331,  0.0384,  0.2024,\n",
            "          0.1562,  0.1139, -0.0869,  0.4229,  0.4157, -0.1100, -0.0523,  0.2168,\n",
            "         -0.0137, -0.0510, -0.0120,  0.0217, -0.2230, -0.0639, -0.1433, -0.1976,\n",
            "         -0.0436,  0.3725,  0.3008, -0.2202, -0.1577, -0.1153,  0.0283, -0.0739,\n",
            "         -0.0101, -0.1867,  0.3106, -0.0477, -0.2587, -0.0937,  0.1437, -0.0162,\n",
            "         -0.0145, -0.0874,  0.0424, -0.0734]], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "tensor([[[  2.3032,  -7.8980,  -4.5893,  ..., -15.0030, -14.6186,  -8.0303],\n",
            "         [ -2.6228, -11.8567,  -9.2679,  ..., -15.0391, -14.6351, -11.9981],\n",
            "         [ -4.8663, -12.3069, -11.2832,  ..., -15.1743, -14.5157, -12.4051],\n",
            "         ...,\n",
            "         [ -2.6843,  -3.8285,  -4.4499,  ..., -14.1239, -15.0228,  -3.9914],\n",
            "         [ -2.3588,  -4.1263,  -4.3579,  ..., -14.0856, -15.1861,  -4.2500],\n",
            "         [ -3.2697,  -5.9801,  -3.5235,  ..., -13.5506, -15.0628,  -6.0077]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([[ 0, 16,  4,  7, 16,  8,  6,  5,  9,  7, 15, 15, 14,  6,  8,  8,  7, 15,\n",
            "          7,  8, 23, 15,  5,  8,  6,  6, 11, 18, 17,  8, 19,  5, 18,  8, 22,  7,\n",
            "         10, 16,  5, 14,  6, 16,  6,  4,  9, 22, 20,  6,  8, 12, 12, 14,  4, 18,\n",
            "          6, 18,  7,  7, 19,  5, 16, 15, 18, 16,  6, 10,  7, 11, 12, 11,  5, 13,\n",
            "          9,  8, 11,  8, 11,  5, 19, 20,  9,  4,  8,  8,  4, 10,  8,  9, 13, 11,\n",
            "          5,  7, 19, 19, 23,  5, 10, 19, 18, 13, 11, 19, 17, 17, 19,  6, 18,  5,\n",
            "         17, 22,  6, 16,  6, 11,  4,  7, 11,  7,  8,  8,  5,  8, 11, 15,  6, 14,\n",
            "          8,  7, 18, 14,  4,  5, 14,  8,  8,  6, 11,  5,  5,  4,  6, 23,  4,  7,\n",
            "         15, 13, 19, 18, 14,  9, 14,  7, 11,  7,  8, 22, 17,  8,  6,  5,  4, 11,\n",
            "          8,  6,  7, 21, 11, 18, 14,  5,  7,  4, 16,  8,  8,  6,  4, 19,  8,  4,\n",
            "          8,  8,  7,  7, 11,  7, 14,  8,  8,  8,  4,  6, 11, 16, 11, 19, 12, 23,\n",
            "         17,  7, 17, 21, 15, 14,  8, 17, 11, 15,  7, 13, 15, 15,  7,  9, 14, 15,\n",
            "          8,  9, 18,  2]], device='cuda:0')\n",
            "total loss: tensor(5.7960, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "l2 loss: tensor(2.3940, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "ce loss: tensor(3.3840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "binding loss: tensor(0.0180, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "initial energy score shape torch.Size([1, 747])\n",
            "intiial prot emb shape torch.Size([1, 747, 1280])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36m<cell line: 854>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initial energy score shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menergy_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'intiial prot emb shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotein_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeptide_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotein_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotif_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36mp_losses\u001b[0;34m(self, x0, protein_emb, motif_emb, t, target_seq, energy_scores, noise)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m       \u001b[0mx_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m       \u001b[0mpredicted_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinding_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_noisy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotein_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotif_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m       \u001b[0ml2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# l2 loss objective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, protein_emb, motif_emb, t)\u001b[0m\n\u001b[1;32m    501\u001b[0m         )):\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_checkpoint\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 x = torch.utils.checkpoint.checkpoint(\n\u001b[0m\u001b[1;32m    504\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresampled_motif\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresampled_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive_ln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36m_forward_layer\u001b[0;34m(self, x, resampled_motif, resampled_combined, layer, res_layer, norm, adaptive_ln, se_block, t)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mse_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e87623c91a91>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import re\n",
        "import esm\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "import wandb\n",
        "wandb.login()\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/')\n",
        "\n",
        "# ESM Model Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model = esm_model.to(device)\n",
        "esm_model.eval()\n",
        "for param in esm_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[dataset['protein_RCSB'] != dataset['peptide_source_RCSB']]\n",
        "\n",
        "# Dataset Class\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        peptide_seq = row['peptide_derived_sequence']\n",
        "        protein_seq = row['protein_derived_sequence']\n",
        "        energy_scores = row['energy_scores']\n",
        "\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = self.one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        # Convert energy scores to tensor\n",
        "        energy_scores = torch.tensor(energy_scores, dtype=torch.float32)\n",
        "\n",
        "        return energy_scores, protein_seq, peptide_seq\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hot_encode_energy_scores(scores):\n",
        "        return [1 if score <= -1 else 0 for score in scores]\n",
        "\n",
        "# Model Components\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=3, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=padding)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (b, n, d)\n",
        "        x = x.transpose(1, 2)  # (b, d, n)\n",
        "        x = self.conv(x)\n",
        "        x = x.transpose(1, 2)  # (b, n, d)\n",
        "        x = self.norm(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]\n",
        "\n",
        "class FourierFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, num_frequencies, d_model):\n",
        "        super().__init__()\n",
        "        self.num_frequencies = num_frequencies\n",
        "        self.d_model = d_model\n",
        "        self.frequencies = nn.Parameter(torch.randn(num_frequencies) * 2 * math.pi)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).float()\n",
        "        features = positions.unsqueeze(-1) * self.frequencies.unsqueeze(0).unsqueeze(0)\n",
        "        features = torch.cat([torch.sin(features), torch.cos(features)], dim=-1)\n",
        "        return features.view(1, x.size(1), -1)[:, :, :self.d_model]\n",
        "\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        inv_freq = 1. / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb[None, :, :]\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(x, sincos):\n",
        "    sin, cos = map(lambda t: t.repeat_interleave(2, dim=-1), sincos.chunk(2, dim=-1))\n",
        "    return (x * cos) + (rotate_half(x) * sin)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        b, n, h = *x.shape[:2], self.heads\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "\n",
        "        kv_input = torch.cat((x, latents), dim=-2)\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim, mult=ff_mult)\n",
        "            ]))\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _ = x.shape\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=b)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return self.norm(latents)\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim=dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media):\n",
        "        x = self.attn(media, x) * self.attn_gate.tanh() + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n",
        "class AdaptiveLayerNorm(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(num_features)\n",
        "        self.alpha = nn.Parameter(torch.ones(1, 1, num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(1, 1, num_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        return self.alpha * x + self.beta\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, in_channels // 2, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels // 2, in_channels // 4, kernel_size=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels // 4, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x.transpose(1, 2))\n",
        "        x2 = self.conv2(x1)\n",
        "        x3 = self.conv3(x2)\n",
        "        return x3.transpose(1, 2) + x\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, drop_prob=0.1, mode=\"row\"):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.drop_prob == 0.:\n",
        "            return x\n",
        "\n",
        "        if self.mode == \"row\":\n",
        "            shape = (x.shape[0], 1, 1)\n",
        "        else:\n",
        "            shape = (1, x.shape[1], 1)\n",
        "\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()  # binarize\n",
        "        output = x.div(keep_prob) * random_tensor\n",
        "        return output\n",
        "\n",
        "class ConditionalBatchNorm1d(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.bn = nn.BatchNorm1d(num_features, affine=False)\n",
        "        self.embed = nn.Embedding(num_classes, num_features * 2)\n",
        "        self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialize scale at 1\n",
        "        self.embed.weight.data[:, num_features:].zero_()  # Initialize bias at 0\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.bn(x)\n",
        "        gamma, beta = self.embed(y).chunk(2, 1)\n",
        "        out = gamma.view(-1, self.num_features, 1) * out + beta.view(-1, self.num_features, 1)\n",
        "        return out\n",
        "\n",
        "# Main Diffusion Model\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, esm_model, num_steps, latent_dim, motif_dim):\n",
        "        super().__init__()\n",
        "        self.esm_model = esm_model\n",
        "        self.num_steps = num_steps\n",
        "        self.latent_dim = latent_dim\n",
        "        self.motif_dim = motif_dim\n",
        "        self.hidden_dim = 256\n",
        "        self.project_combined = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
        "        self.project_to_esm = nn.Linear(self.hidden_dim, 1280)\n",
        "        self.project_from_esm = nn.Linear(1280, self.hidden_dim)\n",
        "\n",
        "        # Projection layers\n",
        "        self.project_to_hidden = nn.Linear(latent_dim, self.hidden_dim)\n",
        "        self.project_from_hidden = nn.Linear(self.hidden_dim, latent_dim)\n",
        "        self.project_motif = nn.Linear(motif_dim, self.hidden_dim)\n",
        "\n",
        "        # Add ESM-2 attention layers\n",
        "        self.esm_attention_layers = nn.ModuleList([\n",
        "            esm_model.layers[-i] for i in range(1, 9)  # Use the last 3 layers\n",
        "        ])\n",
        "        # Get the expected dimension for rotary embedding\n",
        "        self.rotary_dim = esm_model.layers[0].self_attn.rotary_emb.dim if hasattr(esm_model.layers[0].self_attn, 'rotary_emb') else 64  # default to 64 if not found\n",
        "        # Projection layers for rotary embedding\n",
        "        self.embed_dim = esm_model.layers[0].self_attn.embed_dim\n",
        "        self.to_rotary_dim = nn.Linear(self.embed_dim, self.rotary_dim)\n",
        "        self.from_rotary_dim = nn.Linear(self.rotary_dim, self.embed_dim)\n",
        "\n",
        "        # MLP for processing\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.hidden_dim * 4, self.hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.mlp_final = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(latent_dim* 4, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Add & Norm layer\n",
        "        self.add_norm = nn.LayerNorm(self.hidden_dim)\n",
        "\n",
        "        # Scale & Shift layer\n",
        "        self.scale_shift = nn.Linear(self.hidden_dim, self.hidden_dim * 2)\n",
        "\n",
        "        self.motif_resampler = PerceiverResampler(dim=self.hidden_dim, depth=6)\n",
        "        self.combined_resampler = PerceiverResampler(dim=self.hidden_dim, depth=6)\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, self.hidden_dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.hidden_dim * 4, self.hidden_dim)\n",
        "        )\n",
        "        self.positional_encoding = PositionalEncoding(self.hidden_dim)\n",
        "        self.rope = RotaryPositionalEmbedding(self.hidden_dim)\n",
        "        self.fourier_emb = FourierFeatureEmbedding(num_frequencies=self.hidden_dim//2, d_model=self.hidden_dim)\n",
        "\n",
        "        self.denoiser = nn.ModuleList([\n",
        "            GatedCrossAttentionBlock(self.hidden_dim, self.hidden_dim) for _ in range(6)\n",
        "        ])\n",
        "\n",
        "        self.residual_layers = nn.ModuleList([\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim) for _ in range(6)\n",
        "        ])\n",
        "\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.hidden_dim) for _ in range(6)\n",
        "        ])\n",
        "\n",
        "        self.adaptive_ln = nn.ModuleList([\n",
        "            AdaptiveLayerNorm(self.hidden_dim) for _ in range(6)\n",
        "        ])\n",
        "\n",
        "        self.se_blocks = nn.ModuleList([\n",
        "            SqueezeExcitation(self.hidden_dim) for _ in range(6)\n",
        "        ])\n",
        "\n",
        "        self.final_layer = nn.Linear(latent_dim, latent_dim)\n",
        "        self.binding_predictor = nn.Linear(latent_dim, 1)\n",
        "\n",
        "        self.transformer_denoiser = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=self.hidden_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "\n",
        "        self.glu = nn.GLU()\n",
        "        self.fpn = FeaturePyramidNetwork(latent_dim)\n",
        "        self.focal_loss = FocalLoss(alpha=0.25, gamma=5)\n",
        "        self.stochastic_depth = StochasticDepth(drop_prob=0.1, mode=\"row\")\n",
        "        self.cond_bn = ConditionalBatchNorm1d(latent_dim, num_steps)\n",
        "\n",
        "        self.time_attention = nn.MultiheadAttention(self.hidden_dim, num_heads=8)\n",
        "\n",
        "        self.use_checkpoint = True\n",
        "        self.upsample = lambda x: F.interpolate(x.transpose(1, 2), scale_factor=2, mode='linear', align_corners=False).transpose(1, 2)\n",
        "        self.downsample = lambda x: F.avg_pool1d(x.transpose(1, 2), kernel_size=2, stride=2).transpose(1, 2)\n",
        "\n",
        "    def forward(self, x, protein_emb, motif_emb, t):\n",
        "\n",
        "        # Project inputs to hidden dimension\n",
        "        x = self.project_to_hidden(x)\n",
        "        protein_emb = self.project_to_hidden(protein_emb)\n",
        "        motif_emb = self.project_motif(motif_emb)\n",
        "\n",
        "        t_embedding = self.time_embed(t.float().unsqueeze(-1))\n",
        "\n",
        "        ## positional embeddings\n",
        "        rope_emb = self.rope(x)\n",
        "        x = apply_rotary_pos_emb(x, rope_emb)\n",
        "        fourier_emb = self.fourier_emb(x)\n",
        "        x = self.positional_encoding(x) + fourier_emb\n",
        "\n",
        "        # Apply time attention\n",
        "        x_t = x + t_embedding.unsqueeze(1)\n",
        "        x_t, _ = self.time_attention(x_t, x_t, x_t)\n",
        "\n",
        "        resampled_motif = self.motif_resampler(motif_emb)\n",
        "\n",
        "        # Process combined protein and motif information\n",
        "        combined = torch.cat([protein_emb, motif_emb], dim=-1)\n",
        "        combined = self.project_combined(combined)\n",
        "        resampled_combined = self.combined_resampler(combined)\n",
        "\n",
        "        scale_shift = self.scale_shift(t_embedding).unsqueeze(1)\n",
        "        scale, shift = scale_shift.chunk(2, dim=-1)\n",
        "        x = x * (scale + 1) + shift\n",
        "\n",
        "        # x = self.transformer_denoiser(x) # not needed if we are doing ESM\n",
        "\n",
        "        # Project to ESM dimension before attention layers\n",
        "        x = self.project_to_esm(x)\n",
        "\n",
        "        ## esm layers\n",
        "        # # Convert latent representations to token probabilities\n",
        "        # token_probs = self.to_tokens(x).softmax(dim=-1)\n",
        "        # # Use argmax to get token IDs (you might want to use sampling for more diversity)\n",
        "        # token_ids = token_probs.argmax(dim=-1)\n",
        "        # Apply ESM attention layers\n",
        "        for esm_layer in self.esm_attention_layers:\n",
        "            # Create attention mask (\n",
        "            attention_mask = torch.ones(x.shape[0], x.shape[1], dtype=torch.bool, device=x.device)\n",
        "\n",
        "            # Extract only the self-attention part of the ESM layer\n",
        "            self_attn = esm_layer.self_attn\n",
        "\n",
        "            # Transpose x to match ESM's expected format: (seq_length, batch_size, embed_dim)\n",
        "            x = x.transpose(0, 1)\n",
        "\n",
        "            # Apply self-attention\n",
        "            residual = x\n",
        "            x = esm_layer.self_attn_layer_norm(x)\n",
        "            x, _ = esm_layer.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=~attention_mask,\n",
        "                need_weights=False\n",
        "            )\n",
        "            x = residual + x\n",
        "\n",
        "            # Apply feed-forward network\n",
        "            residual = x\n",
        "            x = esm_layer.final_layer_norm(x)\n",
        "            x = esm_layer.fc1(x)\n",
        "            x = F.gelu(x)\n",
        "            x = esm_layer.fc2(x)\n",
        "            x = residual + x\n",
        "\n",
        "            # Transpose x back to (batch_size, seq_length, embed_dim)\n",
        "            x = x.transpose(0, 1)\n",
        "\n",
        "        # Project back to hidden dimension after attention layers\n",
        "        x = self.project_from_esm(x)\n",
        "\n",
        "        x = self.add_norm(x + self.mlp(x))\n",
        "\n",
        "        for i, (layer, res_layer, norm, adaptive_ln, se_block) in enumerate(zip(\n",
        "            self.denoiser, self.residual_layers, self.layer_norms, self.adaptive_ln, self.se_blocks\n",
        "        )):\n",
        "            if self.use_checkpoint and self.training:\n",
        "                x = torch.utils.checkpoint.checkpoint(\n",
        "                    self._forward_layer, x, resampled_motif, resampled_combined, layer, res_layer, norm, adaptive_ln, se_block, t\n",
        "                )\n",
        "            else:\n",
        "                x = self._forward_layer(x, resampled_motif, resampled_combined, layer, res_layer, norm, adaptive_ln, se_block, t)\n",
        "\n",
        "            if i % 2 == 0:\n",
        "                # Upsample sequence length without changing hidden dimension\n",
        "                x = F.interpolate(x.transpose(1, 2), scale_factor=2, mode='linear', align_corners=False).transpose(1, 2)\n",
        "            else:\n",
        "                # Downsample sequence length without changing hidden dimension\n",
        "                x = F.avg_pool1d(x.transpose(1, 2), kernel_size=2, stride=2).transpose(1, 2)\n",
        "\n",
        "        x = self.project_from_hidden(x)\n",
        "        # x = self.glu(x)\n",
        "        # print('x shape after glu:', x.shape)\n",
        "        x = self.fpn(x)\n",
        "        x = self.mlp_final(x)\n",
        "        x = self.cond_bn(x.transpose(1, 2), t).transpose(1, 2)\n",
        "        x = self.stochastic_depth(x)\n",
        "\n",
        "        x = self.final_layer(x)\n",
        "        binding_pred = self.binding_predictor(x).squeeze(-1)\n",
        "\n",
        "        return x, binding_pred\n",
        "\n",
        "    def _forward_layer(self, x, resampled_motif, resampled_combined, layer, res_layer, norm, adaptive_ln, se_block, t):\n",
        "      residual = res_layer(x)\n",
        "\n",
        "      x = layer(x, resampled_motif)\n",
        "      x = layer(x, resampled_combined)\n",
        "      x = norm(x + residual)\n",
        "      x = adaptive_ln(x)\n",
        "      x = se_block(x.transpose(1, 2)).transpose(1, 2)\n",
        "      return x\n",
        "\n",
        "    def loss_function(self, pred, target):\n",
        "        return self.focal_loss(pred, target)\n",
        "\n",
        "class LatentDiffusion(nn.Module):\n",
        "    def __init__(self, esm_model, num_steps, latent_dim, motif_dim, device):\n",
        "        super().__init__()\n",
        "        self.esm_model = esm_model\n",
        "        self.num_steps = num_steps\n",
        "        self.latent_dim = latent_dim\n",
        "        self.motif_dim = motif_dim\n",
        "        self.diffusion_model = DiffusionModel(esm_model, num_steps, latent_dim=1280, motif_dim=1)\n",
        "        self.device = device\n",
        "        self.hidden_dim=256\n",
        "\n",
        "        # Define beta schedule\n",
        "        self.beta = torch.linspace(1e-4, 0.02, num_steps).to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0).to(self.device)\n",
        "        return (\n",
        "            self.sqrt_alpha_bar[t, None, None] * x0 +\n",
        "            self.sqrt_one_minus_alpha_bar[t, None, None] * noise\n",
        "        )\n",
        "\n",
        "    def p_losses(self, x0, protein_emb, motif_emb, t, target_seq, energy_scores, noise=None):\n",
        "      if noise is None:\n",
        "          noise = torch.randn_like(x0).to(self.device)\n",
        "\n",
        "      x_noisy = self.q_sample(x0, t, noise=noise)\n",
        "      predicted_noise, binding_pred = self.diffusion_model(x_noisy, protein_emb, motif_emb, t)\n",
        "\n",
        "      l2_loss = F.mse_loss(predicted_noise, noise) # l2 loss objective\n",
        "      binding_loss = self.diffusion_model.loss_function(binding_pred, energy_scores)\n",
        "      print(binding_pred)\n",
        "\n",
        "\n",
        "      # esm logits loss (ce)\n",
        "      esm_logits = self.esm_model.lm_head(predicted_noise)\n",
        "      # Cross-entropy loss for ESM logits\n",
        "      ce_loss = F.cross_entropy(esm_logits.view(-1, esm_logits.size(-1)), target_seq.view(-1))\n",
        "      print(esm_logits)\n",
        "      print(target_seq)\n",
        "\n",
        "      total_loss = (1.5*l2_loss) + ce_loss + (3*binding_loss)\n",
        "      print(\"total loss:\", total_loss)\n",
        "      print(\"l2 loss:\", (1.5*l2_loss))\n",
        "      print(\"ce loss:\", ce_loss)\n",
        "      print(\"binding loss:\", (3*binding_loss))\n",
        "\n",
        "      return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x, motif, t):\n",
        "        betas_t = self.beta[t][:, None, None]\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alpha_bar[t][:, None, None]\n",
        "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alpha[t])[:, None, None]\n",
        "\n",
        "        model_mean, _ = self.diffusion_model(x, motif, t)\n",
        "        model_mean = sqrt_recip_alphas_t * (\n",
        "            x - betas_t * model_mean / sqrt_one_minus_alphas_cumprod_t\n",
        "        )\n",
        "\n",
        "        if t[0] > 0:\n",
        "            noise = torch.randn_like(x).to(self.device)\n",
        "            return model_mean + torch.sqrt(betas_t) * noise\n",
        "        else:\n",
        "            return model_mean\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, num_samples, sequence_length, motif):\n",
        "        device = next(self.parameters()).device\n",
        "        shape = (num_samples, sequence_length, self.latent_dim)\n",
        "        x = torch.randn(shape, device=device)\n",
        "        motif = motif.to(device)\n",
        "\n",
        "        for t in reversed(range(0, self.num_steps)):\n",
        "            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "            x = self.p_sample(x, motif, t_batch)\n",
        "\n",
        "        return x\n",
        "\n",
        "def embeddings_to_sequence(embeddings, esm_model, alphabet):\n",
        "    aa_toks = alphabet.all_toks\n",
        "    aa_idxs = [alphabet.get_idx(aa) for aa in aa_toks]\n",
        "    aa_logits = esm_model.lm_head(embeddings)[:, :, aa_idxs]\n",
        "    predictions = torch.argmax(aa_logits, dim=-1).tolist()\n",
        "    generated_peptides = [''.join([aa_toks[pred] for pred in seq]) for seq in predictions]\n",
        "    return generated_peptides\n",
        "\n",
        "def calculate_sequence_recovery(generated_sequences, ground_truth_sequences):\n",
        "    correct = sum(gen == gt for gen, gt in zip(generated_sequences, ground_truth_sequences))\n",
        "    total = len(ground_truth_sequences)\n",
        "    return correct / total\n",
        "\n",
        "def train(model, dataloader, optimizer, num_epochs, device):\n",
        "\n",
        "    wandb.init(project=\"latent-diffusion-protein\", entity=\"vskavi2003\")\n",
        "    wandb.config.update({\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": dataloader.batch_size\n",
        "    })\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "\n",
        "            # Tokenize protein and peptide sequences\n",
        "            batch_converter = model.esm_model.alphabet.get_batch_converter()\n",
        "\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            protein_tokens = protein_tokens.to(device)\n",
        "\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "            peptide_tokens = peptide_tokens.to(device)\n",
        "            target_seq = peptide_tokens\n",
        "\n",
        "            # Encode protein and peptide sequences\n",
        "            with torch.no_grad():\n",
        "                protein_embedding = model.esm_model(protein_tokens, repr_layers=[33], return_contacts=False)\n",
        "                peptide_embedding = model.esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)\n",
        "\n",
        "            protein_embedding = protein_embedding[\"representations\"][33]\n",
        "            peptide_embedding = peptide_embedding[\"representations\"][33]\n",
        "\n",
        "            # Process motif embeddings\n",
        "            motif_embeddings = energy_scores.unsqueeze(-1).float()\n",
        "            # Pad sequences to the same length\n",
        "            max_len = max(protein_embedding.shape[1], motif_embeddings.shape[1], peptide_embedding.shape[1])\n",
        "\n",
        "            protein_embedding = F.pad(protein_embedding, (0, 0, 0, max_len - protein_embedding.shape[1]))\n",
        "            motif_embeddings = F.pad(motif_embeddings, (0, 0, 0, max_len - motif_embeddings.shape[1]))\n",
        "            peptide_embedding = F.pad(peptide_embedding, (0, 0, 0, max_len - peptide_embedding.shape[1]))\n",
        "            energy_scores = F.pad(energy_scores, (0, max_len - energy_scores.shape[1], 0, 0))\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            print('initial energy score shape',energy_scores.shape)\n",
        "            print('intiial prot emb shape',protein_embedding.shape)\n",
        "            loss = model.p_losses(peptide_embedding, protein_embedding, motif_embeddings, t, target_seq, energy_scores)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Log batch loss\n",
        "            wandb.log({\"batch_loss training\": loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")\n",
        "\n",
        "        # Log epoch loss\n",
        "        wandb.log({\"epoch\": epoch+1, \"avg_loss training\": avg_loss})\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "\n",
        "            # Tokenize protein and peptide sequences\n",
        "            batch_converter = model.esm_model.alphabet.get_batch_converter()\n",
        "\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            protein_tokens = protein_tokens.to(device)\n",
        "\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "            peptide_tokens = peptide_tokens.to(device)\n",
        "            target_seq = peptide_tokens\n",
        "\n",
        "            # Encode protein and peptide sequences\n",
        "            with torch.no_grad():\n",
        "                protein_embedding = model.esm_model(protein_tokens, repr_layers=[33], return_contacts=False)\n",
        "                peptide_embedding = model.esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)\n",
        "\n",
        "            protein_embedding = protein_embedding[\"representations\"][33]\n",
        "            peptide_embedding = peptide_embedding[\"representations\"][33]\n",
        "\n",
        "            # Process motif embeddings\n",
        "            motif_embeddings = energy_scores.unsqueeze(-1).float()\n",
        "            # Pad sequences to the same length\n",
        "            max_len = max(protein_embedding.shape[1], motif_embeddings.shape[1], peptide_embedding.shape[1])\n",
        "\n",
        "            protein_embedding = F.pad(protein_embedding, (0, 0, 0, max_len - protein_embedding.shape[1]))\n",
        "            motif_embeddings = F.pad(motif_embeddings, (0, 0, 0, max_len - motif_embeddings.shape[1]))\n",
        "            peptide_embedding = F.pad(peptide_embedding, (0, 0, 0, max_len - peptide_embedding.shape[1]))\n",
        "            energy_scores = F.pad(energy_scores, (0, max_len - energy_scores.shape[1], 0, 0))\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "\n",
        "            loss = model.p_losses(peptide_embedding, protein_embedding, motif_embeddings, t, target_seq)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "    val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "    test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "    train_snp = filter_datasets(train_snp)\n",
        "    val_snp = filter_datasets(val_snp)\n",
        "    test_snp = filter_datasets(test_snp)\n",
        "\n",
        "    # Calculate max_length\n",
        "    all_seqs = pd.concat([\n",
        "        train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "        val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence'],\n",
        "        test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence']\n",
        "    ])\n",
        "    max_length = max(len(seq) for seq in all_seqs)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ProteinInteractionDataset(train_snp)\n",
        "    val_dataset = ProteinInteractionDataset(val_snp)\n",
        "    test_dataset = ProteinInteractionDataset(test_snp)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize LatentDiffusion model\n",
        "    latent_dim = esm_model.embed_dim\n",
        "    motif_dim = max_length  # Assuming motif is represented by one-hot encoded energy scores\n",
        "    num_steps = 1000\n",
        "    model = LatentDiffusion(esm_model, num_steps, latent_dim, motif_dim, device)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    num_epochs = 8  # Increase the number of epochs for better tracking\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, optimizer, num_epochs, device)  # Train for 1 epoch\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "\n",
        "    # After training the model\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     # Generate samples for the validation set\n",
        "#     val_motifs = []\n",
        "#     val_protein_seqs = []\n",
        "#     val_peptide_seqs = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             energy_scores, protein_seq, peptide_seq = batch\n",
        "#             energy_scores = energy_scores.to(device)\n",
        "#             results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "\n",
        "#             # Encode protein sequences\n",
        "#             protein_embedding = model.esm_model(protein_seq)['last_hidden_state']\n",
        "\n",
        "#             # Generate peptide embeddings\n",
        "#             generated_embeddings = model.sample(energy_scores.size(0), max_length, energy_scores)\n",
        "\n",
        "#             val_motifs.append(energy_scores)\n",
        "#             val_protein_seqs.extend(protein_seq)\n",
        "#             val_peptide_seqs.extend(peptide_seq)\n",
        "\n",
        "#     # Concatenate all batches\n",
        "#     val_motifs = torch.cat(val_motifs, dim=0)\n",
        "\n",
        "#     # Convert generated embeddings to sequences\n",
        "#     generated_sequences = embeddings_to_sequence(generated_embeddings, esm_model, alphabet)\n",
        "\n",
        "#     print(\"Sample of generated sequences:\")\n",
        "#     for i in range(min(10, len(generated_sequences))):\n",
        "#         print(f\"Generated: {generated_sequences[i]}\")\n",
        "#         print(f\"Ground truth: {val_peptide_seqs[i]}\")\n",
        "#         print()\n",
        "\n",
        "#     # Calculate sequence recovery\n",
        "#     recovery_rate = calculate_sequence_recovery(generated_sequences, val_peptide_seqs)\n",
        "#     print(f\"Sequence recovery rate: {recovery_rate:.4f}\")\n",
        "\n",
        "#     # Calculate per-position accuracy\n",
        "#     per_position_accuracy = calculate_per_position_accuracy(generated_sequences, val_peptide_seqs)\n",
        "#     print(f\"Per-position accuracy: {per_position_accuracy:.4f}\")\n",
        "\n",
        "# def calculate_per_position_accuracy(generated_sequences, ground_truth_sequences):\n",
        "#     total_correct = 0\n",
        "#     total_positions = 0\n",
        "\n",
        "#     for gen, gt in zip(generated_sequences, ground_truth_sequences):\n",
        "#         for g, t in zip(gen, gt):\n",
        "#             if g == t:\n",
        "#                 total_correct += 1\n",
        "#             total_positions += 1\n",
        "\n",
        "#     return total_correct / total_positions if total_positions > 0 else 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}