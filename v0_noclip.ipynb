{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "k-OWRYMLzzG0",
        "lCVCpLwsz2ZY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "568b76a0326c48d9972f23240ba2e8df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93506af9c8e74e7fbb5d8e63036b59dd",
              "IPY_MODEL_30bb148261b54a8d87ba393ff9e7a655"
            ],
            "layout": "IPY_MODEL_3d600601667b4bb98d1c0966c5531c05"
          }
        },
        "93506af9c8e74e7fbb5d8e63036b59dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8746b97377147eaa0bc7dc70d7d09ad",
            "placeholder": "​",
            "style": "IPY_MODEL_8eb9d5cb7a114bd1bfae1d10d1ed2f74",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "30bb148261b54a8d87ba393ff9e7a655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae43d0adabf84fa3b3caac063213343c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31c45f47f217429c972526948d7a4d22",
            "value": 1
          }
        },
        "3d600601667b4bb98d1c0966c5531c05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8746b97377147eaa0bc7dc70d7d09ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb9d5cb7a114bd1bfae1d10d1ed2f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae43d0adabf84fa3b3caac063213343c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c45f47f217429c972526948d7a4d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pip installs"
      ],
      "metadata": {
        "id": "k-OWRYMLzzG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install black\n",
        "!pip install fair-esm\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlB53XWXSmW",
        "outputId": "3ba7bd08-8d5d-4d65-e997-de9aa8ae23ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 rapidfuzz-3.9.6\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting einops_exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops_exts) (0.8.0)\n",
            "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: einops_exts\n",
            "Successfully installed einops_exts-0.0.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting black\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, mypy-extensions, black\n",
            "Successfully installed black-24.8.0 mypy-extensions-1.0.0 pathspec-0.12.1\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.12.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "lCVCpLwsz2ZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYIaRxvBMU0G",
        "outputId": "e4bd68e1-aabd-47a1-bea8-9a1ac388d454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import re\n",
        "import esm\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import wandb\n",
        "wandb.login()\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/')\n",
        "\n",
        "# ESM Model Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm_model = esm_model.to(device)\n",
        "esm_model.eval()\n",
        "for param in esm_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[dataset['protein_RCSB'] != dataset['peptide_source_RCSB']]\n",
        "\n",
        "# Dataset Class\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.mismatched_lengths = 0\n",
        "        self.total_samples = len(dataframe)\n",
        "        self.check_lengths()\n",
        "\n",
        "    def check_lengths(self):\n",
        "        for idx in range(self.total_samples):\n",
        "            row = self.dataframe.iloc[idx]\n",
        "            peptide_seq = row['peptide_derived_sequence']\n",
        "            energy_scores = row['energy_scores']\n",
        "\n",
        "            energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "            energy_scores = [float(score) for score in energy_scores]\n",
        "\n",
        "            if len(energy_scores) != len(peptide_seq):\n",
        "                self.mismatched_lengths += 1\n",
        "\n",
        "        print(f\"Total samples: {self.total_samples}\")\n",
        "        print(f\"Mismatched lengths: {self.mismatched_lengths}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        peptide_seq = row['peptide_derived_sequence']\n",
        "        protein_seq = row['protein_derived_sequence']\n",
        "        energy_scores = row['energy_scores']\n",
        "\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = self.one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        # Convert energy scores to tensor\n",
        "        energy_scores = torch.tensor(energy_scores, dtype=torch.float32)\n",
        "\n",
        "        return energy_scores, peptide_seq, protein_seq # energy scores are aligned with the peptide (we will keep peptide as protien)\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hot_encode_energy_scores(scores):\n",
        "        return [1 if score <= -1 else 0 for score in scores]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "train_snp = filter_datasets(train_snp)\n",
        "val_snp = filter_datasets(val_snp)\n",
        "test_snp = filter_datasets(test_snp)\n",
        "\n",
        "train_snp, val_snp, test_snp = train_snp[:16], val_snp[16:24], test_snp[24:32] # subset code\n",
        "\n",
        "# Calculate max_length\n",
        "all_seqs = pd.concat([\n",
        "    train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "    val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence'],\n",
        "    test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence']\n",
        "])\n",
        "max_length = max(len(seq) for seq in all_seqs)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinInteractionDataset(train_snp)\n",
        "val_dataset = ProteinInteractionDataset(val_snp)\n",
        "test_dataset = ProteinInteractionDataset(test_snp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydU34BngfUeT",
        "outputId": "b787729d-9806-4239-f5f4-223c2d300520"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 16\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n",
            "Total samples: 8\n",
            "Mismatched lengths: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):  # Adjust range to view more samples\n",
        "    energy_scores, protein_seq, peptide_seq = train_dataset[i]\n",
        "    print(f\"Sample {i}:\")\n",
        "\n",
        "    # Print energy scores and their length\n",
        "    print(f\"Energy Scores: {energy_scores}\")\n",
        "    print(f\"Length of Energy Scores: {energy_scores.shape[0]}\")\n",
        "\n",
        "    # Print protein sequence and its length\n",
        "    print(f\"Protein Sequence: {protein_seq}\")\n",
        "    print(f\"Length of Protein Sequence: {len(protein_seq)}\")\n",
        "\n",
        "    # Print peptide sequence and its length\n",
        "    print(f\"Peptide Sequence: {peptide_seq}\")\n",
        "    print(f\"Length of Peptide Sequence: {len(peptide_seq)}\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDhIUkdKfXey",
        "outputId": "403c3b93-9322-4672-b016-b7593b74b7ae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Energy Scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 221\n",
            "Protein Sequence: EVQLVQSGAEVKKPGESLNISCKASGYSFTIYWIAWVRQLPGKGLEWMGIIYPGDSDTRYSPSFQGQVTISADKSISTAYLQWRSLKASDSAVYYCARGVAVDWYFDLWGRGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKS\n",
            "Length of Protein Sequence: 221\n",
            "Peptide Sequence: QSVLTQPPSVSGAPGQRVTISCAGSSSNIGAGFDVYWYQQLPGTAPKLLIYGNNNRPSGVPDRFSGSKSGTSASLAITGLQAEDEADYYCQSSGSVLSDLYVFGTGTKVTVLGQPKAAPSVTLFPPSSEELQANKATLVCLISDFYPGAVTVAWKADSSPVKAGVETTTPSKQSNNKYAASSYLSLTPEQWKSHRSYSCQVTHEGSTVEKTVAPTE\n",
            "Length of Peptide Sequence: 216\n",
            "\n",
            "\n",
            "Sample 1:\n",
            "Energy Scores: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 270\n",
            "Protein Sequence: NLTCDFNDVYKLEFHPNQQTSVTKLCNLTPNVLEKVTIKCGSDKLNYNLYPPTCFEEVYASRNMMHLKKIKEFVIGSSMFMRRSLTPNKINEVSFRIPPNMMPEKPIYCFCENKKTITINGSNGNPSSKKDIINRGIVEIIIPSLNEKVKGCDFTTSESTIFSKGYSINEISQDIVCTVKAHANDLIGFKCPSNYSVEPHDCFVSAFNLSGKNENLENKLKLTNIIMDHYNNTFYSRLPSLISDNWKFFCVCSKDNEKKLVFTVEASISS\n",
            "Length of Protein Sequence: 270\n",
            "Peptide Sequence: LQESGGGLVQAGGSLRLSCAASGRTFSSYGMGWFRQAPGTEREFVAAISWSGDSTYYADSVKGRFTISIDKAKNTVYLQMNSLKPEDTAVYYCAADHALVVGGTYNYWGQGTQVTVSS\n",
            "Length of Peptide Sequence: 118\n",
            "\n",
            "\n",
            "Sample 2:\n",
            "Energy Scores: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Length of Energy Scores: 110\n",
            "Protein Sequence: NMFKSKHKLDFSLVSMDQRGKHILGYELVNMGGYDLVHYDDLAYVASAHQELLKTGASGMIAYRYQKKDGEWQWLQTSSRLVYKNSKPDFVICTHRQLMDEEGHDLLGKR\n",
            "Length of Protein Sequence: 110\n",
            "Peptide Sequence: TEFISRHNIEGIFTFVDHRCVATVGYQPQELLGKNIVEFCHPEDQQLLRDSFQQVVKLKGQVLSVMFRFRSKTREWLWMRTSSFTFQNPYSDEIEYIICTNTNV\n",
            "Length of Peptide Sequence: 104\n",
            "\n",
            "\n",
            "Sample 3:\n",
            "Energy Scores: tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 132\n",
            "Protein Sequence: ANAFLLRPGSLRCKQCSFARIFKDARTKLFWISYSDGDQCASSPCQNGGSCKDQLQSYICFCLPAFEGRNCETHKDDQLICVNENGGCEQYCSDHTGTKRSCRCHEGYSLLADGVSCTPTVEYPCGKIPILE\n",
            "Length of Protein Sequence: 132\n",
            "Peptide Sequence: EPLYENSPEFTPYLETNLGQPTIQSFEQVGTKVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTVNRKSTDSPVECM\n",
            "Length of Peptide Sequence: 116\n",
            "\n",
            "\n",
            "Sample 4:\n",
            "Energy Scores: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Length of Energy Scores: 154\n",
            "Protein Sequence: SMAASRRLMKELEEIRKCGMKNFRNIQVDEANLLTWQGLIVPDNPPYDKGAFRIEINFPAEYPFKPPKITFKTKIYHPNIDEKGQVCLPVISAENWKPATKTDQVIQSLIALVNDPQPEHPLRADLAEEYSKDRKKFCKNAEEFTKKYGEKRPV\n",
            "Length of Protein Sequence: 154\n",
            "Peptide Sequence: GSEFQECAVCGWALPHNRMQALTSCECTICPDCFRQHFTIALKEKHITDMVCPACGRPDLTDDTQLLSYFSTLDIQLRESLEPDAYALFHKKLTEGVLMRD\n",
            "Length of Peptide Sequence: 101\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "b5j2iJtxz8rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RefinedESMEncoderDecoder(nn.Module):\n",
        "    def __init__(self, esm_dim=1280, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(esm_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=latent_dim, nhead=8),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.final_layer = nn.Linear(latent_dim, len(esm_model.alphabet))\n",
        "\n",
        "    def encode(self, x):\n",
        "        return torch.tanh(self.encoder(x))\n",
        "\n",
        "    def decode(self, latent):\n",
        "        decoded = self.decoder(latent, latent)\n",
        "        return self.final_layer(decoded)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encode(x)\n",
        "        return self.decode(latent)\n",
        "\n",
        "class RefinedDenoiser(nn.Module):\n",
        "    def __init__(self, latent_dim, protein_dim, alphabet_size):\n",
        "        super().__init__()\n",
        "        self.protein_binder_transformer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=8)\n",
        "        self.target_protein_transformer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=8)\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=8)\n",
        "        self.onehot_projection = nn.Linear(alphabet_size, latent_dim)\n",
        "        self.final_layer = nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, protein_emb, motif_emb, onehot_seq, t, use_classifier_free=False):\n",
        "        if use_classifier_free:\n",
        "            protein_emb = torch.zeros_like(protein_emb)\n",
        "            motif_emb = torch.zeros_like(motif_emb)\n",
        "            onehot_seq = torch.zeros_like(onehot_seq)\n",
        "\n",
        "        seq_len = x.size(1)\n",
        "        protein_emb = F.interpolate(protein_emb.transpose(1, 2), size=seq_len, mode='linear', align_corners=False).transpose(1, 2)\n",
        "\n",
        "        x = self.protein_binder_transformer(x)\n",
        "        protein_emb = self.target_protein_transformer(protein_emb)\n",
        "\n",
        "        x = x.permute(1, 0, 2)\n",
        "        protein_emb = protein_emb.permute(1, 0, 2)\n",
        "\n",
        "        x, _ = self.cross_attention(x, protein_emb, protein_emb)\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        onehot_proj = self.onehot_projection(onehot_seq)\n",
        "        x = x + motif_emb + onehot_proj\n",
        "        return self.final_layer(x)\n",
        "\n",
        "class SimpleLatentDiffusion(nn.Module):\n",
        "    def __init__(self, esm_model, num_steps, latent_dim, protein_dim, device):\n",
        "        super().__init__()\n",
        "        self.esm_model = esm_model\n",
        "        self.num_steps = num_steps\n",
        "        self.latent_dim = latent_dim\n",
        "        self.protein_dim = protein_dim\n",
        "        self.device = device\n",
        "        self.alphabet_size = len(esm_model.alphabet)\n",
        "\n",
        "        self.esm_encoder_decoder = RefinedESMEncoderDecoder(esm_dim=1280, latent_dim=latent_dim)\n",
        "        self.denoiser = RefinedDenoiser(latent_dim=latent_dim, protein_dim=protein_dim, alphabet_size=self.alphabet_size)\n",
        "\n",
        "        self.beta = torch.linspace(1e-4, 0.02, num_steps).to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0).to(self.device)\n",
        "        return (\n",
        "            self.sqrt_alpha_bar[t, None, None] * x0 +\n",
        "            self.sqrt_one_minus_alpha_bar[t, None, None] * noise\n",
        "        )\n",
        "\n",
        "    def p_losses(self, peptide_latent, protein_emb, motif_emb, onehot_seq, t, target_seq, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(peptide_latent).to(self.device)\n",
        "\n",
        "        x_noisy = self.q_sample(peptide_latent, t, noise=noise)\n",
        "\n",
        "        batch_size, seq_len, _ = x_noisy.shape\n",
        "        protein_emb = F.interpolate(protein_emb.transpose(1, 2), size=seq_len, mode='linear', align_corners=False).transpose(1, 2)\n",
        "        motif_emb = motif_emb.expand(-1, seq_len, -1)\n",
        "        onehot_seq = F.interpolate(onehot_seq.float().transpose(1, 2), size=seq_len, mode='linear', align_corners=False).transpose(1, 2)\n",
        "\n",
        "        use_classifier_free = random.random() < 0.1\n",
        "        predicted_noise = self.denoiser(x_noisy, protein_emb, motif_emb, onehot_seq, t, use_classifier_free)\n",
        "\n",
        "        loss = F.mse_loss(predicted_noise, noise)\n",
        "\n",
        "        decoded_seq = self.esm_encoder_decoder.decode(peptide_latent)\n",
        "        ce_loss = F.cross_entropy(decoded_seq.view(-1, decoded_seq.size(-1)), target_seq.view(-1))\n",
        "\n",
        "        total_loss = loss + ce_loss\n",
        "        return total_loss\n",
        "\n",
        "def pad_or_truncate(tensor, target_length, pad_value=0):\n",
        "    current_length = tensor.size(1)\n",
        "    if current_length < target_length:\n",
        "        padding = torch.full((tensor.size(0), target_length - current_length, *tensor.size()[2:]), pad_value, device=tensor.device)\n",
        "        return torch.cat([tensor, padding], dim=1)\n",
        "    else:\n",
        "        return tensor[:, :target_length]\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
        "    wandb.init(project=\"simple_latent_diffusion\", entity=\"vskavi2003\")\n",
        "    wandb.config.update({\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": train_loader.batch_size\n",
        "    })\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            padded_energy_scores = F.pad(energy_scores, (1, 1), value=0)\n",
        "\n",
        "            batch_converter = model.esm_model.alphabet.get_batch_converter()\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "\n",
        "            max_seq_len = max(protein_tokens.size(1), peptide_tokens.size(1), padded_energy_scores.size(1))\n",
        "            protein_tokens = pad_or_truncate(protein_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            padded_energy_scores = pad_or_truncate(padded_energy_scores, max_seq_len)\n",
        "\n",
        "            protein_tokens = protein_tokens.to(device)\n",
        "            peptide_tokens = peptide_tokens.to(device)\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(model.esm_model.alphabet)).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                protein_embedding = model.esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "                peptide_embedding = model.esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "            peptide_latent = model.esm_encoder_decoder.encoder(peptide_embedding)\n",
        "            motif_emb = (padded_energy_scores <= -1).float().unsqueeze(-1)\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            loss = model.p_losses(peptide_latent, protein_latent, motif_emb, protein_onehot, t, peptide_tokens)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            energy_scores, protein_seq, peptide_seq = batch\n",
        "            energy_scores = energy_scores.to(device)\n",
        "            padded_energy_scores = F.pad(energy_scores, (1, 1), value=0)\n",
        "\n",
        "            batch_converter = model.esm_model.alphabet.get_batch_converter()\n",
        "            _, _, protein_tokens = batch_converter([(0, protein_seq[0])])\n",
        "            _, _, peptide_tokens = batch_converter([(0, peptide_seq[0])])\n",
        "\n",
        "            max_seq_len = max(protein_tokens.size(1), peptide_tokens.size(1), padded_energy_scores.size(1))\n",
        "            protein_tokens = pad_or_truncate(protein_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            peptide_tokens = pad_or_truncate(peptide_tokens, max_seq_len, pad_value=model.esm_model.alphabet.padding_idx)\n",
        "            padded_energy_scores = pad_or_truncate(padded_energy_scores, max_seq_len)\n",
        "\n",
        "            protein_tokens = protein_tokens.to(device)\n",
        "            peptide_tokens = peptide_tokens.to(device)\n",
        "            protein_onehot = F.one_hot(protein_tokens, num_classes=len(model.esm_model.alphabet)).float()\n",
        "\n",
        "            protein_embedding = model.esm_model(protein_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "            peptide_embedding = model.esm_model(peptide_tokens, repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "            protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "            peptide_latent = model.esm_encoder_decoder.encoder(peptide_embedding)\n",
        "            motif_emb = (padded_energy_scores <= -1).float().unsqueeze(-1)\n",
        "\n",
        "            t = torch.randint(0, model.num_steps, (protein_embedding.shape[0],), device=device).long()\n",
        "            loss = model.p_losses(peptide_latent, protein_latent, motif_emb, protein_onehot, t, peptide_tokens)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss_plot.png')\n",
        "    wandb.log({\"loss_plot\": wandb.Image('loss_plot.png')})\n",
        "\n",
        "def generate_protein_binders(model, protein_seq, motif, num_samples=1, guidance_scale=3.0):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Generate protein embedding\n",
        "    with torch.no_grad():\n",
        "        protein_tokens = model.esm_model.encode(protein_seq)\n",
        "        protein_embedding = model.esm_model(protein_tokens.to(device), repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "\n",
        "    # Process motif based on input type\n",
        "    if isinstance(motif, str):\n",
        "        # Motif is a sequence\n",
        "        motif_tokens = model.esm_model.encode(motif)\n",
        "        with torch.no_grad():\n",
        "            motif_embedding = model.esm_model(motif_tokens.to(device), repr_layers=[33], return_contacts=False)[\"representations\"][33]\n",
        "        motif_repr = model.refined_representation(motif_tokens, torch.ones_like(motif_embedding[:, :, 0]))\n",
        "    elif isinstance(motif, torch.Tensor):\n",
        "        # Motif is a binary tensor\n",
        "        motif_embedding = motif.float().to(device)\n",
        "        motif_repr = model.refined_representation(protein_tokens, motif_embedding)\n",
        "    else:\n",
        "        raise ValueError(\"Motif must be either a string (sequence) or a torch.Tensor (binary representation)\")\n",
        "\n",
        "    # Create refined representations\n",
        "    protein_repr = model.refined_representation(protein_tokens, torch.zeros_like(protein_embedding[:, :, 0]))\n",
        "\n",
        "    # Process through ESM encoder-decoder\n",
        "    protein_latent = model.esm_encoder_decoder.encoder(protein_embedding)\n",
        "    motif_latent = model.esm_encoder_decoder.encoder(motif_embedding)\n",
        "\n",
        "    # Sample from the model\n",
        "    latent_samples = model.sample(num_samples, protein_latent.shape[1], protein_latent, motif_latent, guidance_scale)\n",
        "\n",
        "    # Decode the latent samples to amino acid sequences\n",
        "    generated_sequences = model.esm_encoder_decoder.decoder(latent_samples)\n",
        "\n",
        "    return generated_sequences\n",
        "\n",
        "def generate_protein_binders_without_guidance(model, sequence_length, num_samples=1):\n",
        "    # Sample from the model without guidance\n",
        "    latent_samples = model.sample_without_guidance(num_samples, sequence_length)\n",
        "\n",
        "    # Decode the latent samples to amino acid sequences\n",
        "    generated_sequences = model.esm_encoder_decoder.decoder(latent_samples)\n",
        "\n",
        "    return generated_sequences\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "    val_snp = preprocess_snp_data('validation_dataset.csv')\n",
        "    test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "\n",
        "    train_snp = filter_datasets(train_snp)\n",
        "    val_snp = filter_datasets(val_snp)\n",
        "    test_snp = filter_datasets(test_snp)\n",
        "\n",
        "    # train_snp, val_snp, test_snp = train_snp[:16], val_snp[16:24], test_snp[24:32] # subset code\n",
        "\n",
        "    # Calculate max_length\n",
        "    all_seqs = pd.concat([\n",
        "        train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "        val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence'],\n",
        "        test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence']\n",
        "    ])\n",
        "    max_length = max(len(seq) for seq in all_seqs)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ProteinInteractionDataset(train_snp)\n",
        "    val_dataset = ProteinInteractionDataset(val_snp)\n",
        "    test_dataset = ProteinInteractionDataset(test_snp)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize SimpleLatentDiffusion model\n",
        "    latent_dim = 128\n",
        "    protein_dim = esm_model.embed_dim\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_steps = 1000\n",
        "\n",
        "    latent_diffusion_model = SimpleLatentDiffusion(esm_model, num_steps, latent_dim, protein_dim, device).to(device)\n",
        "\n",
        "    # Train SimpleLatentDiffusion model\n",
        "    optimizer = torch.optim.AdamW(latent_diffusion_model.parameters(), lr=1e-4)\n",
        "    train(latent_diffusion_model, train_loader, val_loader, optimizer, num_epochs=10, device=device)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(latent_diffusion_model.state_dict(), 'latent_diffusion_model.pth')\n",
        "\n",
        "    # Generation (dummy examples)\n",
        "    latent_diffusion_model.eval()\n",
        "\n",
        "    # Example usage of generation with guidance (sequence-based motif)\n",
        "    protein_seq = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n",
        "    motif_seq = \"LRSLGY\"\n",
        "    generated_binders_seq = generate_protein_binders(latent_diffusion_model, protein_seq, motif_seq, num_samples=3, guidance_scale=3.0)\n",
        "\n",
        "    print(\"Generated protein binders with guidance (sequence-based motif):\")\n",
        "    for i, seq in enumerate(generated_binders_seq):\n",
        "        print(f\"Binder {i+1}: {seq}\")\n",
        "\n",
        "    # Example usage of generation with guidance (binary motif)\n",
        "    binary_motif = torch.zeros(len(protein_seq))\n",
        "    binary_motif[30:36] = 1  # Assuming the motif is in this region\n",
        "    generated_binders_binary = generate_protein_binders(latent_diffusion_model, protein_seq, binary_motif, num_samples=3, guidance_scale=3.0)\n",
        "\n",
        "    print(\"\\nGenerated protein binders with guidance (binary motif):\")\n",
        "    for i, seq in enumerate(generated_binders_binary):\n",
        "        print(f\"Binder {i+1}: {seq}\")\n",
        "\n",
        "    # Example usage of generation without guidance\n",
        "    generated_binders_no_guidance = generate_protein_binders_without_guidance(latent_diffusion_model, sequence_length=100, num_samples=3)\n",
        "\n",
        "    print(\"\\nGenerated protein binders without guidance:\")\n",
        "    for i, seq in enumerate(generated_binders_no_guidance):\n",
        "        print(f\"Binder {i+1}: {seq}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580,
          "referenced_widgets": [
            "568b76a0326c48d9972f23240ba2e8df",
            "93506af9c8e74e7fbb5d8e63036b59dd",
            "30bb148261b54a8d87ba393ff9e7a655",
            "3d600601667b4bb98d1c0966c5531c05",
            "e8746b97377147eaa0bc7dc70d7d09ad",
            "8eb9d5cb7a114bd1bfae1d10d1ed2f74",
            "ae43d0adabf84fa3b3caac063213343c",
            "31c45f47f217429c972526948d7a4d22"
          ]
        },
        "id": "HbV8-2KMz-CL",
        "outputId": "1f336358-2e76-41ea-a7ec-831400053eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 12353\n",
            "Mismatched lengths: 0\n",
            "Total samples: 2390\n",
            "Mismatched lengths: 0\n",
            "Total samples: 2782\n",
            "Mismatched lengths: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:ns541cnv) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "568b76a0326c48d9972f23240ba2e8df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>1.02998</td></tr><tr><td>val_loss</td><td>1.12116</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jolly-energy-2</strong> at: <a href='https://wandb.ai/vskavi2003/simple_latent_diffusion/runs/ns541cnv' target=\"_blank\">https://wandb.ai/vskavi2003/simple_latent_diffusion/runs/ns541cnv</a><br/> View project at: <a href='https://wandb.ai/vskavi2003/simple_latent_diffusion' target=\"_blank\">https://wandb.ai/vskavi2003/simple_latent_diffusion</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240811_225205-ns541cnv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:ns541cnv). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1EBYzKC5cdf8cu4kkYeHgWcmUZAPc6Fli/Programmable Biology Group/Srikar/Code/proteins/flamingo-diffusion/data_dump/old_dat/wandb/run-20240811_225742-hy5jj5aa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vskavi2003/simple_latent_diffusion/runs/hy5jj5aa' target=\"_blank\">ruby-gorge-3</a></strong> to <a href='https://wandb.ai/vskavi2003/simple_latent_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vskavi2003/simple_latent_diffusion' target=\"_blank\">https://wandb.ai/vskavi2003/simple_latent_diffusion</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vskavi2003/simple_latent_diffusion/runs/hy5jj5aa' target=\"_blank\">https://wandb.ai/vskavi2003/simple_latent_diffusion/runs/hy5jj5aa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 60/12353 [00:20<1:18:52,  2.60it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HvrTuj0fP8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
